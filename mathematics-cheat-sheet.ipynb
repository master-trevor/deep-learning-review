{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§  Deep Learning Mathematics Cheat Sheet\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… 1. Derivative Rules\n",
    "\n",
    "- **Power Rule:**  \n",
    "  $$ \\frac{d}{dx} x^n = nx^{n-1} $$\n",
    "\n",
    "- **Product Rule:**  \n",
    "  $$ \\frac{d}{dx}[uv] = u'v + uv' $$\n",
    "\n",
    "- **Quotient Rule:**  \n",
    "  $$ \\frac{d}{dx} \\left( \\frac{u}{v} \\right) = \\frac{u'v - uv'}{v^2} $$\n",
    "\n",
    "- **Chain Rule (Single var):**  \n",
    "  $$ \\frac{d}{dx} f(g(x)) = f'(g(x)) \\cdot g'(x) $$\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… 2. Common Function Derivatives\n",
    "\n",
    "- $$ \\frac{d}{dx} \\sin(x) = \\cos(x) $$\n",
    "- $$ \\frac{d}{dx} \\cos(x) = -\\sin(x) $$\n",
    "- $$ \\frac{d}{dx} e^x = e^x $$\n",
    "- $$ \\frac{d}{dx} \\log(x) = \\frac{1}{x} $$\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… 3. Activation Function Derivatives\n",
    "\n",
    "- **Sigmoid:**  \n",
    "  $$ \\sigma(x) = \\frac{1}{1 + e^{-x}} $$\n",
    "  $$ \\sigma'(x) = \\sigma(x)(1 - \\sigma(x)) $$\n",
    "\n",
    "- **Tanh:**  \n",
    "  $$ \\frac{d}{dx} \\tanh(x) = 1 - \\tanh^2(x) $$\n",
    "\n",
    "- **ReLU:**  \n",
    "  $$ \\text{ReLU}(x) = \\max(0, x) $$\n",
    "  $$ \\text{ReLU}'(x) = \\begin{cases} 1 & \\text{if } x > 0 \\\\ 0 & \\text{if } x \\leq 0 \\end{cases} $$\n",
    "\n",
    "- **GELU (approx):**  \n",
    "  $$ \\text{GELU}(x) \\approx 0.5x \\left(1 + \\tanh\\left( \\sqrt{\\frac{2}{\\pi}}(x + 0.0447x^3) \\right) \\right) $$\n",
    "  > Derivative is complex and usually auto-differentiated.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… 4. Partial Derivatives\n",
    "\n",
    "- For $$ f(x, y) = (3x + 4y - 5)^2 $$:\n",
    "  - $$ \\frac{\\partial f}{\\partial x} = 6(3x + 4y - 5) $$\n",
    "  - $$ \\frac{\\partial f}{\\partial y} = 8(3x + 4y - 5) $$\n",
    "\n",
    "- **Gradient Vector:**  \n",
    "  $$ \\nabla f = \\left[ \\frac{\\partial f}{\\partial x_1},\\ \\frac{\\partial f}{\\partial x_2},\\ \\dots \\right] $$\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… 5. Chain Rule (Multivariable / Backprop)\n",
    "\n",
    "For composite:\n",
    "\n",
    "$$\n",
    "L = (\\hat{y} - y)^2,\\quad \\hat{y} = h^2,\\quad h = w_1x + b\n",
    "$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\frac{dL}{dw_1} = \\frac{dL}{d\\hat{y}} \\cdot \\frac{d\\hat{y}}{dh} \\cdot \\frac{dh}{dw_1} = 2(\\hat{y} - y) \\cdot 2h \\cdot x\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… 6. Softmax Function\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = \\frac{e^{z_i}}{\\sum_j e^{z_j}}\n",
    "$$\n",
    "\n",
    "- Output is a probability distribution\n",
    "- Sensitive to large logits\n",
    "- Used in final layer of classification models\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… 7. Log-Softmax Identity\n",
    "\n",
    "$$\n",
    "\\log(\\text{softmax}(z_i)) = z_i - \\log \\left( \\sum_j e^{z_j} \\right)\n",
    "$$\n",
    "\n",
    "- Helps with numerical stability\n",
    "- Efficient loss computation (built into frameworks)\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… 8. Cross-Entropy Loss\n",
    "\n",
    "Given:\n",
    "- $$ y = [0, 1, 0] $$\n",
    "- $$ \\hat{y} = \\text{softmax}(z) $$\n",
    "\n",
    "Cross-entropy:\n",
    "\n",
    "$$\n",
    "L = -\\sum_i y_i \\log(\\hat{y}_i) = -\\log(\\hat{y}_{\\text{true class}})\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… 9. Gradient of Cross-Entropy w.r.t Logits\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z_k} = \\hat{y}_k - y_k\n",
    "$$\n",
    "\n",
    "- Comes from combining softmax + cross-entropy\n",
    "- Applies chain rule after log-sum-exp trick\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… 10. Jacobian of Softmax\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\hat{y}_i}{\\partial z_j} =\n",
    "\\begin{cases}\n",
    "\\hat{y}_i(1 - \\hat{y}_i) & \\text{if } i = j \\\\\n",
    "-\\hat{y}_i \\hat{y}_j & \\text{if } i \\neq j\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Matrix form:\n",
    "\n",
    "$$\n",
    "J = \\text{diag}(\\hat{y}) - \\hat{y} \\hat{y}^\\top\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… 11. Log-Sum-Exp Trick\n",
    "\n",
    "To avoid overflow in:\n",
    "\n",
    "$$\n",
    "\\log \\left( \\sum_j e^{z_j} \\right)\n",
    "$$\n",
    "\n",
    "Use:\n",
    "\n",
    "$$\n",
    "\\log \\left( \\sum_j e^{z_j} \\right) = \\max(z) + \\log \\left( \\sum_j e^{z_j - \\max(z)} \\right)\n",
    "$$\n",
    "\n",
    "- Prevents numerical instability\n",
    "- Shifts logits before exponentiation\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… 12. Final Summary: Backprop Essentials\n",
    "\n",
    "| Component             | Gradient Form                                 |\n",
    "|------------------------|------------------------------------------------|\n",
    "| Linear layer           | $$ \\frac{dL}{dW} = \\delta \\cdot x^\\top $$     |\n",
    "| Activation (sigmoid, tanh) | Use chain rule w/ activation derivative |\n",
    "| Softmax + Cross-Entropy | $$ \\frac{dL}{dz_k} = \\hat{y}_k - y_k $$      |\n",
    "| Hidden layers          | Apply multivariable chain rule backwards     |\n",
    "| Jacobian               | For vector outputs like softmax               |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
