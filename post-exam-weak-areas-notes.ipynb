{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîÅ Deep Learning Review Notes ‚Äî Targeted Gaps\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 1. Activation Derivatives\n",
    "\n",
    "### **Sigmoid**\n",
    "- Definition:  \n",
    "  $$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "- Derivative:  \n",
    "  $$\\frac{d}{dx} \\sigma(x) = \\sigma(x)(1 - \\sigma(x))$$\n",
    "- Notes: Derivative is small for large |x| ‚Üí vanishing gradients.\n",
    "\n",
    "---\n",
    "\n",
    "### **Tanh**\n",
    "- Definition:  \n",
    "  $$\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$\n",
    "- Derivative:  \n",
    "  $$\\frac{d}{dx} \\tanh(x) = 1 - \\tanh^2(x)$$\n",
    "- Notes: Zero-centered ‚Üí often better than sigmoid.\n",
    "\n",
    "---\n",
    "\n",
    "### **ReLU vs GELU**\n",
    "\n",
    "| Feature               | ReLU               | GELU (used in Transformers)         |\n",
    "|-----------------------|--------------------|-------------------------------------|\n",
    "| Formula               | $\\max(0, x)$       | $\\text{GELU}(x) \\approx 0.5x(1 + \\tanh(\\sqrt{2/\\pi}(x + 0.0447x^3)))$ |\n",
    "| Derivative            | 0 (x < 0), 1 (x > 0) | Smooth, always non-zero             |\n",
    "| Gradient Behavior     | Harsh cutoff       | Probabilistic, soft cutoff          |\n",
    "| Problem               | Dead neurons       | No dead zones                       |\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ 2. Optimizer Theory\n",
    "\n",
    "### **Adam Optimizer (Adaptive Moment Estimation)**\n",
    "\n",
    "1. Gradient:  \n",
    "   $$g_t = \\nabla_\\theta L(\\theta_t)$$\n",
    "\n",
    "2. First moment estimate (mean):  \n",
    "   $$m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t$$\n",
    "\n",
    "3. Second moment estimate (uncentered variance):  \n",
    "   $$v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2$$\n",
    "\n",
    "4. Bias correction:  \n",
    "   $$\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}$$\n",
    "\n",
    "5. Parameter update:  \n",
    "   $$\\theta_t = \\theta_{t-1} - \\eta \\cdot \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}$$\n",
    "\n",
    "- Defaults: $\\beta_1 = 0.9$, $\\beta_2 = 0.999$, $\\epsilon = 10^{-8}$\n",
    "\n",
    "---\n",
    "\n",
    "### **AdamW (Decoupled Weight Decay)**\n",
    "\n",
    "- **Old approach (Adam + L2):**\n",
    "  $$g_t \\leftarrow g_t + \\lambda \\cdot \\theta$$\n",
    "\n",
    "- **AdamW decouples it:**\n",
    "  $$\\theta \\leftarrow \\theta - \\eta \\cdot \\text{AdamUpdate} - \\eta \\cdot \\lambda \\cdot \\theta$$\n",
    "\n",
    "- **Why it matters**: Regularization is applied directly to weights, not gradients ‚Üí more consistent behavior.\n",
    "\n",
    "- **Used in**: All modern transformer training (BERT, GPT, T5, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "## üìâ 3. Learning Rate Scheduling\n",
    "\n",
    "### Why Use a Schedule?\n",
    "\n",
    "- Large LR: fast but unstable  \n",
    "- Small LR: slow but stable  \n",
    "- Schedules give you the **best of both** (start warm, then cool)\n",
    "\n",
    "---\n",
    "\n",
    "### **Linear Warmup**\n",
    "\n",
    "- Slowly ramp up the LR over the first $T_{\\text{warmup}}$ steps:\n",
    "  $$\\text{lr}_t = \\eta \\cdot \\frac{t}{T_{\\text{warmup}}}$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Cosine Decay**\n",
    "\n",
    "- After warmup, gradually decay using a cosine function:\n",
    "  $$\n",
    "  \\text{lr}_t = \\eta \\cdot 0.5 \\left(1 + \\cos\\left(\\frac{t - T_{\\text{warmup}}}{T_{\\text{total}} - T_{\\text{warmup}}} \\cdot \\pi \\right)\\right)\n",
    "  $$\n",
    "\n",
    "- Smoothly transitions learning rate to near zero by the end of training.\n",
    "\n",
    "---\n",
    "\n",
    "### **Visual Summary**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
