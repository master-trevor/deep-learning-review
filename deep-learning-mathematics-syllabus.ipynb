{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📐 Deep Learning Math Refresher Syllabus (Bonus Review)\n",
    "\n",
    "This bonus section reinforces the core mathematical tools behind deep learning, including calculus, derivatives, Jacobians, and special functions. You don't need to master everything — but understanding these pieces will improve your ability to debug, design, and optimize AI systems.\n",
    "\n",
    "---\n",
    "\n",
    "## ⏱️ Total Time Estimate: ~6–8 hours\n",
    "\n",
    "Can be done in 3–5 sessions depending on your pace.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 Section 1: Derivative Rules Refresher\n",
    "\n",
    "🕒 Estimated Time: 1–1.5 hours\n",
    "\n",
    "### ✅ Goals:\n",
    "- Re-learn the product, quotient, and chain rules\n",
    "- Practice taking derivatives of:\n",
    "  - Polynomials\n",
    "  - Exponentials\n",
    "  - Logarithmic functions\n",
    "  - Trig functions (sin, cos, tan)\n",
    "  - Hyperbolic functions (tanh, sigmoid)\n",
    "\n",
    "### 📌 Suggested Checkpoints:\n",
    "- Derivative cheat sheet\n",
    "- Practice quiz\n",
    "- Apply to activation functions (e.g., ReLU, sigmoid, tanh)\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 Section 2: Partial Derivatives\n",
    "\n",
    "🕒 Estimated Time: 45 min – 1 hour\n",
    "\n",
    "### ✅ Goals:\n",
    "- Understand multivariable functions\n",
    "- Learn how to compute gradients w.r.t. one input while keeping others fixed\n",
    "- Apply to simple loss functions like:\n",
    "  - \\( L = (w_1x_1 + w_2x_2 - y)^2 \\)\n",
    "\n",
    "### 📌 Suggested Checkpoints:\n",
    "- Visual examples (slope of a surface)\n",
    "- Practice calculating ∂L/∂w\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 Section 3: Chain Rule (Multivariable)\n",
    "\n",
    "🕒 Estimated Time: 1 hour\n",
    "\n",
    "### ✅ Goals:\n",
    "- Apply the chain rule through multiple layers\n",
    "- Understand how gradients propagate backward\n",
    "- Key to understanding backpropagation\n",
    "\n",
    "### 📌 Suggested Checkpoints:\n",
    "- Hand-derive gradients through a 2-layer NN\n",
    "- Diagram-style walkthroughs\n",
    "- Connect with reverse-mode autodiff\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 Section 4: Jacobian Matrix\n",
    "\n",
    "🕒 Estimated Time: 45 min – 1 hour\n",
    "\n",
    "### ✅ Goals:\n",
    "- Learn what the Jacobian matrix is\n",
    "- Understand why it generalizes gradients to vector-valued functions\n",
    "- See how it appears in deep learning frameworks during backpropagation\n",
    "\n",
    "### 📌 Suggested Checkpoints:\n",
    "- Compute a small Jacobian by hand\n",
    "- Visualize transformation of input → output\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 Section 5: Derivatives of Special Functions\n",
    "\n",
    "🕒 Estimated Time: 45 min – 1 hour\n",
    "\n",
    "### ✅ Goals:\n",
    "- Derivatives and properties of:\n",
    "  - Sigmoid\n",
    "  - Tanh\n",
    "  - ReLU\n",
    "  - Softmax\n",
    "- Why these functions are chosen\n",
    "- Where vanishing gradients come from\n",
    "\n",
    "### 📌 Suggested Checkpoints:\n",
    "- Plot functions + derivatives\n",
    "- Practice gradient of sigmoid, tanh, ReLU\n",
    "- Understand behavior at edges\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 Section 6: Vector & Matrix Derivatives (Advanced Tier)\n",
    "\n",
    "🕒 Estimated Time: 1–1.5 hours (optional)\n",
    "\n",
    "### ✅ Goals:\n",
    "- Understand how to compute derivatives with respect to vectors/matrices\n",
    "- Practice with:\n",
    "  - \\( y = Wx + b \\)\n",
    "  - Cross-entropy loss\n",
    "  - Softmax + log combination\n",
    "- Recognize common shorthand in papers\n",
    "\n",
    "### 📌 Suggested Checkpoints:\n",
    "- Derive dL/dW for linear layer\n",
    "- Apply chain rule across matrix operations\n",
    "- Read a paper section using matrix calculus\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Final Recap / Self-Quiz\n",
    "\n",
    "- Explain backprop in terms of chain rule\n",
    "- Derive gradients for a 2-layer NN with sigmoid/tanh\n",
    "- Identify vanishing/exploding gradients from function shapes\n",
    "- Compute a Jacobian and explain what it represents\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLE ??? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
