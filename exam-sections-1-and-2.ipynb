{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🧠 Deep Learning Exam Worksheet\n",
    "**Coverage**: 01-nn-fundamentals + 02-model-training-and-optimization  \n",
    "**Format**: Derivations, math, conceptual explanations  \n",
    "**Total Points**: 100\n",
    "\n",
    "---\n",
    "\n",
    "## 📐 Section 1: Matrix Ops, Vectorization, and Broadcasting (15 pts)\n",
    "\n",
    "1. **[3 pts] Matrix Multiplication**  \n",
    "   Given:  \n",
    "   $A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}$,  \n",
    "   $B = \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix}$  \n",
    "   Compute $C = AB$ using:  \n",
    "   $$\n",
    "   C_{ij} = \\sum_k A_{ik} B_{kj}\n",
    "   $$\n",
    "\n",
    "2. **[3 pts] Broadcasting Conditions**  \n",
    "   Explain the rules of broadcasting and illustrate using tensors of shapes (2, 3) and (1, 3). Why does it work?\n",
    "\n",
    "3. **[3 pts] Bias Addition via Broadcasting**  \n",
    "   Given $x \\in \\mathbb{R}^{n \\times d}$, $W \\in \\mathbb{R}^{m \\times d}$, and $b \\in \\mathbb{R}^{m}$, explain how broadcasting computes:  \n",
    "   $$\n",
    "   y = xW^T + b\n",
    "   $$  \n",
    "   What is the shape of $y$?\n",
    "\n",
    "4. **[3 pts] Vectorization vs Looping**  \n",
    "   Define vectorization. Rewrite the following using vector notation:  \n",
    "   $$\n",
    "   \\text{sum} = \\sum_{i=1}^{n} a_i b_i\n",
    "   $$\n",
    "\n",
    "5. **[3 pts] Tensor Shape Flow**  \n",
    "   Track shapes through:  \n",
    "   `Linear(100 → 64) → ReLU → Linear(64 → 32) → ReLU → Linear(32 → 1) → Sigmoid`  \n",
    "   Input batch shape: (16, 100). What are the output shapes after each layer?\n",
    "\n",
    "---\n",
    "\n",
    "## 🔥 Section 2: Activations & Gradients (20 pts)\n",
    "\n",
    "6. **[4 pts] Activation Derivatives**  \n",
    "   Derive:\n",
    "   - $\\frac{d}{dx} \\sigma(x)$ where $\\sigma(x) = \\frac{1}{1 + e^{-x}}$\n",
    "   - $\\frac{d}{dx} \\tanh(x)$\n",
    "\n",
    "7. **[3 pts] GELU Approximation**  \n",
    "   Write the GELU approximation formula. Why is it preferred in Transformer models over ReLU?\n",
    "\n",
    "8. **[3 pts] Vanishing vs Exploding Gradients**  \n",
    "   Define both problems and explain why they happen during backpropagation.\n",
    "\n",
    "9. **[3 pts] ReLU vs GELU**  \n",
    "   Compare in terms of behavior on negatives, gradient flow, and impact on deep learning.\n",
    "\n",
    "10. **[4 pts] Softmax + Cross-Entropy**  \n",
    "    Given logits $z = [2.0, 1.0, 0.1]$, compute:\n",
    "    - Softmax outputs: $\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}}$\n",
    "    - Cross-entropy loss if true class is index 0: $-\\log(\\text{softmax}_0)$\n",
    "\n",
    "---\n",
    "\n",
    "## 🧮 Section 3: Loss Functions & Optimization (25 pts)\n",
    "\n",
    "11. **[3 pts] BCE Derivation**  \n",
    "    Derive:  \n",
    "    $$\n",
    "    \\mathcal{L}_{\\text{BCE}} = - \\left[ y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y}) \\right]\n",
    "    $$  \n",
    "    and explain its connection to Maximum Likelihood Estimation.\n",
    "\n",
    "12. **[3 pts] MSE Gradient**  \n",
    "    Given:  \n",
    "    $$\n",
    "    \\mathcal{L}_{\\text{MSE}} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "    $$  \n",
    "    Derive gradients w.r.t. weights $w$ and bias $b$ for a linear model.\n",
    "\n",
    "13. **[3 pts] Cross-Entropy vs MSE**  \n",
    "    Why is cross-entropy preferred for classification problems? Use softmax output as an example.\n",
    "\n",
    "14. **[4 pts] Adam Optimizer**  \n",
    "    Define and explain:\n",
    "    - First moment estimate $m_t$\n",
    "    - Second moment estimate $v_t$\n",
    "    - Bias correction\n",
    "    - Final update rule:  \n",
    "      $$\n",
    "      \\theta_t = \\theta_{t-1} - \\eta \\cdot \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n",
    "      $$\n",
    "\n",
    "15. **[3 pts] Adam vs SGD**  \n",
    "    Compare in terms of speed, robustness, and generalization.\n",
    "\n",
    "16. **[3 pts] AdamW Fix**  \n",
    "    How does AdamW decouple weight decay from gradients? Why is this better than L2 + Adam?\n",
    "\n",
    "17. **[3 pts] LR Schedules**  \n",
    "    Describe cosine decay and linear warmup. Why are they useful in large model training?\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ Section 4: Regularization & Normalization (20 pts)\n",
    "\n",
    "18. **[3 pts] L1 vs L2 Regularization**  \n",
    "    Define each penalty term:  \n",
    "    - L1: $\\lambda \\sum_i |w_i|$  \n",
    "    - L2: $\\lambda \\sum_i w_i^2$  \n",
    "    Which one induces sparsity and why?\n",
    "\n",
    "19. **[3 pts] Dropout**  \n",
    "    Explain how dropout is applied to activations and why outputs are scaled by the keep probability.\n",
    "\n",
    "20. **[4 pts] BatchNorm**  \n",
    "    Given:  \n",
    "    $$\n",
    "    \\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}, \\quad y_i = \\gamma \\hat{x}_i + \\beta\n",
    "    $$  \n",
    "    Explain each term and how BatchNorm helps training.\n",
    "\n",
    "21. **[3 pts] LayerNorm vs BatchNorm**  \n",
    "    Complete:\n",
    "\n",
    "    | Property               | BatchNorm        | LayerNorm        |\n",
    "    |------------------------|------------------|------------------|\n",
    "    | Normalizes over        |                  |                  |\n",
    "    | Needs batch stats?     |                  |                  |\n",
    "    | Best for               | CNNs             | Transformers     |\n",
    "    | Works with batch size=1|                  |                  |\n",
    "\n",
    "22. **[4 pts] Gradient Clipping**  \n",
    "    Provide the clipping formula:  \n",
    "    $$\n",
    "    \\text{If } \\|\\mathbf{g}\\|_2 > \\text{max\\_norm}, \\quad \\mathbf{g} \\leftarrow \\mathbf{g} \\cdot \\frac{\\text{max\\_norm}}{\\|\\mathbf{g}\\|_2}\n",
    "    $$  \n",
    "    Why is this useful in training deep or unstable networks?\n",
    "\n",
    "23. **[3 pts] Compatibility Warnings**  \n",
    "    Explain why the following combinations can cause issues:\n",
    "    - BatchNorm + Dropout\n",
    "    - Adam + L2 Regularization (not AdamW)\n",
    "\n",
    "---\n",
    "\n",
    "## 📈 Section 5: Training & Debugging (20 pts)\n",
    "\n",
    "24. **[3 pts] Overfitting**  \n",
    "    What signs appear in loss curves? List 3 techniques to reduce it.\n",
    "\n",
    "25. **[3 pts] Underfitting**  \n",
    "    If both training and validation losses are high, what changes might help?\n",
    "\n",
    "26. **[4 pts] Loss ↑, Accuracy ↑**  \n",
    "    Why can accuracy improve while loss increases in classification?\n",
    "\n",
    "27. **[4 pts] Generalization Gap**  \n",
    "    Define:  \n",
    "    $$\n",
    "    \\text{Gap} = \\mathcal{L}_{\\text{val}} - \\mathcal{L}_{\\text{train}}\n",
    "    $$  \n",
    "    What does a growing gap suggest? How can it be fixed?\n",
    "\n",
    "28. **[3 pts] Instability Diagnosis**  \n",
    "    What might cause:\n",
    "    - Flat training loss early\n",
    "    - NaNs or spikes in loss  \n",
    "    Suggest fixes.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "18+5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "24+7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10+24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "14+32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
