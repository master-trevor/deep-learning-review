{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📚 Word Embeddings & Representation Learning\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 What Are Word Embeddings?\n",
    "\n",
    "- **Embeddings** are dense vector representations of discrete tokens (e.g., words, subwords, items).\n",
    "- They capture **semantic similarity**: similar words end up close together in vector space.\n",
    "- Used to convert categorical/textual data into **numeric form** for neural networks.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔢 From Text to Embeddings: Step-by-Step\n",
    "\n",
    "### 1. **Tokenization**\n",
    "Break text into tokens:\n",
    "- Word-level: `\"the cat sleeps\"` → `[\"the\", \"cat\", \"sleeps\"]`\n",
    "- Subword-level (e.g. BERT): `\"sleeping\"` → `[\"sleep\", \"##ing\"]`\n",
    "\n",
    "### 2. **Vocabulary Mapping**\n",
    "Each token maps to a unique integer ID using a vocab dictionary:\n",
    "```python\n",
    "vocab = {\n",
    "  \"[PAD]\": 0,\n",
    "  \"[UNK]\": 1,\n",
    "  \"the\": 2,\n",
    "  \"cat\": 3,\n",
    "  \"sleeps\": 4\n",
    "}\n",
    "```\n",
    "\n",
    "### 3. **Embedding Lookup**\n",
    "Tokens → IDs → vectors:\n",
    "```python\n",
    "nn.Embedding(num_embeddings=10000, embedding_dim=300)\n",
    "```\n",
    "- Input: `[2, 3, 4]`\n",
    "- Output: shape `(3, 300)`\n",
    "\n",
    "---\n",
    "\n",
    "## 🧊 Frozen vs 🧠 Contextual Embeddings\n",
    "\n",
    "| Feature                     | Frozen Embeddings              | Contextual Embeddings                  |\n",
    "|----------------------------|--------------------------------|----------------------------------------|\n",
    "| Meaning changes by context | ❌ No                          | ✅ Yes                                 |\n",
    "| Token granularity          | Word-level                    | Subword-level                         |\n",
    "| Examples                   | Word2Vec, GloVe, FastText      | BERT, GPT, RoBERTa                     |\n",
    "| Computation                | Precomputed (lookup)          | Computed dynamically (transformers)    |\n",
    "| Representation type        | Static vector per token       | Depends on full sentence context       |\n",
    "\n",
    "### 🔄 Example (semantic vector math):\n",
    "If the model captures relational meaning:\n",
    "$$\n",
    "\\text{embedding}(\"king\") - \\text{embedding}(\"man\") + \\text{embedding}(\"woman\") \\approx \\text{embedding}(\"queen\")\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 🛠 Special Tokens\n",
    "\n",
    "| Token     | Purpose                                        |\n",
    "|-----------|------------------------------------------------|\n",
    "| `[PAD]`   | Used to equalize sequence lengths in batching  |\n",
    "| `[UNK]`   | Represents unknown/out-of-vocab tokens         |\n",
    "\n",
    "---\n",
    "\n",
    "## 📏 Similarity Measures\n",
    "\n",
    "To measure similarity between embeddings:\n",
    "- **Cosine similarity**:\n",
    "$$\n",
    "\\text{cosine\\_similarity}(u, v) = \\frac{u \\cdot v}{\\|u\\| \\|v\\|}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Embedding Layer Behavior\n",
    "\n",
    "- It's just a **lookup table** with learnable weights:\n",
    "$$\n",
    "\\text{EmbeddingMatrix} \\in \\mathbb{R}^{V \\times D}\n",
    "$$\n",
    "Where:\n",
    "- $V$ = vocabulary size  \n",
    "- $D$ = embedding dimension\n",
    "\n",
    "Each token ID $i$ retrieves the vector from row $i$ of this matrix.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Summary\n",
    "\n",
    "- Embeddings convert symbolic tokens into trainable dense vectors.\n",
    "- They allow models to **learn internal representations** of language, categories, users, etc.\n",
    "- Frozen embeddings = static vectors from pretraining  \n",
    "- Contextual embeddings = dynamic vectors based on full sentence meaning  \n",
    "- Modern models tokenize flexibly (subwords) to minimize `[UNK]` tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔁 Transfer Learning, Pretraining & Fine-Tuning\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 What is Pretraining?\n",
    "\n",
    "- **Pretraining** = train a model on a large general-purpose task (e.g., masked language modeling, next-token prediction).\n",
    "- Models learn broad features like syntax, grammar, and real-world facts before being used for specific tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧰 Leveraging Pretrained Models\n",
    "\n",
    "### 1. **Pretrained Embeddings Only**\n",
    "- Use pretrained static embeddings like Word2Vec, GloVe.\n",
    "- Can be frozen or fine-tuned.\n",
    "\n",
    "### 2. **Pretrained Transformers (BERT, GPT, etc.)**\n",
    "- Use the full stack (embeddings + encoder layers).\n",
    "- Typically followed by fine-tuning on your downstream task (e.g., classification, NER).\n",
    "\n",
    "---\n",
    "\n",
    "## 🔧 Fine-Tuning Pretrained Models\n",
    "\n",
    "### 🔹 Workflow:\n",
    "1. Load a pretrained model.\n",
    "2. Add a task-specific head (e.g., linear classifier).\n",
    "3. Fine-tune the full model (or parts of it) using your labeled data.\n",
    "\n",
    "### ✅ When to fine-tune:\n",
    "- You have enough labeled data.\n",
    "- Your task is different from pretraining (domain-specific, e.g., medical text).\n",
    "\n",
    "---\n",
    "\n",
    "## 🆚 Feature Extraction vs Fine-Tuning\n",
    "\n",
    "| Strategy           | Updates Pretrained Weights? | Use Case                        |\n",
    "|--------------------|-----------------------------|----------------------------------|\n",
    "| Feature extraction | ❌ No                        | Fast, low-data, prototyping      |\n",
    "| Fine-tuning        | ✅ Yes                       | Max performance, custom tasks   |\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ Learning Rate Best Practices\n",
    "\n",
    "- Fine-tuning typically uses **lower LRs** (e.g., $2e\\text{-}5$ to $5e\\text{-}5$).\n",
    "- Too high = catastrophic forgetting.\n",
    "- Too low = underfitting or slow training.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔥 Warmup: What & Why\n",
    "\n",
    "- **Warmup** = gradually increase the LR at the start of training.\n",
    "- Prevents unstable updates early on.\n",
    "- Commonly used with transformers.\n",
    "\n",
    "### 🔢 Linear Warmup Formula:\n",
    "\n",
    "$$\n",
    "\\text{LR}(t) = \\text{initial\\_lr} \\times \\frac{t}{\\text{warmup\\_steps}} \\quad \\text{for } t \\leq \\text{warmup\\_steps}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 📉 Learning Rate Decay Strategies\n",
    "\n",
    "### 🔸 Linear Decay:\n",
    "$$\n",
    "\\text{LR}(t) = \\text{initial\\_lr} \\times \\left(1 - \\frac{t}{T}\\right)\n",
    "$$\n",
    "\n",
    "### 🔸 Cosine Decay:\n",
    "$$\n",
    "\\text{LR}(t) = \\text{initial\\_lr} \\times \\frac{1}{2} \\left(1 + \\cos\\left(\\frac{t \\pi}{T}\\right)\\right)\n",
    "$$\n",
    "\n",
    "| Strategy       | Behavior                          | Use Case                       |\n",
    "|----------------|-----------------------------------|--------------------------------|\n",
    "| Linear         | Steady decrease                   | Most common default            |\n",
    "| Cosine         | Gentle start, sharper end         | Better generalization on some tasks |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧊 Layer Freezing Strategies\n",
    "\n",
    "### ✅ Why Freeze Layers?\n",
    "- Preserve general knowledge from pretraining.\n",
    "- Reduce overfitting and speed up training.\n",
    "\n",
    "| Layer Type           | Freeze?  | Reason                        |\n",
    "|----------------------|----------|-------------------------------|\n",
    "| Embeddings / Early   | ✅ Often | Store general syntax/semantics |\n",
    "| Middle / Top Layers  | ❌       | Needed for task adaptation    |\n",
    "| Classifier Head      | ❌       | Task-specific output layer    |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 Progressive Unfreezing (Optional)\n",
    "\n",
    "- Start with all layers frozen.\n",
    "- Gradually unfreeze top → bottom one layer at a time.\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Layer-wise Learning Rate Decay (LLRD)\n",
    "\n",
    "### 🔹 Concept:\n",
    "Use **lower learning rates** for lower layers and **higher LRs** for upper layers and the head.\n",
    "\n",
    "### 🔢 Example:\n",
    "If base LR is $2e\\text{-}5$ and decay factor is $0.95$:\n",
    "\n",
    "$$\n",
    "\\text{LR}_i = \\text{base\\_lr} \\times (0.95)^{\\text{depth}_{\\text{top}} - i}\n",
    "$$\n",
    "\n",
    "Where $i$ is the layer index.\n",
    "\n",
    "| Layer     | Learning Rate Scaling |\n",
    "|-----------|------------------------|\n",
    "| Bottom    | $0.25 \\times$ base LR |\n",
    "| Middle    | $0.5 \\times$ base LR  |\n",
    "| Top       | $1.0 \\times$ base LR  |\n",
    "| Classifier| $2.0 \\times$ base LR  |\n",
    "\n",
    "> 🎯 This gives you **fine-grained control** over how much each layer learns.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Freezing vs LLRD: A Mental Model\n",
    "\n",
    "| Concept       | Characteristic         |\n",
    "|---------------|------------------------|\n",
    "| Freezing      | Binary (train or don't)|\n",
    "| LLRD          | Smooth control (scale) |\n",
    "| ReLU : Freezing | All-or-nothing        |\n",
    "| GELU : LLRD     | Smooth, probabilistic |\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ TL;DR\n",
    "\n",
    "- Fine-tuning pretrained models requires **careful learning rate control**.\n",
    "- Combine **warmup**, **decay**, **freezing**, and **LLRD** to train safely and effectively.\n",
    "- Try different combinations and use validation loss to guide strategy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔄 Tensor Shape Manipulation: `unsqueeze()` vs `squeeze()`\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Overview\n",
    "\n",
    "- **`unsqueeze(dim)`** → adds a new dimension of size `1` at the specified `dim` index\n",
    "- **`squeeze(dim=None)`** → removes any dimension of size `1` (or a specific one, if you provide `dim`)\n",
    "\n",
    "These functions are used to **reshape tensors**, especially for:\n",
    "- Batching\n",
    "- Broadcasting\n",
    "- Compatibility with model input/output shapes\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 `unsqueeze(dim)` — Add dimension\n",
    "\n",
    "### 📌 Usage:\n",
    "```python\n",
    "x = torch.tensor([1, 2, 3])        # Shape: [3]\n",
    "x.unsqueeze(0).shape               # [1, 3] — adds a batch dimension\n",
    "x.unsqueeze(1).shape               # [3, 1] — makes it a column vector\n",
    "```\n",
    "\n",
    "### 🧠 Intuition:\n",
    "- Think of `unsqueeze(0)` as: \"wrap this tensor in a batch\"\n",
    "- Think of `unsqueeze(1)` as: \"turn this 1D row into a 2D column\"\n",
    "\n",
    "### 📐 Shape Rule:\n",
    "If tensor has shape `[D1, D2, ..., Dn]`:\n",
    "- You can insert a new dimension at any index from `0` to `n`\n",
    "- Negative indices count from the end (just like Python lists)\n",
    "\n",
    "| Input Shape | Operation          | Output Shape | Meaning                      |\n",
    "|-------------|--------------------|--------------|------------------------------|\n",
    "| `[3]`       | `.unsqueeze(0)`    | `[1, 3]`     | Add batch dimension          |\n",
    "| `[3]`       | `.unsqueeze(1)`    | `[3, 1]`     | Convert to column shape      |\n",
    "| `[3, 4]`     | `.unsqueeze(-1)`   | `[3, 4, 1]`   | Add trailing singleton dim   |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔻 `squeeze(dim=None)` — Remove dimension(s)\n",
    "\n",
    "### 📌 Usage:\n",
    "```python\n",
    "x = torch.tensor([[[1], [2], [3]]])  # Shape: [1, 3, 1]\n",
    "x.squeeze().shape                    # [3] — removes all singleton dimensions\n",
    "x.squeeze(0).shape                   # [3, 1] — removes only dim 0 if it's size 1\n",
    "```\n",
    "\n",
    "### 🧠 Intuition:\n",
    "- Use `.squeeze()` to get rid of unnecessary `[1]`s in your tensor shape\n",
    "- Common when reducing outputs like `[1, 1, N]` → `[N]`\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 When to use\n",
    "\n",
    "| Use Case                      | Recommendation            |\n",
    "|-------------------------------|----------------------------|\n",
    "| One input → batch model       | Use `.unsqueeze(0)`        |\n",
    "| Fix shape mismatch errors     | Use `.unsqueeze()` or `.squeeze()` |\n",
    "| Collapse outputs              | Use `.squeeze()` to simplify |\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ TL;DR\n",
    "\n",
    "- `unsqueeze(n)` = **insert** new axis of size `1` at index `n`\n",
    "- `squeeze(n)` = **remove** axis of size `1` (or all of them if no dim is given)\n",
    "- These are your tools for making tensor shapes compatible with model input/output expectations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
