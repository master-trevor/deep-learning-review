{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔁 Recurrent Neural Networks (RNNs)\n",
    "\n",
    "## 📌 What is an RNN?\n",
    "\n",
    "A **Recurrent Neural Network (RNN)** is a type of neural network designed to process **sequential data**, such as text, time series, or speech.\n",
    "\n",
    "The key feature is a **hidden state** that carries information from previous time steps, enabling the network to have \"memory.\"\n",
    "\n",
    "---\n",
    "\n",
    "## 🔧 RNN Cell Update Formula\n",
    "\n",
    "At each time step \\( t \\), the hidden state is updated as:\n",
    "\n",
    "$$\n",
    "h_t = \\tanh(W_{xh} x_t + W_{hh} h_{t-1} + b_h)\n",
    "$$\n",
    "\n",
    "- \\( x_t \\): input at time step \\( t \\)\n",
    "- \\( h_{t-1} \\): hidden state from the previous step\n",
    "- \\( W_{xh} \\): input-to-hidden weights\n",
    "- \\( W_{hh} \\): hidden-to-hidden weights (shared across all time steps)\n",
    "- \\( b_h \\): bias\n",
    "- \\( \\tanh \\): non-linearity to keep activations bounded\n",
    "\n",
    "---\n",
    "\n",
    "## 🔄 Unrolling the RNN\n",
    "\n",
    "For a sequence of length \\( T \\), the RNN is \"unrolled\" like this:\n",
    "\n",
    "$$\n",
    "h_1 = \\tanh(W_{xh} x_1 + W_{hh} h_0 + b_h) \\\\\n",
    "h_2 = \\tanh(W_{xh} x_2 + W_{hh} h_1 + b_h) \\\\\n",
    "\\vdots \\\\\n",
    "h_T = \\tanh(W_{xh} x_T + W_{hh} h_{T-1} + b_h)\n",
    "$$\n",
    "\n",
    "This shows how the hidden state flows through time, accumulating knowledge from earlier inputs.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Key Concepts\n",
    "\n",
    "### ✅ Strengths:\n",
    "- Naturally handles variable-length sequences\n",
    "- Captures temporal patterns (e.g. language, stock prices)\n",
    "- Simple and intuitive architecture\n",
    "\n",
    "### ❌ Limitations:\n",
    "- **Vanishing gradients**: early inputs have negligible effect on output\n",
    "- **Exploding gradients**: gradients can become too large and destabilize training\n",
    "- **Slow training**: can't parallelize across time steps\n",
    "\n",
    "---\n",
    "\n",
    "## 📉 Vanishing & Exploding Gradients\n",
    "\n",
    "To compute the gradient of the loss \\( L \\) with respect to a past hidden state:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial h_{t-k}} = \\frac{\\partial L}{\\partial h_t} \\cdot \\prod_{i=1}^{k} \\frac{\\partial h_{t-i+1}}{\\partial h_{t-i}}\n",
    "$$\n",
    "\n",
    "### Vanishing:\n",
    "If each term in the product is \\( < 1 \\), the gradient shrinks:\n",
    "\n",
    "$$\n",
    "\\prod (0.5) \\cdot (0.5) \\cdot (0.5) = 0.125\n",
    "$$\n",
    "\n",
    "→ Early time steps have **near-zero gradient**, so they don't learn.\n",
    "\n",
    "### Exploding:\n",
    "If each term is \\( > 1 \\), the gradient grows:\n",
    "\n",
    "$$\n",
    "\\prod (5) \\cdot (5) \\cdot (5) = 125\n",
    "$$\n",
    "\n",
    "→ Early time steps have **huge gradient**, which can blow up weights.\n",
    "\n",
    "---\n",
    "\n",
    "## 🛡️ Fixes\n",
    "\n",
    "- **Gradient clipping** to prevent explosion:\n",
    "  ```python\n",
    "  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔐 Long Short-Term Memory (LSTM)\n",
    "\n",
    "## 📌 What is an LSTM?\n",
    "\n",
    "An **LSTM (Long Short-Term Memory)** is a type of recurrent neural network (RNN) designed to **learn long-term dependencies** more effectively than a basic RNN.\n",
    "\n",
    "It addresses the **vanishing gradient problem** by introducing a **cell state** and **gating mechanisms** that control the flow of information.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Core Components\n",
    "\n",
    "LSTM maintains:\n",
    "- A **hidden state** $h_t$ — short-term working memory\n",
    "- A **cell state** $c_t$ — long-term memory\n",
    "- **Gates** to manage memory updates\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ LSTM Cell Equations\n",
    "\n",
    "At time step $t$, with input $x_t$, previous hidden state $h_{t-1}$, and previous cell state $c_{t-1}$:\n",
    "\n",
    "### 🔒 Forget gate:\n",
    "$$\n",
    "f_t = \\sigma(W_f x_t + U_f h_{t-1} + b_f)\n",
    "$$\n",
    "\n",
    "### ➕ Input gate:\n",
    "$$\n",
    "i_t = \\sigma(W_i x_t + U_i h_{t-1} + b_i)\n",
    "$$\n",
    "\n",
    "### 🧠 Candidate memory:\n",
    "$$\n",
    "\\tilde{c}_t = \\tanh(W_c x_t + U_c h_{t-1} + b_c)\n",
    "$$\n",
    "\n",
    "### 🔁 Cell state update:\n",
    "$$\n",
    "c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t\n",
    "$$\n",
    "\n",
    "### 📤 Output gate:\n",
    "$$\n",
    "o_t = \\sigma(W_o x_t + U_o h_{t-1} + b_o)\n",
    "$$\n",
    "\n",
    "### 🧠 Hidden state update:\n",
    "$$\n",
    "h_t = o_t \\odot \\tanh(c_t)\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- $\\sigma$ is the sigmoid activation function  \n",
    "- $\\tanh$ is the hyperbolic tangent function  \n",
    "- $\\odot$ is element-wise multiplication\n",
    "\n",
    "---\n",
    "\n",
    "## 🧱 What Each Gate Does\n",
    "\n",
    "| Gate | Equation | Purpose |\n",
    "|------|----------|---------|\n",
    "| Forget Gate $f_t$ | $\\sigma(W_f x_t + U_f h_{t-1})$ | Decides what to forget from $c_{t-1}$ |\n",
    "| Input Gate $i_t$ | $\\sigma(W_i x_t + U_i h_{t-1})$ | Decides what new info to add |\n",
    "| Candidate $\\tilde{c}_t$ | $\\tanh(W_c x_t + U_c h_{t-1})$ | Proposes new memory |\n",
    "| Output Gate $o_t$ | $\\sigma(W_o x_t + U_o h_{t-1})$ | Decides what to output as $h_t$ |\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 Why LSTMs Solve the Vanishing Gradient Problem\n",
    "\n",
    "- The **cell state** $c_t$ enables gradients to flow **additively**, not multiplicatively, through time.\n",
    "- When $f_t \\approx 1$ and $i_t \\approx 0$, the cell simply carries forward unchanged:\n",
    "  $$\n",
    "  c_t \\approx c_{t-1}\n",
    "  $$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Activation Function Roles\n",
    "\n",
    "- **Sigmoid**: used in gates — outputs in \\( (0, 1) \\), perfect for gating/filtering\n",
    "- **Tanh**: used for candidate and output signal — outputs in \\( (-1, 1) \\), good for representing values with direction (positive or negative)\n",
    "\n",
    "| Function | Used For | Role |\n",
    "|----------|----------|------|\n",
    "| \\( \\sigma \\) (sigmoid) | Forget, Input, Output gates | Controls how much info flows |\n",
    "| \\( \\tanh \\) | Candidate memory, final hidden state | Represents information signal |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Summary\n",
    "\n",
    "- LSTM introduces a **cell state** that persists through time with minimal modification.\n",
    "- **Gates** allow the model to learn what to keep, forget, and output.\n",
    "- Fixes vanishing gradients by preserving important long-term dependencies.\n",
    "- Used heavily before transformers, still useful in speech, time series, and edge devices.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 💡 Why Gradients in LSTM Flow Additively\n",
    "\n",
    "The core LSTM cell state update equation is:\n",
    "\n",
    "$$\n",
    "c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t\n",
    "$$\n",
    "\n",
    "Let’s look at how this evolves over multiple time steps:\n",
    "\n",
    "### 🧮 Step-by-Step Cell State Flow\n",
    "\n",
    "Unrolling the cell across 3 time steps:\n",
    "\n",
    "- $c_1 = f_1 \\odot c_0 + i_1 \\odot \\tilde{c}_1$\n",
    "- $c_2 = f_2 \\odot c_1 + i_2 \\odot \\tilde{c}_2$\n",
    "- $c_3 = f_3 \\odot c_2 + i_3 \\odot \\tilde{c}_3$\n",
    "\n",
    "Now substitute $c_1$ and $c_2$ recursively:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "c_3 &= f_3 \\odot [f_2 \\odot (f_1 \\odot c_0 + i_1 \\odot \\tilde{c}_1) + i_2 \\odot \\tilde{c}_2] + i_3 \\odot \\tilde{c}_3 \\\\\n",
    "&= f_3 \\odot f_2 \\odot f_1 \\odot c_0 + f_3 \\odot f_2 \\odot i_1 \\odot \\tilde{c}_1 + f_3 \\odot i_2 \\odot \\tilde{c}_2 + i_3 \\odot \\tilde{c}_3\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### ✅ Key Insight\n",
    "\n",
    "- Each contribution is **gated** and then **added**.\n",
    "- This avoids a long chain of multiplications like in vanilla RNNs.\n",
    "- So gradients **don’t vanish as easily** — useful information can persist much longer!\n",
    "\n",
    "---\n",
    "\n",
    "# 🎯 Why Use Sigmoid vs Tanh?\n",
    "\n",
    "## 🧩 Sigmoid: For Gates\n",
    "\n",
    "The sigmoid function:\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}} \\in (0, 1)\n",
    "$$\n",
    "\n",
    "Used in:\n",
    "- Forget gate: $f_t$\n",
    "- Input gate: $i_t$\n",
    "- Output gate: $o_t$\n",
    "\n",
    "### ✅ Reason:\n",
    "- Acts like a **soft switch**\n",
    "- Values near 0 block the signal; values near 1 let it pass\n",
    "- Perfect for **controlling flow**\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Tanh: For Memory & Signal\n",
    "\n",
    "The tanh function:\n",
    "\n",
    "$$\n",
    "\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} \\in (-1, 1)\n",
    "$$\n",
    "\n",
    "Used in:\n",
    "- Candidate memory: $\\tilde{c}_t$\n",
    "- Hidden state output: $h_t = o_t \\odot \\tanh(c_t)$\n",
    "\n",
    "### ✅ Reason:\n",
    "- Outputs centered around 0 (good for **expressiveness**)\n",
    "- Can encode **positive and negative** values\n",
    "- Keeps the activations **bounded**, which helps training stability\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 Summary Table\n",
    "\n",
    "| Component               | Activation | Purpose                                |\n",
    "|------------------------|------------|----------------------------------------|\n",
    "| Forget/Input/Output Gates | Sigmoid    | Soft control (0 to 1)                  |\n",
    "| Candidate Memory, Output | Tanh       | Rich signal (−1 to 1)                  |\n",
    "| Cell State Update        | Additive   | Preserves memory across time steps     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🧠 Gated Recurrent Units (GRUs)\n",
    "\n",
    "## Overview\n",
    "\n",
    "GRUs are a type of recurrent neural network that aim to solve the **vanishing gradient problem** found in vanilla RNNs. They do this using a simplified gating mechanism compared to LSTMs, while still enabling the network to retain or forget information over long sequences.\n",
    "\n",
    "GRUs are often faster to train than LSTMs due to fewer parameters and offer competitive performance.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔢 Key Equations\n",
    "\n",
    "Let:\n",
    "- $x_t$ = input at time $t$\n",
    "- $h_{t-1}$ = previous hidden state\n",
    "- $h_t$ = current hidden state\n",
    "- $\\sigma$ = sigmoid activation\n",
    "- $\\tanh$ = tanh activation\n",
    "- $\\odot$ = element-wise multiplication\n",
    "\n",
    "**1. Update Gate**\n",
    "\\[\n",
    "z_t = \\sigma(W_z x_t + U_z h_{t-1} + b_z)\n",
    "\\]\n",
    "\n",
    "**2. Reset Gate**\n",
    "\\[\n",
    "r_t = \\sigma(W_r x_t + U_r h_{t-1} + b_r)\n",
    "\\]\n",
    "\n",
    "**3. Candidate Hidden State**\n",
    "\\[\n",
    "\\tilde{h}_t = \\tanh(W_h x_t + U_h (r_t \\odot h_{t-1}) + b_h)\n",
    "\\]\n",
    "\n",
    "**4. Final Hidden State**\n",
    "\\[\n",
    "h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Intuition\n",
    "\n",
    "- The **update gate** $z_t$ decides how much of the new candidate state vs. the previous hidden state to keep.\n",
    "- The **reset gate** $r_t$ controls how much of the past hidden state to forget *when computing the candidate*.\n",
    "- The final hidden state $h_t$ is a **blend** of the previous hidden state and the new candidate, controlled by $z_t$.\n",
    "- This **additive structure** helps preserve gradient flow across time steps, avoiding vanishing gradients.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Why GRUs Help with Vanishing Gradients\n",
    "\n",
    "While GRUs do use **multiplicative gates**, it’s the **additive composition** of the final hidden state:\n",
    "\\[\n",
    "h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t\n",
    "\\]\n",
    "that allows gradients to flow through time steps **without excessive shrinking or blowing up**, unlike vanilla RNNs which rely on recursive multiplications.\n",
    "\n",
    "---\n",
    "\n",
    "# 🔁 Comparison Table: Vanilla RNN vs LSTM vs GRU\n",
    "\n",
    "| Feature                      | Vanilla RNN                    | LSTM                                          | GRU                                      |\n",
    "|-----------------------------|--------------------------------|-----------------------------------------------|------------------------------------------|\n",
    "| States                      | $h_t$                          | $h_t$, $c_t$ (hidden + cell)                  | $h_t$ only                               |\n",
    "| Gates                       | None                           | Input, Forget, Output                         | Update, Reset                            |\n",
    "| Update Equation             | $h_t = \\tanh(Wx_t + Uh_{t-1})$ | Complex gating with cell and hidden state     | Blended candidate and hidden state       |\n",
    "| Vanishing Gradient Handling | ❌                             | ✅ via cell state with additive updates       | ✅ via additive updates in $h_t$         |\n",
    "| Training Speed              | Fast                           | Slower due to more parameters                 | Faster than LSTM                         |\n",
    "| Parameter Count             | Low                            | High                                          | Medium                                   |\n",
    "| Use Case Suitability        | Short-term dependencies        | Long-term dependencies, large datasets        | Similar to LSTM but better for smaller tasks |\n",
    "| Output                      | $h_t$                          | $h_t$ (modulated by $o_t$ and $c_t$)          | $h_t$                                    |\n",
    "\n",
    "---\n",
    "\n",
    "# ✅ Summary\n",
    "\n",
    "- GRUs simplify LSTMs by combining the input and forget mechanisms into one **update gate**, and removing the separate cell state.\n",
    "- They perform similarly to LSTMs on many tasks while being faster to train and less likely to overfit on small datasets.\n",
    "- The key to avoiding vanishing gradients is the **additive blending** in the hidden state update.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
