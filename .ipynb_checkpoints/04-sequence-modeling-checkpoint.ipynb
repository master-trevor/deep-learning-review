{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔁 Recurrent Neural Networks (RNNs)\n",
    "\n",
    "## 📌 What is an RNN?\n",
    "\n",
    "A **Recurrent Neural Network (RNN)** is a type of neural network designed to process **sequential data**, such as text, time series, or speech.\n",
    "\n",
    "The key feature is a **hidden state** that carries information from previous time steps, enabling the network to have \"memory.\"\n",
    "\n",
    "---\n",
    "\n",
    "## 🔧 RNN Cell Update Formula\n",
    "\n",
    "At each time step \\( t \\), the hidden state is updated as:\n",
    "\n",
    "$$\n",
    "h_t = \\tanh(W_{xh} x_t + W_{hh} h_{t-1} + b_h)\n",
    "$$\n",
    "\n",
    "- \\( x_t \\): input at time step \\( t \\)\n",
    "- \\( h_{t-1} \\): hidden state from the previous step\n",
    "- \\( W_{xh} \\): input-to-hidden weights\n",
    "- \\( W_{hh} \\): hidden-to-hidden weights (shared across all time steps)\n",
    "- \\( b_h \\): bias\n",
    "- \\( \\tanh \\): non-linearity to keep activations bounded\n",
    "\n",
    "---\n",
    "\n",
    "## 🔄 Unrolling the RNN\n",
    "\n",
    "For a sequence of length \\( T \\), the RNN is \"unrolled\" like this:\n",
    "\n",
    "$$\n",
    "h_1 = \\tanh(W_{xh} x_1 + W_{hh} h_0 + b_h) \\\\\n",
    "h_2 = \\tanh(W_{xh} x_2 + W_{hh} h_1 + b_h) \\\\\n",
    "\\vdots \\\\\n",
    "h_T = \\tanh(W_{xh} x_T + W_{hh} h_{T-1} + b_h)\n",
    "$$\n",
    "\n",
    "This shows how the hidden state flows through time, accumulating knowledge from earlier inputs.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Key Concepts\n",
    "\n",
    "### ✅ Strengths:\n",
    "- Naturally handles variable-length sequences\n",
    "- Captures temporal patterns (e.g. language, stock prices)\n",
    "- Simple and intuitive architecture\n",
    "\n",
    "### ❌ Limitations:\n",
    "- **Vanishing gradients**: early inputs have negligible effect on output\n",
    "- **Exploding gradients**: gradients can become too large and destabilize training\n",
    "- **Slow training**: can't parallelize across time steps\n",
    "\n",
    "---\n",
    "\n",
    "## 📉 Vanishing & Exploding Gradients\n",
    "\n",
    "To compute the gradient of the loss \\( L \\) with respect to a past hidden state:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial h_{t-k}} = \\frac{\\partial L}{\\partial h_t} \\cdot \\prod_{i=1}^{k} \\frac{\\partial h_{t-i+1}}{\\partial h_{t-i}}\n",
    "$$\n",
    "\n",
    "### Vanishing:\n",
    "If each term in the product is \\( < 1 \\), the gradient shrinks:\n",
    "\n",
    "$$\n",
    "\\prod (0.5) \\cdot (0.5) \\cdot (0.5) = 0.125\n",
    "$$\n",
    "\n",
    "→ Early time steps have **near-zero gradient**, so they don't learn.\n",
    "\n",
    "### Exploding:\n",
    "If each term is \\( > 1 \\), the gradient grows:\n",
    "\n",
    "$$\n",
    "\\prod (5) \\cdot (5) \\cdot (5) = 125\n",
    "$$\n",
    "\n",
    "→ Early time steps have **huge gradient**, which can blow up weights.\n",
    "\n",
    "---\n",
    "\n",
    "## 🛡️ Fixes\n",
    "\n",
    "- **Gradient clipping** to prevent explosion:\n",
    "  ```python\n",
    "  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔐 Long Short-Term Memory (LSTM)\n",
    "\n",
    "## 📌 What is an LSTM?\n",
    "\n",
    "An **LSTM (Long Short-Term Memory)** is a type of recurrent neural network (RNN) designed to **learn long-term dependencies** more effectively than a basic RNN.\n",
    "\n",
    "It addresses the **vanishing gradient problem** by introducing a **cell state** and **gating mechanisms** that control the flow of information.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Core Components\n",
    "\n",
    "LSTM maintains:\n",
    "- A **hidden state** $h_t$ — short-term working memory\n",
    "- A **cell state** $c_t$ — long-term memory\n",
    "- **Gates** to manage memory updates\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ LSTM Cell Equations\n",
    "\n",
    "At time step $t$, with input $x_t$, previous hidden state $h_{t-1}$, and previous cell state $c_{t-1}$:\n",
    "\n",
    "### 🔒 Forget gate:\n",
    "$$\n",
    "f_t = \\sigma(W_f x_t + U_f h_{t-1} + b_f)\n",
    "$$\n",
    "\n",
    "### ➕ Input gate:\n",
    "$$\n",
    "i_t = \\sigma(W_i x_t + U_i h_{t-1} + b_i)\n",
    "$$\n",
    "\n",
    "### 🧠 Candidate memory:\n",
    "$$\n",
    "\\tilde{c}_t = \\tanh(W_c x_t + U_c h_{t-1} + b_c)\n",
    "$$\n",
    "\n",
    "### 🔁 Cell state update:\n",
    "$$\n",
    "c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t\n",
    "$$\n",
    "\n",
    "### 📤 Output gate:\n",
    "$$\n",
    "o_t = \\sigma(W_o x_t + U_o h_{t-1} + b_o)\n",
    "$$\n",
    "\n",
    "### 🧠 Hidden state update:\n",
    "$$\n",
    "h_t = o_t \\odot \\tanh(c_t)\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- $\\sigma$ is the sigmoid activation function  \n",
    "- $\\tanh$ is the hyperbolic tangent function  \n",
    "- $\\odot$ is element-wise multiplication\n",
    "\n",
    "---\n",
    "\n",
    "## 🧱 What Each Gate Does\n",
    "\n",
    "| Gate | Equation | Purpose |\n",
    "|------|----------|---------|\n",
    "| Forget Gate $f_t$ | $\\sigma(W_f x_t + U_f h_{t-1})$ | Decides what to forget from $c_{t-1}$ |\n",
    "| Input Gate $i_t$ | $\\sigma(W_i x_t + U_i h_{t-1})$ | Decides what new info to add |\n",
    "| Candidate $\\tilde{c}_t$ | $\\tanh(W_c x_t + U_c h_{t-1})$ | Proposes new memory |\n",
    "| Output Gate $o_t$ | $\\sigma(W_o x_t + U_o h_{t-1})$ | Decides what to output as $h_t$ |\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 Why LSTMs Solve the Vanishing Gradient Problem\n",
    "\n",
    "- The **cell state** $c_t$ enables gradients to flow **additively**, not multiplicatively, through time.\n",
    "- When $f_t \\approx 1$ and $i_t \\approx 0$, the cell simply carries forward unchanged:\n",
    "  $$\n",
    "  c_t \\approx c_{t-1}\n",
    "  $$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Activation Function Roles\n",
    "\n",
    "- **Sigmoid**: used in gates — outputs in \\( (0, 1) \\), perfect for gating/filtering\n",
    "- **Tanh**: used for candidate and output signal — outputs in \\( (-1, 1) \\), good for representing values with direction (positive or negative)\n",
    "\n",
    "| Function | Used For | Role |\n",
    "|----------|----------|------|\n",
    "| \\( \\sigma \\) (sigmoid) | Forget, Input, Output gates | Controls how much info flows |\n",
    "| \\( \\tanh \\) | Candidate memory, final hidden state | Represents information signal |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Summary\n",
    "\n",
    "- LSTM introduces a **cell state** that persists through time with minimal modification.\n",
    "- **Gates** allow the model to learn what to keep, forget, and output.\n",
    "- Fixes vanishing gradients by preserving important long-term dependencies.\n",
    "- Used heavily before transformers, still useful in speech, time series, and edge devices.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 💡 Why Gradients in LSTM Flow Additively\n",
    "\n",
    "The core LSTM cell state update equation is:\n",
    "\n",
    "$$\n",
    "c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t\n",
    "$$\n",
    "\n",
    "Let’s look at how this evolves over multiple time steps:\n",
    "\n",
    "### 🧮 Step-by-Step Cell State Flow\n",
    "\n",
    "Unrolling the cell across 3 time steps:\n",
    "\n",
    "- $c_1 = f_1 \\odot c_0 + i_1 \\odot \\tilde{c}_1$\n",
    "- $c_2 = f_2 \\odot c_1 + i_2 \\odot \\tilde{c}_2$\n",
    "- $c_3 = f_3 \\odot c_2 + i_3 \\odot \\tilde{c}_3$\n",
    "\n",
    "Now substitute $c_1$ and $c_2$ recursively:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "c_3 &= f_3 \\odot [f_2 \\odot (f_1 \\odot c_0 + i_1 \\odot \\tilde{c}_1) + i_2 \\odot \\tilde{c}_2] + i_3 \\odot \\tilde{c}_3 \\\\\n",
    "&= f_3 \\odot f_2 \\odot f_1 \\odot c_0 + f_3 \\odot f_2 \\odot i_1 \\odot \\tilde{c}_1 + f_3 \\odot i_2 \\odot \\tilde{c}_2 + i_3 \\odot \\tilde{c}_3\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### ✅ Key Insight\n",
    "\n",
    "- Each contribution is **gated** and then **added**.\n",
    "- This avoids a long chain of multiplications like in vanilla RNNs.\n",
    "- So gradients **don’t vanish as easily** — useful information can persist much longer!\n",
    "\n",
    "---\n",
    "\n",
    "# 🎯 Why Use Sigmoid vs Tanh?\n",
    "\n",
    "## 🧩 Sigmoid: For Gates\n",
    "\n",
    "The sigmoid function:\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}} \\in (0, 1)\n",
    "$$\n",
    "\n",
    "Used in:\n",
    "- Forget gate: $f_t$\n",
    "- Input gate: $i_t$\n",
    "- Output gate: $o_t$\n",
    "\n",
    "### ✅ Reason:\n",
    "- Acts like a **soft switch**\n",
    "- Values near 0 block the signal; values near 1 let it pass\n",
    "- Perfect for **controlling flow**\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Tanh: For Memory & Signal\n",
    "\n",
    "The tanh function:\n",
    "\n",
    "$$\n",
    "\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} \\in (-1, 1)\n",
    "$$\n",
    "\n",
    "Used in:\n",
    "- Candidate memory: $\\tilde{c}_t$\n",
    "- Hidden state output: $h_t = o_t \\odot \\tanh(c_t)$\n",
    "\n",
    "### ✅ Reason:\n",
    "- Outputs centered around 0 (good for **expressiveness**)\n",
    "- Can encode **positive and negative** values\n",
    "- Keeps the activations **bounded**, which helps training stability\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 Summary Table\n",
    "\n",
    "| Component               | Activation | Purpose                                |\n",
    "|------------------------|------------|----------------------------------------|\n",
    "| Forget/Input/Output Gates | Sigmoid    | Soft control (0 to 1)                  |\n",
    "| Candidate Memory, Output | Tanh       | Rich signal (−1 to 1)                  |\n",
    "| Cell State Update        | Additive   | Preserves memory across time steps     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🧠 Gated Recurrent Units (GRUs)\n",
    "\n",
    "## Overview\n",
    "\n",
    "GRUs are a type of recurrent neural network that aim to solve the **vanishing gradient problem** found in vanilla RNNs. They do this using a simplified gating mechanism compared to LSTMs, while still enabling the network to retain or forget information over long sequences.\n",
    "\n",
    "GRUs are often faster to train than LSTMs due to fewer parameters and offer competitive performance.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔢 Key Equations\n",
    "\n",
    "Let:\n",
    "- $x_t$ = input at time $t$\n",
    "- $h_{t-1}$ = previous hidden state\n",
    "- $h_t$ = current hidden state\n",
    "- $\\sigma$ = sigmoid activation\n",
    "- $\\tanh$ = tanh activation\n",
    "- $\\odot$ = element-wise multiplication\n",
    "\n",
    "**1. Update Gate**\n",
    "\\[\n",
    "$$z_t = \\sigma(W_z x_t + U_z h_{t-1} + b_z)$$\n",
    "\\]\n",
    "\n",
    "**2. Reset Gate**\n",
    "\\[\n",
    "$$r_t = \\sigma(W_r x_t + U_r h_{t-1} + b_r)$$\n",
    "\\]\n",
    "\n",
    "**3. Candidate Hidden State**\n",
    "\\[\n",
    "$$\\tilde{h}_t = \\tanh(W_h x_t + U_h (r_t \\odot h_{t-1}) + b_h)$$\n",
    "\\]\n",
    "\n",
    "**4. Final Hidden State**\n",
    "\\[\n",
    "$$h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t$$\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Intuition\n",
    "\n",
    "- The **update gate** $z_t$ decides how much of the new candidate state vs. the previous hidden state to keep.\n",
    "- The **reset gate** $r_t$ controls how much of the past hidden state to forget *when computing the candidate*.\n",
    "- The final hidden state $h_t$ is a **blend** of the previous hidden state and the new candidate, controlled by $z_t$.\n",
    "- This **additive structure** helps preserve gradient flow across time steps, avoiding vanishing gradients.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Why GRUs Help with Vanishing Gradients\n",
    "\n",
    "While GRUs do use **multiplicative gates**, it’s the **additive composition** of the final hidden state:\n",
    "\\[\n",
    "h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t\n",
    "\\]\n",
    "that allows gradients to flow through time steps **without excessive shrinking or blowing up**, unlike vanilla RNNs which rely on recursive multiplications.\n",
    "\n",
    "---\n",
    "\n",
    "# 🔁 Comparison Table: Vanilla RNN vs LSTM vs GRU\n",
    "\n",
    "| Feature                      | Vanilla RNN                    | LSTM                                          | GRU                                      |\n",
    "|-----------------------------|--------------------------------|-----------------------------------------------|------------------------------------------|\n",
    "| States                      | $h_t$                          | $h_t$, $c_t$ (hidden + cell)                  | $h_t$ only                               |\n",
    "| Gates                       | None                           | Input, Forget, Output                         | Update, Reset                            |\n",
    "| Update Equation             | $h_t = \\tanh(Wx_t + Uh_{t-1})$ | Complex gating with cell and hidden state     | Blended candidate and hidden state       |\n",
    "| Vanishing Gradient Handling | ❌                             | ✅ via cell state with additive updates       | ✅ via additive updates in $h_t$         |\n",
    "| Training Speed              | Fast                           | Slower due to more parameters                 | Faster than LSTM                         |\n",
    "| Parameter Count             | Low                            | High                                          | Medium                                   |\n",
    "| Use Case Suitability        | Short-term dependencies        | Long-term dependencies, large datasets        | Similar to LSTM but better for smaller tasks |\n",
    "| Output                      | $h_t$                          | $h_t$ (modulated by $o_t$ and $c_t$)          | $h_t$                                    |\n",
    "\n",
    "---\n",
    "\n",
    "# ✅ Summary\n",
    "\n",
    "- GRUs simplify LSTMs by combining the input and forget mechanisms into one **update gate**, and removing the separate cell state.\n",
    "- They perform similarly to LSTMs on many tasks while being faster to train and less likely to overfit on small datasets.\n",
    "- The key to avoiding vanishing gradients is the **additive blending** in the hidden state update.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding and Masking in Sequence Models\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "- Real-world sequences such as sentences or time-series data naturally have different lengths.\n",
    "- Neural networks, particularly when trained in batches, require fixed-size inputs.\n",
    "- **Padding** is used to bring all sequences to the same length, while **masking** tells the model which parts of the sequence are actual data versus padding.\n",
    "\n",
    "---\n",
    "\n",
    "## Padding Sequences\n",
    "\n",
    "When you pad a sequence, you add a special token (often represented by 0 or a token like `[PAD]`) to sequences shorter than the maximum length in a batch so that every sequence has the same length. For example, if you have three sequences:\n",
    "\n",
    "- Sequence 1: [1, 2, 3]\n",
    "- Sequence 2: [4, 5]\n",
    "- Sequence 3: [6, 7, 8, 9]\n",
    "\n",
    "After padding (to match the length 4, which is the longest), you get:\n",
    "\n",
    "- Padded Sequence 1: [1, 2, 3, 0]\n",
    "- Padded Sequence 2: [4, 5, 0, 0]\n",
    "- Padded Sequence 3: [6, 7, 8, 9]\n",
    "\n",
    "Here, 0 acts as the padding token.\n",
    "\n",
    "---\n",
    "\n",
    "## Creating a Mask\n",
    "\n",
    "A mask is created alongside the padded sequences to indicate which tokens are real data and which are padding. In the example above, the mask would be:\n",
    "\n",
    "- For Sequence 1: [1, 1, 1, 0]\n",
    "- For Sequence 2: [1, 1, 0, 0]\n",
    "- For Sequence 3: [1, 1, 1, 1]\n",
    "\n",
    "A '1' (or `True`) indicates a real token, while a '0' (or `False`) indicates a padded token. This mask is used during model computations (like in attention mechanisms or loss calculations) so that the padding does not affect the learning process.\n",
    "\n",
    "---\n",
    "\n",
    "## Optional: Packing for RNNs\n",
    "\n",
    "When working with RNNs (such as GRUs or LSTMs), it is sometimes beneficial to use packing techniques (e.g., using functions like `pack_padded_sequence` in PyTorch). Packing sequences allows the RNN to process only the real data without spending computation on the padded portions. This leads to a more efficient model training, as the model can ignore the padded timesteps altogether.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Term     | Purpose                                      |\n",
    "|----------|----------------------------------------------|\n",
    "| Padding  | Makes sequences the same length by adding tokens (e.g., 0 or [PAD]) to shorter sequences |\n",
    "| Masking  | Identifies which positions in the sequence are real data vs. padding, so that the model can ignore the padding during computation |\n",
    "| Packing  | (Optional) Groups valid data together for RNNs to avoid processing padding tokens, leading to efficient training |\n",
    "\n",
    "This approach—combining padding, masking, and optionally packing—ensures that variable-length sequences can be handled effectively by neural networks without contaminating the learning process with irrelevant padding information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🧱 Sequence Preprocessing: Padding, Masking, Tokenization & Special Tokens\n",
    "\n",
    "---\n",
    "\n",
    "## 📏 Managing Variable-Length Sequences\n",
    "\n",
    "### 🔹 Padding\n",
    "\n",
    "- Sequences (like sentences) are rarely the same length.\n",
    "- Neural networks, especially in batched training, require fixed-size inputs.\n",
    "- Padding is used to make all sequences the same length by appending a special token (commonly `0` or `[PAD]`).\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Raw tokenized sequences:  \n",
    "[5, 3, 7]  \n",
    "[12, 5]  \n",
    "[15, 8, 9, 4]  \n",
    "\n",
    "After padding to max length 4:  \n",
    "[5, 3, 7, 0]  \n",
    "[12, 5, 0, 0]  \n",
    "[15, 8, 9, 4]  \n",
    "\n",
    "---\n",
    "\n",
    "### 🔸 Masking\n",
    "\n",
    "- Padding introduces fake data — we don’t want the model to learn from that.\n",
    "- A **mask** is created to indicate which tokens are real (1) and which are padding (0).\n",
    "\n",
    "**Example Mask:**  \n",
    "[1, 1, 1, 0]  \n",
    "[1, 1, 0, 0]  \n",
    "[1, 1, 1, 1]  \n",
    "\n",
    "Masks are used in:  \n",
    "- Attention layers (to block out padding)\n",
    "- Loss calculations (to ignore padded tokens)\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Tokenization Methods\n",
    "\n",
    "Transformers operate on **token IDs**, not raw text. Tokenization turns raw text into smaller units called tokens.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Word-Level Tokenization\n",
    "\n",
    "- Splits text by spaces or punctuation.\n",
    "- Easy but can't handle rare or unknown words.\n",
    "- Example:  \n",
    "  `\"unhappiness\"` → `[\"unhappiness\"]`\n",
    "\n",
    "---\n",
    "\n",
    "### 🔸 Character-Level Tokenization\n",
    "\n",
    "- Breaks text into individual characters.\n",
    "- Always works (no OOV), but loses word-level semantics and creates long sequences.  \n",
    "  `\"hello\"` → `[\"h\", \"e\", \"l\", \"l\", \"o\"]`\n",
    "\n",
    "---\n",
    "\n",
    "### 🔶 Subword Tokenization (used in BERT, GPT, T5, etc.)\n",
    "\n",
    "Breaks rare or complex words into more frequent, reusable **subword units**.\n",
    "\n",
    "---\n",
    "\n",
    "#### 🧩 WordPiece (Used in BERT)\n",
    "\n",
    "- Learns subwords that maximize likelihood of training data.\n",
    "- Adds `##` to mark subword continuations.\n",
    "- Example:  \n",
    "  `\"unhappiness\"` → `[\"un\", \"##happi\", \"##ness\"]`\n",
    "\n",
    "---\n",
    "\n",
    "#### 🔗 Byte-Pair Encoding (BPE) (Used in GPT-2, GPT-3)\n",
    "\n",
    "- Inspired by compression algorithms.\n",
    "- Starts with characters, then merges most frequent **adjacent pairs**.\n",
    "- Example:  \n",
    "  `\"blockchainbro\"` → `[\"block\", \"chain\", \"bro\"]`  \n",
    "  `\"unhappiness\"` → `[\"un\", \"happiness\"]` or `[\"un\", \"happi\", \"ness\"]`\n",
    "\n",
    "---\n",
    "\n",
    "#### 📦 SentencePiece (Used in T5, mT5)\n",
    "\n",
    "- Trained directly on raw text (no whitespace splitting).\n",
    "- Useful for multilingual models or languages without spaces.\n",
    "- Operates at the Unicode level.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 Special Tokens in Transformer Models\n",
    "\n",
    "| Token     | Purpose                                                                 |\n",
    "|-----------|-------------------------------------------------------------------------|\n",
    "| `[PAD]`   | Padding token; ignored by model and attention                           |\n",
    "| `[MASK]`  | Used during masked language modeling (BERT pretraining)                 |\n",
    "| `[CLS]`   | Classification token; summary vector for sentence(s)                    |\n",
    "| `[SEP]`   | Separator token between sentences or segments                           |\n",
    "| `[UNK]`   | Unknown token (used when subword split is impossible — rare in practice) |\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 Example: BERT Input Format\n",
    "\n",
    "Suppose you're inputting two sentences for classification:\n",
    "\n",
    "- Sentence A: \"I love AI.\"\n",
    "- Sentence B: \"It is powerful.\"\n",
    "\n",
    "**BERT Input Tokens (structured format):**  \n",
    "`[CLS] I love AI . [SEP] It is powerful . [SEP]`\n",
    "\n",
    "- `[CLS]` goes at the start and its final embedding is used for classification.\n",
    "- `[SEP]` separates Sentence A and B.\n",
    "- These tokens are then tokenized into subwords using WordPiece and mapped to **input IDs**.\n",
    "\n",
    "---\n",
    "\n",
    "### Behind the scenes, the BERT tokenizer outputs:\n",
    "\n",
    "- `input_ids`: Token IDs including `[CLS]`, `[SEP]`, subwords, etc.\n",
    "- `token_type_ids`: 0s for Sentence A, 1s for Sentence B\n",
    "- `attention_mask`: 1s for real tokens, 0s for padding\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Summary\n",
    "\n",
    "| Concept              | Purpose                                                                 |\n",
    "|----------------------|-------------------------------------------------------------------------|\n",
    "| Padding              | Aligns sequence lengths for batch processing                            |\n",
    "| Masking              | Prevents model from learning from padded tokens                         |\n",
    "| Word-Level Tokenization | Splits by word, simple but limited                                    |\n",
    "| Character-Level      | No OOV, but loses structure                                              |\n",
    "| Subword Tokenization | Breaks into reusable chunks, balances vocab size and coverage           |\n",
    "| WordPiece            | BERT-style subwords with `##` continuation markers                       |\n",
    "| BPE                  | GPT-style subword merges based on frequency of adjacent character pairs |\n",
    "| SentencePiece        | Raw-text trained, good for multilingual or no-space languages           |\n",
    "| `[PAD]`, `[MASK]`, `[CLS]`, `[SEP]` | Special tokens that signal padding, masking, classification, and segmentation |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
