{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“˜ Transfer Learning & Word Embedding Practice Notebook\n",
    "# Works 100% on CPU with Anaconda + Jupyter + minimal setup\n",
    "\n",
    "# ======================\n",
    "# ðŸ“¦ 0. Setup + Imports\n",
    "# ======================\n",
    "!pip install torch torchvision torchaudio --quiet\n",
    "!pip install transformers gensim --quiet\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Section 1: Embedding Layer Basics\n",
      "\n",
      "Token IDs: [1, 2, 3]\n",
      "Embeddings: tensor([[[-0.3462, -0.9656, -0.6613, -1.1851, -0.4164],\n",
      "         [-0.6087,  0.7011, -0.3392,  0.3363, -1.2427],\n",
      "         [-0.3030,  1.0460,  1.0224,  0.9657, -0.8577]]],\n",
      "       grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# ðŸ“˜ 1. Learnable Embeddings\n",
    "# ==========================\n",
    "print(\"\\nðŸ”¹ Section 1: Embedding Layer Basics\\n\")\n",
    "\n",
    "# Define vocab and toy tokenizer\n",
    "vocab = {\"[PAD]\": 0, \"hello\": 1, \"world\": 2, \"bert\": 3, \"rocks\": 4}\n",
    "inv_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "def tokenize(text):\n",
    "    return [vocab.get(token, 0) for token in text.lower().split()]\n",
    "\n",
    "# TODO: Change this sentence to test different token combos\n",
    "sentence = \"hello world bert\"\n",
    "token_ids = tokenize(sentence)\n",
    "tokens_tensor = torch.tensor(token_ids).unsqueeze(0)  # batch size 1\n",
    "\n",
    "# Embedding layer\n",
    "embedding = nn.Embedding(num_embeddings=len(vocab), embedding_dim=5)\n",
    "embedded_output = embedding(tokens_tensor)\n",
    "\n",
    "print(\"Token IDs:\", token_ids)\n",
    "print(\"Embeddings:\", embedded_output)\n",
    "\n",
    "# Mini challenge: Try printing the shape of `embedded_output`\n",
    "# Mini challenge: Try adding another sentence and compare output shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "t = torch.tensor([1, 2, 3])\n",
    "print(t.shape)  # torch.Size([3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n",
      "tensor([[1, 2, 3]])\n"
     ]
    }
   ],
   "source": [
    "print(t)\n",
    "print(t.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# ðŸ“˜ 2. Pretrained GloVe Embeddings (via gensim)\n",
    "# ==========================\n",
    "print(\"\\nðŸ”¹ Section 2: GloVe Embeddings\\n\")\n",
    "\n",
    "from gensim.downloader import load\n",
    "word_vectors = load(\"glove-wiki-gigaword-50\")  # 50d for speed\n",
    "\n",
    "# TODO: Try checking similarity between different word pairs\n",
    "print(\"Similarity between king and queen:\", word_vectors.similarity(\"king\", \"queen\"))\n",
    "print(\"Most similar to 'neural':\", word_vectors.most_similar(\"neural\", topn=3))\n",
    "\n",
    "# Mini challenge: Try glove.most_similar(positive=['woman', 'king'], negative=['man'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# ðŸ“˜ 3. BERT Feature Extraction\n",
    "# ==========================\n",
    "print(\"\\nðŸ”¹ Section 3: BERT as Feature Extractor\\n\")\n",
    "\n",
    "# Load BERT base model\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# TODO: Change the sentence to test different contexts\n",
    "inputs = tokenizer(\"The cat sat on the mat.\", return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Extract CLS embedding\n",
    "cls_embedding = outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
    "print(\"[CLS] embedding shape:\", cls_embedding.shape)\n",
    "\n",
    "# Mini challenge: Extract the embedding for a specific word token (e.g., \"cat\")\n",
    "# Mini challenge: Try two different sentences and compare [CLS] embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# ðŸ“˜ 4. Mini Fine-Tuning Example (Optional CPU-safe)\n",
    "# ==========================\n",
    "print(\"\\nðŸ”¹ Section 4: Mini Fine-Tune Setup\\n\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# TODO: Add your own text examples below to try different classes\n",
    "inputs = tokenizer([\"This is great!\", \"This is terrible!\"], padding=True, return_tensors=\"pt\")\n",
    "labels = torch.tensor([1, 0])\n",
    "\n",
    "# Forward pass + loss\n",
    "outputs = model(**inputs, labels=labels)\n",
    "loss = outputs.loss\n",
    "print(\"Loss from dummy classification task:\", loss.item())\n",
    "\n",
    "# Mini challenge: Flip the labels and see what happens to the loss\n",
    "# Mini challenge: Try freezing BERT and only training the classifier\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# ðŸ“˜ 5. Freezing Layers + Visualizing\n",
    "# ==========================\n",
    "print(\"\\nðŸ”¹ Section 5: Freezing BERT Layers\\n\")\n",
    "\n",
    "# Freeze everything\n",
    "for param in model.bert.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# TODO: Try unfreezing only first N layers instead of last 2\n",
    "for layer in model.bert.encoder.layer[-2:]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "# Count trainable params\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable parameters: {trainable}/{total}\")\n",
    "\n",
    "# Mini challenge: Try freezing/unfreezing different blocks and track parameter count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# ðŸ“˜ 6. Simulate LLRD Setup\n",
    "# ==========================\n",
    "print(\"\\nðŸ”¹ Section 6: Layer-wise Learning Rates (Simulated)\\n\")\n",
    "\n",
    "base_lr = 2e-5\n",
    "decay = 0.9\n",
    "optim_groups = []\n",
    "\n",
    "# TODO: Try changing decay rate or base_lr to see the impact\n",
    "for i, layer in enumerate(model.bert.encoder.layer):\n",
    "    lr = base_lr * (decay ** (11 - i))\n",
    "    optim_groups.append({\"params\": layer.parameters(), \"lr\": lr})\n",
    "    print(f\"Layer {i}: LR = {lr:.6f}\")\n",
    "\n",
    "# Add classifier head with higher LR\n",
    "optim_groups.append({\"params\": model.classifier.parameters(), \"lr\": base_lr * 2})\n",
    "\n",
    "print(\"\\nSimulated optimizer groups created with LLRD-style scaling.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
