{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Attention Mechanisms ‚Äî Core Concepts & Intuition\n",
    "\n",
    "---\n",
    "\n",
    "## üí° What Is Attention?\n",
    "\n",
    "Attention lets each token in a sequence decide **which other tokens to pay attention to**, and **how much**, when constructing its contextualized representation.\n",
    "\n",
    "Unlike RNNs, attention is not sequential ‚Äî every token **attends to all tokens in parallel**, including itself.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ The Goal\n",
    "\n",
    "For each token in a sequence:  \n",
    "> ‚ÄúWhat other tokens should I care about, and how much should their information contribute to my final vector?‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ Q, K, V: Query, Key, and Value\n",
    "\n",
    "Given an input sequence of token embeddings:\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "\\vdots \\\\\n",
    "x_n\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{n \\times d_{\\text{model}}}\n",
    "$$\n",
    "\n",
    "We compute:\n",
    "\n",
    "- **Queries**: $Q = X W^Q$\n",
    "- **Keys**: $K = X W^K$\n",
    "- **Values**: $V = X W^V$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $W^Q, W^K, W^V \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}$ are **learned projection matrices**\n",
    "- Each token embedding is linearly projected into 3 spaces:\n",
    "  - **Query** = ‚ÄúWhat am I looking for?‚Äù\n",
    "  - **Key** = ‚ÄúWhat do I offer?‚Äù\n",
    "  - **Value** = ‚ÄúWhat content should you use from me?‚Äù\n",
    "\n",
    "‚úÖ This part is critical: Q, K, and V are **not the embeddings themselves**, but **learned views** of them based on the task.\n",
    "\n",
    "---\n",
    "\n",
    "## üìê Scaled Dot-Product Attention Formula\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right) V\n",
    "$$\n",
    "\n",
    "This formula happens in **3 core steps**:\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: Similarity Scoring ‚Äî $QK^\\top$\n",
    "\n",
    "This is where each token‚Äôs **query** is compared to every other token‚Äôs **key** using a dot product:\n",
    "\n",
    "- The output is a matrix $\\in \\mathbb{R}^{n \\times n}$\n",
    "- Entry $(i, j)$ is the similarity between:\n",
    "  - Token $i$‚Äôs **query**\n",
    "  - Token $j$‚Äôs **key**\n",
    "\n",
    "‚úÖ This is **exactly where you interpret how much attention token $i$ gives to token $j$**.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Scaling\n",
    "\n",
    "Divide each score by $\\sqrt{d_k}$ to prevent large dot products from dominating softmax.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Softmax ‚Äî Normalizing to Attention Weights\n",
    "\n",
    "Apply softmax across each row:\n",
    "\n",
    "- Turns raw scores into probabilities (weights between 0 and 1)\n",
    "- Each row now sums to 1\n",
    "- Row $i$ gives a **distribution over how much token $i$ should attend to all tokens (including itself)**\n",
    "\n",
    "‚úÖ This softmax output is the **attention weight matrix** ‚Äî it holds the interpretable attention values.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Weighted Sum ‚Äî Multiply with V\n",
    "\n",
    "$$\n",
    "\\text{Output} = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right) V\n",
    "$$\n",
    "\n",
    "- Multiply attention weights by value vectors\n",
    "- This gives a new vector for each token ‚Äî a **blend** of all token values, weighted by how relevant they are\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Intuition Summary\n",
    "\n",
    "| Role   | Meaning                                                   |\n",
    "|--------|-----------------------------------------------------------|\n",
    "| Query  | What am I looking for?                                    |\n",
    "| Key    | What do I offer?                                          |\n",
    "| Value  | What content should you use if you care about me?         |\n",
    "| $Q \\cdot K^\\top$ | How well does one token match another                     |\n",
    "| $\\text{softmax}(QK^\\top)$ | Attention weights ‚Äî how much focus to give each token |\n",
    "| Output | Contextualized vector based on others‚Äô values             |\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Additional Insights\n",
    "\n",
    "- **Self-attention** = Q, K, V all come from the same input\n",
    "- **Cross-attention** = Q comes from one input, K/V from another (e.g., encoder-decoder setup)\n",
    "- **Attention is fully differentiable** and learned via backprop\n",
    "- The attention output has the **same shape** as the input, so it can be combined via residual connections\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ This Is Where Interpretation Happens:\n",
    "\n",
    "> The attention **weights** (after softmax of $QK^\\top$) are **directly interpretable** as:  \n",
    "> ‚ÄúHow much should this token care about each other token (including itself)?‚Äù\n",
    "\n",
    "That matrix holds the **core meaning** of attention.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Single-Head vs Multi-Head Attention ‚Äî Dimensions, Design, and Intuition\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ Single-Head (Standard) Attention\n",
    "\n",
    "In standard attention (no heads), we work directly with the full embedding dimension:\n",
    "\n",
    "Let:\n",
    "- $X \\in \\mathbb{R}^{n \\times d_{\\text{model}}}$ ‚Äî input sequence\n",
    "- $n$: number of tokens in sequence\n",
    "- $d_{\\text{model}}$: embedding size per token\n",
    "\n",
    "We compute:\n",
    "\n",
    "- $Q = X W^Q$, $K = X W^K$, $V = X W^V$\n",
    "- Where each projection matrix $W^Q, W^K, W^V \\in \\mathbb{R}^{d_{\\text{model}} \\times d_{\\text{model}}}$\n",
    "\n",
    "So:\n",
    "- $Q, K, V \\in \\mathbb{R}^{n \\times d_{\\text{model}}}$\n",
    "- Meaning:\n",
    "  $$\n",
    "  d_k = d_v = d_{\\text{model}}\n",
    "  $$\n",
    "\n",
    "‚úÖ We retain the full expressiveness of each token's embedding for computing attention.\n",
    "\n",
    "---\n",
    "\n",
    "## üîÄ Multi-Head Attention\n",
    "\n",
    "We split attention into $h$ **parallel heads**, each operating in a smaller subspace.\n",
    "\n",
    "We set:\n",
    "$$\n",
    "d_k = d_v = \\frac{d_{\\text{model}}}{h}\n",
    "$$\n",
    "\n",
    "Each head computes:\n",
    "\n",
    "$$\n",
    "Q_i = X W_i^Q,\\quad K_i = X W_i^K,\\quad V_i = X W_i^V\n",
    "\\quad \\text{with} \\quad W_i^Q, W_i^K, W_i^V \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}\n",
    "$$\n",
    "\n",
    "Each head returns:\n",
    "\n",
    "$$\n",
    "\\text{head}_i = \\text{softmax}\\left( \\frac{Q_i K_i^\\top}{\\sqrt{d_k}} \\right) V_i \\in \\mathbb{R}^{n \\times d_v}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### üîß Concatenation and Final Projection\n",
    "\n",
    "All heads are concatenated:\n",
    "$$\n",
    "\\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h) \\in \\mathbb{R}^{n \\times (h \\cdot d_v)} = \\mathbb{R}^{n \\times d_{\\text{model}}}\n",
    "$$\n",
    "\n",
    "Then passed through a final linear projection:\n",
    "$$\n",
    "\\text{Output} = \\text{Concat}(\\dots) \\cdot W^O,\\quad W^O \\in \\mathbb{R}^{d_{\\text{model}} \\times d_{\\text{model}}}\n",
    "$$\n",
    "\n",
    "‚úÖ Final output shape: $n \\times d_{\\text{model}}$\n",
    "\n",
    "---\n",
    "\n",
    "## ü§î Why Set $d_k = d_v = \\frac{d_{\\text{model}}}{h}$?\n",
    "\n",
    "- Keeps computation per head low\n",
    "- Preserves total output size after concatenation\n",
    "- Enables diversity across heads (each learns a different ‚Äúview‚Äù)\n",
    "- Helps avoid overfitting by specializing heads\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ùå What if $d_k$ or $d_v$ were too small?\n",
    "\n",
    "- $d_k$ too small ‚Üí poor matching resolution ‚Üí queries can't ‚Äúfind‚Äù relevant keys effectively\n",
    "- $d_v$ too small ‚Üí limited content to pass into final representation\n",
    "- Bottom line: shrinking Q/K/V dimensions **too much** causes **information loss**\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "\n",
    "| Concept                     | Single-Head                     | Multi-Head                              |\n",
    "|-----------------------------|----------------------------------|------------------------------------------|\n",
    "| Q/K/V dimension             | $d_{\\text{model}}$               | $\\frac{d_{\\text{model}}}{h}$             |\n",
    "| Output shape                | $n \\times d_{\\text{model}}$     | $n \\times d_{\\text{model}}$ (via concat + proj) |\n",
    "| Why use multiple heads?     | N/A                              | Captures different types of attention in parallel |\n",
    "| Final projection?           | No                               | Yes ‚Äî to mix head outputs               |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìç Positional Information in Transformers ‚Äî Complete Guide\n",
    "\n",
    "This note summarizes all the key concepts, comparisons, and implementations for encoding positional information into transformer models. It includes positional **embeddings**, **encodings**, and **rotary embeddings (RoPE)**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ Why We Need Positional Information\n",
    "\n",
    "Transformers process tokens in parallel, meaning they have **no inherent sense of token order**. Positional information must be added explicitly so that:\n",
    "\n",
    "- Tokens can be aware of **absolute position** (e.g., beginning, middle, end)\n",
    "- Attention can reflect **relative distance** (e.g., \"how far apart\")\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Positional Embeddings (Learned)\n",
    "\n",
    "- Each position (0, 1, 2, ...) is assigned a **learnable vector** of size $d_{\\text{model}}$\n",
    "- Looked up from a table and **added to token embeddings** before the first transformer layer\n",
    "- Example: `input = token_embedding + position_embedding`\n",
    "\n",
    "### ‚úÖ Pros:\n",
    "- Task-specific optimization (learned from data)\n",
    "- Compatible with any tokenizer and model\n",
    "\n",
    "### ‚ùå Cons:\n",
    "- Does **not generalize** to longer sequences\n",
    "- Positional meaning is **not interpretable**\n",
    "\n",
    "---\n",
    "\n",
    "## üî¢ Sinusoidal Positional Encodings (Fixed)\n",
    "\n",
    "### Formula:\n",
    "For position $ \\text{pos} $ and dimension $ i $:\n",
    "\n",
    "If $ i $ is even:\n",
    "\n",
    "$$\n",
    "\\text{PE}_{\\text{pos}, i} = \\sin\\left(\\frac{\\text{pos}}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right)\n",
    "$$\n",
    "\n",
    "If $ i $ is odd:\n",
    "\n",
    "$$\n",
    "\\text{PE}_{\\text{pos}, i} = \\cos\\left(\\frac{\\text{pos}}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "- Injected **by addition** to token embeddings\n",
    "- Dimensions vary by **frequency**, allowing **multi-scale position awareness**\n",
    "\n",
    "### ‚úÖ Pros:\n",
    "- Generalizes to **arbitrarily long sequences**\n",
    "- Smooth, interpretable, and deterministic\n",
    "\n",
    "### ‚ùå Cons:\n",
    "- Not task-optimized\n",
    "- Position only affects **input embeddings**, not attention directly\n",
    "\n",
    "---\n",
    "\n",
    "## üåÄ Rotary Positional Embeddings (RoPE)\n",
    "\n",
    "RoPE encodes position **inside the attention mechanism itself** by rotating Q and K vectors.\n",
    "\n",
    "### Core Idea:\n",
    "- Split each Q/K vector into **2D pairs**\n",
    "- Rotate each pair by an angle:\n",
    "\n",
    "$$\n",
    "\\theta_i = \\frac{\\text{pos}}{10000^{\\frac{2i}{d_{\\text{model}}}}}\n",
    "$$\n",
    "\n",
    "\n",
    "- Rotation uses a 2D matrix:\n",
    "\n",
    "$$\n",
    "R(\\theta) =\n",
    "\\begin{bmatrix}\n",
    "\\cos(\\theta) & -\\sin(\\theta) \\\\\n",
    "\\sin(\\theta) & \\cos(\\theta)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "### üîÅ Multi-scale Encoding:\n",
    "- **Slow-rotating** pairs capture **global** position\n",
    "- **Fast-rotating** pairs capture **fine-grained** distances\n",
    "\n",
    "### ‚úÖ Pros:\n",
    "- Encodes **relative position** through dot products\n",
    "- Generalizes to **longer sequences** naturally\n",
    "- No extra memory or learnable parameters\n",
    "- Lightweight and fused into attention calculation\n",
    "\n",
    "### ‚ùå Cons:\n",
    "- Rotation must be manually implemented per 2D pair\n",
    "\n",
    "### üîç Real Output Example:\n",
    "Dot product between Q vectors after RoPE (simulated):\n",
    "\n",
    "| Q\\_pos=0 vs | Dot Product |\n",
    "|------------|-------------|\n",
    "| Q\\_pos=0    | 1.35         |\n",
    "| Q\\_pos=1    | 0.89         |\n",
    "| Q\\_pos=2    | -0.07        |\n",
    "| Q\\_pos=3    | -0.65        |\n",
    "\n",
    "> Demonstrates how attention degrades smoothly with increasing distance via rotation.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Final Summary\n",
    "\n",
    "| Method                   | Type     | Generalizes | Relative Awareness | Added to Embedding? | Interpretable? |\n",
    "|--------------------------|----------|-------------|---------------------|----------------------|-----------------|\n",
    "| Learned Pos Embedding    | Learned  | ‚ùå          | ‚ùå                  | ‚úÖ                   | ‚ùå              |\n",
    "| Sinusoidal Encoding      | Fixed    | ‚úÖ          | ‚úÖ (with effort)     | ‚úÖ                   | ‚úÖ              |\n",
    "| Rotary Embedding (RoPE)  | Fixed    | ‚úÖ          | ‚úÖ (by design)       | ‚ùå (used in Q/K)     | ‚úÖ (via rotation) |\n",
    "\n",
    "RoPE gives the most **elegant and functional** way to encode position **without any addition to embeddings**, directly into **how tokens attend to each other**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¢ Sinusoidal Positional Encoding ‚Äî Simple Toy Example (d_model = 4)\n",
    "\n",
    "### ‚ùó Key Reminder:\n",
    "- **`pos`** = the position of the token in the sequence (e.g., 0, 1, 2, ...)\n",
    "- **`i`** = the dimension index of the embedding (0 through d_model - 1)\n",
    "- ‚úÖ **`i ‚â† pos`** ‚Äî they are totally separate axes\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Setup:\n",
    "- Sequence length = 2 (positions 0 and 1)\n",
    "- Embedding size = 4\n",
    "\n",
    "Formulas:\n",
    "\n",
    "For even $i$:\n",
    "\n",
    "$$\n",
    "\\text{PE}(\\text{pos}, i) = \\sin\\left(\\frac{\\text{pos}}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right)\n",
    "$$\n",
    "\n",
    "For odd $i$:\n",
    "\n",
    "$$\n",
    "\\text{PE}(\\text{pos}, i) = \\cos\\left(\\frac{\\text{pos}}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## üìç Position 0:\n",
    "\n",
    "- $\\text{PE}(0, 0) = \\sin(0) = 0$\n",
    "- $\\text{PE}(0, 1) = \\cos(0) = 1$\n",
    "- $\\text{PE}(0, 2) = \\sin(0) = 0$\n",
    "- $\\text{PE}(0, 3) = \\cos(0) = 1$\n",
    "\n",
    "**Resulting vector:**  \n",
    "`[0.0, 1.0, 0.0, 1.0]`\n",
    "\n",
    "---\n",
    "\n",
    "## üìç Position 1:\n",
    "\n",
    "- For $i = 0, 1$: $\\frac{1}{10000^{0}} = 1$\n",
    "- For $i = 2, 3$: $\\frac{1}{10000^{0.5}} = \\frac{1}{\\sqrt{10000}} = 0.01$\n",
    "\n",
    "- $\\text{PE}(1, 0) = \\sin(1) \\approx 0.841$\n",
    "- $\\text{PE}(1, 1) = \\cos(1) \\approx 0.540$\n",
    "- $\\text{PE}(1, 2) = \\sin(0.01) \\approx 0.010$\n",
    "- $\\text{PE}(1, 3) = \\cos(0.01) \\approx 0.999$\n",
    "\n",
    "**Resulting vector:**  \n",
    "`[0.841, 0.540, 0.010, 0.999]`\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Final PE Matrix (2 positions √ó 4 dimensions):\n",
    "\n",
    "| Position | dim 0 | dim 1 | dim 2 | dim 3 |\n",
    "|----------|--------|--------|--------|--------|\n",
    "| 0        | 0.000  | 1.000  | 0.000  | 1.000  |\n",
    "| 1        | 0.841  | 0.540  | 0.010  | 0.999  |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üåÄ Rotary Positional Embedding (RoPE) ‚Äî Toy Example + Math Explained\n",
    "\n",
    "### ‚ùó Key Points\n",
    "\n",
    "- RoPE applies **rotations directly to Q and K vectors** in attention\n",
    "- It does **not** modify the input embeddings ‚Äî the position is injected **within the attention mechanism**\n",
    "- Each token's Q/K vector is split into **2D subvectors** and rotated using a position-dependent angle\n",
    "- Different 2D subvectors rotate at **different frequencies** (multi-scale)\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Setup: 1 token vector of dimension 4\n",
    "\n",
    "Let:\n",
    "- Token position = 1\n",
    "- Vector = `[x‚ÇÄ, x‚ÇÅ, x‚ÇÇ, x‚ÇÉ] = [1.0, 0.0, 0.5, 0.0]`\n",
    "- We split it into two 2D subvectors: `[1.0, 0.0]` and `[0.5, 0.0]`\n",
    "- Let $d_{\\text{model}} = 4$\n",
    "\n",
    "We will rotate each subvector by an angle:\n",
    "\n",
    "$$\n",
    "\\theta_i = \\frac{\\text{pos}}{10000^{\\frac{2i}{d_{\\text{model}}}}}\n",
    "$$\n",
    "\n",
    "So for:\n",
    "- Pair 0 ($i=0$): $\\theta_0 = \\frac{1}{10000^0} = 1.0$\n",
    "- Pair 1 ($i=1$): $\\theta_1 = \\frac{1}{10000^{0.5}} = 0.01$\n",
    "\n",
    "---\n",
    "\n",
    "### üìê Rotation Matrix\n",
    "\n",
    "Each 2D vector is rotated using:\n",
    "\n",
    "$$\n",
    "R(\\theta) =\n",
    "\\begin{bmatrix}\n",
    "\\cos(\\theta) & -\\sin(\\theta) \\\\\n",
    "\\sin(\\theta) & \\cos(\\theta)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### üßÆ Applying RoPE\n",
    "\n",
    "For subvector 1: $[1.0,\\ 0.0]$ with $\\theta = 1.0$\n",
    "\n",
    "$$\n",
    "\\text{RoPE}_0 = R(1.0) \\cdot\n",
    "\\begin{bmatrix}\n",
    "1.0 \\\\\n",
    "0.0\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\cos(1.0) \\cdot 1.0 + (-\\sin(1.0)) \\cdot 0.0 \\\\\n",
    "\\sin(1.0) \\cdot 1.0 + \\cos(1.0) \\cdot 0.0\n",
    "\\end{bmatrix}\n",
    "\\approx\n",
    "\\begin{bmatrix}\n",
    "0.540 \\\\\n",
    "0.841\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "For subvector 2: $[0.5,\\ 0.0]$ with $\\theta = 0.01$\n",
    "\n",
    "$$\n",
    "\\text{RoPE}_1 = R(0.01) \\cdot\n",
    "\\begin{bmatrix}\n",
    "0.5 \\\\\n",
    "0.0\n",
    "\\end{bmatrix}\n",
    "\\approx\n",
    "\\begin{bmatrix}\n",
    "0.5 \\cdot \\cos(0.01) \\\\\n",
    "0.5 \\cdot \\sin(0.01)\n",
    "\\end{bmatrix}\n",
    "\\approx\n",
    "\\begin{bmatrix}\n",
    "0.4999 \\\\\n",
    "0.005\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Final Rotated Vector:\n",
    "\n",
    "Concatenate the two rotated subvectors:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîÅ Positional Information in Transformers\n",
    "\n",
    "## ‚úÖ Why Do We Need Positional Information?\n",
    "\n",
    "Transformers lack recurrence or convolution, so they process tokens **in parallel**.  \n",
    "To understand **order**, we inject positional information into the model.\n",
    "\n",
    "---\n",
    "\n",
    "## üî∑ Types of Positional Information\n",
    "\n",
    "| Method                         | Category      | Generalizes Beyond Training Length? | Notes |\n",
    "|-------------------------------|---------------|------------------------|-------|\n",
    "| Learned Positional Embeddings | Absolute       | ‚ùå No | Fixed-size lookup table; 1 vector per position |\n",
    "| Sinusoidal Encoding           | Absolute       | ‚úÖ Yes | Uses trigonometric functions to encode positions |\n",
    "| Relative Position Bias (T5, DeBERTa) | Relative       | ‚úÖ Yes | Focuses on token distance, not exact location |\n",
    "| RoPE (Rotary Positional Embeddings) | ‚úÖ Hybrid (Abs + Rel) | ‚úÖ Yes | Injects position into attention via rotations |\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Absolute Positional Encodings\n",
    "\n",
    "- Encode the **exact position** of each token (e.g., \"token 5\").\n",
    "- Either **learned** (lookup table) or **fixed** (sin/cos functions).\n",
    "- Cannot reason about *distances between tokens* directly.\n",
    "\n",
    "---\n",
    "\n",
    "## üî∏ Relative Positional Encodings\n",
    "\n",
    "- Encode **distances** between tokens (e.g., \"token A is 3 steps before B\").\n",
    "- More robust to repeated structures, generalized patterns (e.g., music, DNA).\n",
    "- Often used as bias in the attention score between query and key.\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Rotary Positional Embeddings (RoPE)\n",
    "\n",
    "RoPE rotates each embedding vector based on its **absolute position**,  \n",
    "but the **attention scores reflect relative position**.\n",
    "\n",
    "### üî¨ How It Works\n",
    "\n",
    "1. Split embedding into pairs:  \n",
    "   $$\n",
    "   [x_0, x_1], [x_2, x_3], \\dots\n",
    "   $$\n",
    "\n",
    "2. Apply a **rotation matrix** based on token's position $p$:\n",
    "   $$\n",
    "   \\begin{bmatrix}\n",
    "   \\cos(\\theta_p) & -\\sin(\\theta_p) \\\\\n",
    "   \\sin(\\theta_p) & \\cos(\\theta_p)\n",
    "   \\end{bmatrix}\n",
    "   \\cdot\n",
    "   \\begin{bmatrix}\n",
    "   x_{2i} \\\\\n",
    "   x_{2i+1}\n",
    "   \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "3. At attention time, compute:\n",
    "   $$\n",
    "   \\text{score}(i, j) = \\text{RoPE}(q_i)^T \\cdot \\text{RoPE}(k_j)\n",
    "   $$\n",
    "\n",
    "4. The dot product reflects the **relative angle** between positions $i$ and $j$,  \n",
    "   giving us **relative position sensitivity** while keeping **absolute context**.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Why RoPE is the Best of Both Worlds\n",
    "\n",
    "- ‚úÖ **Absolute position** is encoded via rotation angle.\n",
    "- ‚úÖ **Relative distance** affects the similarity score in attention.\n",
    "- ‚úÖ No need for additional embeddings or memory overhead.\n",
    "- ‚úÖ Smooth generalization to longer sequences.\n",
    "\n",
    "---\n",
    "\n",
    "## üîë Key Takeaways\n",
    "\n",
    "- Transformers need position info because they‚Äôre order-agnostic by default.\n",
    "- Absolute ‚â† Relative ‚Äî they serve different roles.\n",
    "- **RoPE injects both types of info directly into attention**, making it highly efficient and flexible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß± Transformer Architecture ‚Äî Encoder & Decoder Notes\n",
    "\n",
    "This breakdown covers the structure, flow, and components of the original Transformer encoder and decoder, as described in *‚ÄúAttention Is All You Need.‚Äù*\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ Encoder Overview\n",
    "\n",
    "The encoder processes the **entire input sequence** at once and produces a sequence of **context-enriched token representations**.\n",
    "\n",
    "### üí° Structure of Each Encoder Layer:\n",
    "\n",
    "\n",
    "### üîë Key Details:\n",
    "- **Self-attention is unmasked** ‚Üí each token can attend to all others\n",
    "- **Same layer is applied N times** (usually 6‚Äì12)\n",
    "- Positional information is added at the embedding level before the first layer\n",
    "- Outputs a sequence of vectors: one per input token\n",
    "\n",
    "---\n",
    "\n",
    "## üì§ Decoder Overview\n",
    "\n",
    "The decoder generates an output sequence **token by token**, using:\n",
    "- Previously generated tokens (causal masked self-attention)\n",
    "- The full encoder output (via cross-attention)\n",
    "\n",
    "### üí° Structure of Each Decoder Layer:\n",
    "\n",
    "\n",
    "### üîë Key Details:\n",
    "- **Masked self-attention** prevents the model from looking ahead\n",
    "- **Cross-attention** uses keys/values from encoder output and queries from the decoder\n",
    "- Also stacked N times\n",
    "- Final decoder output is projected to vocab logits ‚Üí softmax ‚Üí next token\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Summary: Encoder vs Decoder\n",
    "\n",
    "| Component     | Encoder                        | Decoder                            |\n",
    "|---------------|--------------------------------|-------------------------------------|\n",
    "| Attention 1   | Self-attention (unmasked)      | Masked self-attention (causal)     |\n",
    "| Attention 2   | ‚ùå                              | Cross-attention (with encoder out) |\n",
    "| Feedforward   | ‚úÖ Yes                         | ‚úÖ Yes                              |\n",
    "| Add & Norm    | ‚úÖ Yes (twice per layer)        | ‚úÖ Yes (three times per layer)      |\n",
    "| Positional Info | Added at input               | Added at input                      |\n",
    "| Output        | Contextual token vectors       | Next token logits                   |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Final Output Flow (Decoder Side)\n",
    "\n",
    "1. Decoder produces contextual token representations\n",
    "2. Projected to vocabulary size:  \n",
    "   $$ \\text{logits} = \\text{decoder\\_output} \\cdot W_{\\text{vocab}} + b $$\n",
    "3. Softmax is applied ‚Üí pick next token\n",
    "4. Append token ‚Üí repeat until `<EOS>` or max length\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß± Transformer Architecture ‚Äî Encoder & Decoder Notes (with Sequential Steps)\n",
    "\n",
    "This breakdown covers the structure, flow, and sequential processing steps of the Transformer **encoder** and **decoder**, as introduced in *‚ÄúAttention Is All You Need.‚Äù*\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ Encoder Overview\n",
    "\n",
    "The encoder processes the **entire input sequence** in parallel and transforms each token into a **contextual embedding**.\n",
    "\n",
    "### ‚úÖ Sequential Steps (Per Encoder Layer):\n",
    "\n",
    "1. **Token Embedding + Positional Encoding**  \n",
    "   Input tokens are embedded into vectors, and position encodings are added:  \n",
    "   $$\n",
    "   X = \\text{TokenEmbedding} + \\text{PositionalEncoding}\n",
    "   $$\n",
    "\n",
    "2. **Multi-Head Self-Attention**  \n",
    "   - Compute $Q, K, V$ for each token  \n",
    "   - Perform scaled dot-product attention across all tokens (no mask)  \n",
    "   - Outputs new token representations with context\n",
    "\n",
    "3. **Add & LayerNorm**  \n",
    "   $$\n",
    "   \\text{Norm}_1 = \\text{LayerNorm}(X + \\text{SelfAttention}(X))\n",
    "   $$\n",
    "\n",
    "4. **Feedforward Network (FFN)**  \n",
    "   Apply a 2-layer MLP to each token independently:  \n",
    "   $$\n",
    "   \\text{FFN}(x) = \\text{GELU}(xW_1 + b_1)W_2 + b_2\n",
    "   $$\n",
    "\n",
    "5. **Add & LayerNorm**  \n",
    "   $$\n",
    "   \\text{Output} = \\text{LayerNorm}(\\text{Norm}_1 + \\text{FFN})\n",
    "   $$\n",
    "\n",
    "Repeat this stack $N$ times (e.g., $N = 6$)\n",
    "\n",
    "---\n",
    "\n",
    "## üì§ Decoder Overview\n",
    "\n",
    "The decoder generates the output **one token at a time**, attending to:\n",
    "- **Previously generated tokens** (masked self-attention)\n",
    "- **Encoder output** (via cross-attention)\n",
    "\n",
    "### ‚úÖ Sequential Steps (Per Decoder Layer):\n",
    "\n",
    "1. **Token Embedding + Positional Encoding**  \n",
    "   Decoder input is embedded and positionally encoded\n",
    "\n",
    "2. **Masked Multi-Head Self-Attention**  \n",
    "   - Causal mask applied so each token only sees previous ones  \n",
    "   - Compute attention over the generated sequence so far\n",
    "\n",
    "3. **Add & LayerNorm**  \n",
    "   Residual + normalization as usual\n",
    "\n",
    "4. **Cross-Attention (Encoder-Decoder Attention)**  \n",
    "   - Decoder queries attend to encoder output (keys/values)  \n",
    "   - Lets decoder align output with relevant input context\n",
    "\n",
    "5. **Add & LayerNorm**  \n",
    "   Another residual + normalization\n",
    "\n",
    "6. **Feedforward Network (MLP)**  \n",
    "   Same structure as encoder:  \n",
    "   $$\n",
    "   \\text{FFN}(x) = \\text{GELU}(xW_1 + b_1)W_2 + b_2\n",
    "   $$\n",
    "\n",
    "7. **Add & LayerNorm**  \n",
    "   Final norm and residual\n",
    "\n",
    "Repeat this stack $N$ times\n",
    "\n",
    "8. **Final Output Projection**  \n",
    "   Project last-layer decoder output to vocabulary size:  \n",
    "   $$\n",
    "   \\text{logits} = \\text{DecoderOutput} \\cdot W_{\\text{vocab}} + b\n",
    "   $$\n",
    "\n",
    "9. **Softmax & Sampling**  \n",
    "   Convert logits to probabilities ‚Üí sample or argmax next token\n",
    "\n",
    "10. **Append next token & repeat** until `<EOS>` or max length\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ Summary Table\n",
    "\n",
    "| Step | Encoder | Decoder |\n",
    "|------|---------|---------|\n",
    "| 1    | Embed input + add position | Embed output so far + add position |\n",
    "| 2    | Self-attention (unmasked) | Masked self-attention |\n",
    "| 3    | Add & Norm | Add & Norm |\n",
    "| 4    | Feedforward | Cross-attention (to encoder output) |\n",
    "| 5    | Add & Norm | Add & Norm |\n",
    "| 6    | ‚Äî | Feedforward |\n",
    "| 7    | ‚Äî | Add & Norm |\n",
    "| 8    | Repeat $N$ layers | Repeat $N$ layers |\n",
    "| 9    | ‚Äî | Project to vocab + softmax |\n",
    "| 10   | ‚Äî | Predict next token & repeat |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
