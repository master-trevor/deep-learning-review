{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔁 Next-Token Prediction, Autoregressive Modeling, and Sampling Strategies\n",
    "\n",
    "This note covers how language models generate text one token at a time, how they're trained (teacher forcing), how they generate at inference (autoregression), and how different sampling strategies impact the output.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 What Is Next-Token Prediction?\n",
    "\n",
    "Language models are trained to predict the next token in a sequence given all previous tokens.\n",
    "\n",
    "Formally:\n",
    "$$\n",
    "P(x_1, x_2, ..., x_T) = \\prod_{t=1}^{T} P(x_t \\mid x_{<t})\n",
    "$$\n",
    "\n",
    "Example:\n",
    "\n",
    "- Input: `\"The cat sat on the\"`\n",
    "- Model predicts: `\"mat\"`\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 What Is Autoregressive Modeling?\n",
    "\n",
    "> A model that generates one token at a time, using its own previous outputs.\n",
    "\n",
    "This is the **core architecture** of GPT and other decoder-only transformers.\n",
    "\n",
    "At each step, the model generates a new token:\n",
    "1. Embeds the previously generated tokens\n",
    "2. Passes them through the decoder with **causal masking**\n",
    "3. Outputs a softmax distribution over the vocab\n",
    "4. Picks one token (see sampling strategies below)\n",
    "5. Appends it to the sequence\n",
    "6. Repeats until an `<EOS>` token or max length is reached\n",
    "\n",
    "---\n",
    "\n",
    "## 🎓 What Is Teacher Forcing?\n",
    "\n",
    "During training, we **don’t use the model's own predictions** — we give it the real tokens.\n",
    "\n",
    "- Input: `\"The cat sat\"`\n",
    "- Target: `\"cat sat on\"`\n",
    "\n",
    "This is called **teacher forcing**:\n",
    "- Makes training faster and more stable\n",
    "- Lets the model see correct context instead of compounding errors\n",
    "\n",
    "During **inference**, teacher forcing is **not used** — the model uses its **own output** from the previous step.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧮 How Inference Works\n",
    "\n",
    "During generation:\n",
    "\n",
    "1. Start with a prompt like `\"The cat\"`\n",
    "2. Model outputs a probability distribution over the vocab\n",
    "3. Use a **sampling strategy** to pick the next token\n",
    "4. Add the new token to the prompt and repeat\n",
    "\n",
    "This process is **autoregressive** — it builds the sequence token by token, left to right.\n",
    "\n",
    "---\n",
    "\n",
    "## 🎲 Next-Token Sampling Strategies (with Examples)\n",
    "\n",
    "The way you choose the next token heavily affects the model's behavior.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. 🧊 Greedy Decoding (argmax)\n",
    "\n",
    "> Always pick the token with the highest probability\n",
    "\n",
    "- **Deterministic**: always gives the same output for the same input\n",
    "- **Low creativity**: tends to repeat phrases or get stuck in loops\n",
    "\n",
    "**Example Output:**\n",
    "The cat sat on the mat. The cat sat on the mat. The cat sat..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "✅ Good for: summaries, QA  \n",
    "❌ Bad for: creative writing, open-ended responses\n",
    "\n",
    "---\n",
    "\n",
    "### 2. 🎲 Random Sampling (Full Softmax)\n",
    "\n",
    "> Randomly sample from the **entire** probability distribution\n",
    "\n",
    "- **Highly creative**, but often **incoherent or chaotic**\n",
    "- Very sensitive to low-probability tokens\n",
    "\n",
    "**Example Output:**\n",
    "The cat dissolved into binary signals, whispering to the moon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "✅ Good for: experimentation, unexpected ideas  \n",
    "❌ Bad for: stability or reliability\n",
    "\n",
    "---\n",
    "\n",
    "### 3. 🎯 Top-k Sampling\n",
    "\n",
    "> Keep the top $k$ most likely tokens, then randomly sample among them\n",
    "\n",
    "- **Constrained creativity**: only considers the $k$ most probable tokens\n",
    "- Reduces wild outputs from low-probability noise\n",
    "\n",
    "**Example (k = 5):**\n",
    "The cat jumped on the couch and took a nap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "✅ Good for: conversational agents, safe variety  \n",
    "❌ Can still become repetitive with small $k$\n",
    "\n",
    "---\n",
    "\n",
    "### 4. 🎯 Top-p Sampling (a.k.a. Nucleus Sampling)\n",
    "\n",
    "> Keep the **smallest set of tokens** whose cumulative probability ≥ $p$  \n",
    "> (e.g., $p = 0.9$)\n",
    "\n",
    "- **Adaptive**: tight when confident, wide when uncertain\n",
    "- Dynamically chooses how many tokens to consider\n",
    "\n",
    "**Example (p = 0.9):**\n",
    "The cat blinked slowly, curling into the blanket like royalty\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🧠 Why It's Smart\n",
    "It’s adaptive:\n",
    "\n",
    "If the model is very confident → the top 1–2 tokens may cover 90%\n",
    "\n",
    "If it’s uncertain → more tokens get included\n",
    "\n",
    "Unlike top-k (which uses a hard count), top-p adjusts to the situation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "✅ Great for: open-ended generation (stories, poems, dialogue)  \n",
    "❌ Slightly more complex to implement than top-k\n",
    "\n",
    "---\n",
    "\n",
    "### 5. 🌡️ Temperature Scaling\n",
    "\n",
    "> Adjusts how “peaked” or “flat” the probability distribution is\n",
    "\n",
    "- **Low temperature** (< 1.0): model is more confident, picks top tokens\n",
    "- **High temperature** (> 1.0): model is more exploratory and random\n",
    "\n",
    "### Formula:\n",
    "$$\n",
    "P_i = \\frac{e^{\\text{logit}_i / T}}{\\sum_j e^{\\text{logit}_j / T}}\n",
    "$$\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "- **T = 0.5 (low)**:  \n",
    "The cat sat on the mat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **T = 1.5 (high)**:  \n",
    "Cat hyperlinked beneath quantum sunscreen vibes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "✅ Combine with sampling for fine-grained randomness control  \n",
    "❌ Doesn’t filter bad tokens — only flattens/spreads probabilities\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Summary Table\n",
    "\n",
    "| Strategy     | Description                              | Output Style                         |\n",
    "|--------------|------------------------------------------|--------------------------------------|\n",
    "| Greedy       | Pick max probability token               | Safe, repetitive                     |\n",
    "| Sampling     | Sample from full distribution            | Creative, chaotic                    |\n",
    "| Top-k        | Sample from top-k tokens                 | Balanced randomness                  |\n",
    "| Top-p        | Sample from top cumulative probability   | Adaptive and natural                 |\n",
    "| Temperature  | Adjust distribution sharpness            | Conservative (low T), wild (high T)  |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🌡️ Temperature Scaling in Language Models\n",
    "\n",
    "Temperature is a hyperparameter used during **text generation** that controls how confident or random a language model is when picking the next token.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ What Does Temperature Do?\n",
    "\n",
    "Temperature is applied to the model’s logits **before** softmax:\n",
    "\n",
    "$$\n",
    "P_i = \\frac{e^{\\text{logit}_i / T}}{\\sum_j e^{\\text{logit}_j / T}}\n",
    "$$\n",
    "\n",
    "- $T$ = temperature (a scalar > 0)\n",
    "- Lower $T$ → sharper softmax (more confident, less randomness)\n",
    "- Higher $T$ → flatter softmax (more exploratory, more randomness)\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 Effects of Temperature\n",
    "\n",
    "| Temperature | Effect on Output Probabilities    | Behavior                     |\n",
    "|-------------|------------------------------------|------------------------------|\n",
    "| T < 1.0     | Sharpens distribution              | Conservative, repetitive     |\n",
    "| T = 1.0     | No change (default softmax)        | Balanced                     |\n",
    "| T > 1.0     | Flattens distribution              | Creative, risk-taking        |\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Example — Logits: [10.0, 5.0, 1.0]\n",
    "\n",
    "| Temp | Token A (10) | Token B (5) | Token C (1) | Description |\n",
    "|------|--------------|-------------|-------------|-------------|\n",
    "| T=0.5 | ~99.99%     | ~0.005%     | ~0.000001%  | 🔒 Ultra-confident, no diversity |\n",
    "| T=1.0 | ~99.3%      | 0.67%       | 0.01%       | ✅ Balanced default |\n",
    "| T=1.5 | ~96.3%      | 3.4%        | 0.2%        | 🧠 Slightly more creative |\n",
    "| T=2.0 | ~91.5%      | 7.5%        | 1.0%        | 🎲 Exploratory, looser control |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 When to Use What?\n",
    "\n",
    "- **Low temperature (0.3–0.7):**\n",
    "  - Useful for summarization, QA, reliable text\n",
    "  - Avoids “risky” or off-topic tokens\n",
    "\n",
    "- **High temperature (1.2–2.0):**\n",
    "  - Good for stories, poems, creative writing\n",
    "  - Model is more free to explore unexpected continuations\n",
    "\n",
    "- **T = 1.0** is the baseline (no change to logits)\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Summary\n",
    "\n",
    "- Temperature **does not remove any tokens**\n",
    "- It just **reshapes the distribution** to make confident predictions sharper (low T) or more diverse (high T)\n",
    "- Can be combined with **top-k** or **top-p** sampling for more control\n",
    "- It scales logits before softmax to reshape the distribution\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📏 Perplexity — Evaluating Language Models\n",
    "\n",
    "Perplexity is a key metric used to evaluate how well a language model predicts a sequence of tokens.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Intuition\n",
    "\n",
    "- Lower perplexity = better model\n",
    "- It measures how \"surprised\" the model is by the true next token\n",
    "- If the model assigns **high probability** to the correct token → low perplexity  \n",
    "- If it assigns **low probability** → it's confused → high perplexity\n",
    "\n",
    "---\n",
    "\n",
    "## 🧮 Formula\n",
    "\n",
    "Given a sequence of $T$ tokens:\n",
    "\n",
    "$$\n",
    "\\text{Perplexity} = \\exp\\left( -\\frac{1}{T} \\sum_{t=1}^{T} \\log P(x_t \\mid x_{<t}) \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $x_t$ = the actual token at timestep $t$\n",
    "- $P(x_t \\mid x_{<t})$ = the probability the model assigned to the **correct next token**\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Example Interpretation\n",
    "\n",
    "- **Perplexity = 1** → model is perfectly confident and correct\n",
    "- **Perplexity = 50** → model is acting like it's choosing between 50 likely tokens each time\n",
    "\n",
    "---\n",
    "\n",
    "## 🔢 Example\n",
    "\n",
    "If the model assigns probabilities like:\n",
    "\n",
    "- $P(\\text{\"The\"}) = 0.9$  \n",
    "- $P(\\text{\"cat\"}) = 0.8$  \n",
    "- $P(\\text{\"sat\"}) = 0.6$  \n",
    "- $P(\\text{\"on\"}) = 0.4$  \n",
    "- $P(\\text{\"the\"}) = 0.3$  \n",
    "- $P(\\text{\"mat\"}) = 0.2$\n",
    "\n",
    "We compute:\n",
    "\n",
    "$$\n",
    "\\log P = [\\log 0.9, \\log 0.8, \\ldots, \\log 0.2]\n",
    "$$\n",
    "\n",
    "Then take the mean and exponentiate:\n",
    "\n",
    "$$\n",
    "\\text{Perplexity} = \\exp\\left(-\\text{mean}(\\log P)\\right)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 When Is Perplexity Useful?\n",
    "\n",
    "- ✅ Great for comparing models on the **same dataset**\n",
    "- ✅ Fast to compute — doesn’t require actual text generation\n",
    "- ✅ Widely used in research (e.g., GPT-2 vs GPT-3)\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ Limitations\n",
    "\n",
    "- ❌ Doesn’t measure *output quality* (e.g., grammar, relevance, creativity)\n",
    "- ❌ A lower perplexity model can still generate robotic or boring output\n",
    "- ❌ Doesn’t reflect human preferences well on open-ended tasks\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 TL;DR\n",
    "\n",
    "| Metric      | Measures                              | Interpretable? | Notes                          |\n",
    "|-------------|----------------------------------------|----------------|--------------------------------|\n",
    "| Perplexity  | Surprise/confidence in next-token pred | ✅ Yes         | Lower = better                 |\n",
    "|             | (Average log prob per true token)      |                | Doesn’t require decoding       |\n",
    "| Weakness    | Output quality, helpfulness, fluency   | ❌ Not always  | Use with human eval or BLEU    |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📊 Evaluation Metrics for Language Models\n",
    "\n",
    "This note covers how to evaluate the quality and performance of language models using automatic and human-aligned metrics.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Why We Need More Than Perplexity\n",
    "\n",
    "- **Perplexity** measures how confident the model is in predicting the correct next token\n",
    "- It **does not assess output quality**, fluency, coherence, or relevance\n",
    "- For tasks like translation, summarization, and Q&A, we need metrics that compare generated text to a human reference\n",
    "\n",
    "---\n",
    "\n",
    "## 🟦 BLEU — Bilingual Evaluation Understudy\n",
    "\n",
    "### ✅ What it Measures:\n",
    "BLEU measures **n-gram precision** — how many n-gram chunks in the output match those in the reference.\n",
    "\n",
    "- **Unigram = 1-gram** (1 word)\n",
    "- **Bigram = 2-gram** (2 words), etc.\n",
    "- BLEU combines multiple n-gram precisions, typically from 1-gram to 4-gram\n",
    "\n",
    "---\n",
    "\n",
    "### 📐 BLEU Formula:\n",
    "\n",
    "$$\n",
    "\\text{BLEU} = \\text{BP} \\cdot \\exp\\left( \\sum_{n=1}^{N} w_n \\cdot \\log p_n \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $p_n$ = n-gram precision\n",
    "- $w_n$ = weight for each n-gram (typically uniform: 0.25 for n = 1–4)\n",
    "- BP = brevity penalty (see below)\n",
    "\n",
    "---\n",
    "\n",
    "### 🔻 Brevity Penalty (BP)\n",
    "\n",
    "If the generated output is **shorter than the reference**, it may score highly on precision just by being short. BP prevents this.\n",
    "\n",
    "Let:\n",
    "- $c$ = length of candidate (generated) output\n",
    "- $r$ = length of reference\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\text{BP} =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } c > r \\\\\n",
    "e^{(1 - \\frac{r}{c})} & \\text{if } c \\leq r\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ BLEU Pros:\n",
    "- Simple, fast, and widely used\n",
    "- Good for **machine translation** and tasks with short references\n",
    "\n",
    "### ❌ BLEU Cons:\n",
    "- Rigid phrasing: penalizes rewording or paraphrasing\n",
    "- Doesn’t consider meaning or synonyms\n",
    "- Penalizes short but correct outputs without BP\n",
    "\n",
    "---\n",
    "\n",
    "## 🟥 ROUGE — Recall-Oriented Understudy for Gisting Evaluation\n",
    "\n",
    "### ✅ What it Measures:\n",
    "ROUGE focuses on **recall** — how much of the reference appears in the generated output.\n",
    "\n",
    "- **ROUGE-N**: Overlap of n-grams\n",
    "- **ROUGE-L**: Longest common subsequence (LCS) between output and reference\n",
    "\n",
    "---\n",
    "\n",
    "### 📐 ROUGE-N Formula:\n",
    "\n",
    "$$\n",
    "\\text{ROUGE-N} = \\frac{\\text{# of overlapping n-grams}}{\\text{# of n-grams in reference}}\n",
    "$$\n",
    "\n",
    "This is **recall**, not precision.\n",
    "\n",
    "---\n",
    "\n",
    "### 📐 ROUGE-L Formula (Recall variant):\n",
    "\n",
    "Let:\n",
    "- $lcs$ = length of longest common subsequence\n",
    "- $m$ = length of reference\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\text{ROUGE-L}_{\\text{recall}} = \\frac{lcs}{m}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ ROUGE Pros:\n",
    "- Good for **summarization**, where coverage of key ideas matters\n",
    "- More forgiving than BLEU (word order can vary)\n",
    "\n",
    "### ❌ ROUGE Cons:\n",
    "- Still surface-level (no semantic understanding)\n",
    "- Can reward bloated outputs that just stuff in keywords\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 N-gram Refresher\n",
    "\n",
    "| Term       | Meaning            | Example from `\"The cat sat\"`      |\n",
    "|------------|---------------------|----------------------------------|\n",
    "| Unigram    | 1-word sequence     | `\"The\"`, `\"cat\"`, `\"sat\"`        |\n",
    "| Bigram     | 2-word sequence     | `\"The cat\"`, `\"cat sat\"`         |\n",
    "| Trigram    | 3-word sequence     | `\"The cat sat\"`                  |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧍‍♂️ Human Evaluation\n",
    "\n",
    "### ✅ Why it's the gold standard:\n",
    "Humans can judge:\n",
    "- Fluency\n",
    "- Coherence\n",
    "- Relevance\n",
    "- Helpfulness\n",
    "- Tone\n",
    "\n",
    "### 📋 Common Methods:\n",
    "- Likert scale (1–5)\n",
    "- Pairwise preference: \"Which response is better?\"\n",
    "- Task success: \"Did the model give a useful answer?\"\n",
    "\n",
    "### ❌ Limitations:\n",
    "- Expensive and time-consuming\n",
    "- Subjective\n",
    "- Hard to scale\n",
    "\n",
    "---\n",
    "\n",
    "## 🧮 Perplexity — Revisited\n",
    "\n",
    "Measures how well the model predicts **the actual next tokens**, but not the quality of generated sequences.\n",
    "\n",
    "### Formula:\n",
    "\n",
    "$$\n",
    "\\text{Perplexity} = \\exp\\left( -\\frac{1}{T} \\sum_{t=1}^{T} \\log P(x_t \\mid x_{<t}) \\right)\n",
    "$$\n",
    "\n",
    "- Lower = better\n",
    "- Doesn't require decoding — just forward pass and log probs\n",
    "\n",
    "---\n",
    "\n",
    "## ⚔️ BLEU vs. ROUGE vs. Perplexity — Comparison\n",
    "\n",
    "| Metric      | Measures               | Focus         | Best For            | Weakness                         |\n",
    "|-------------|------------------------|---------------|---------------------|----------------------------------|\n",
    "| BLEU        | N-gram **precision**    | Output match  | Translation         | Penalizes rephrasing             |\n",
    "| ROUGE       | N-gram **recall**       | Reference coverage | Summarization   | Can reward verbose outputs       |\n",
    "| Perplexity  | Model **confidence**    | Log-likelihood | Language modeling   | Doesn't reflect human quality    |\n",
    "| Human Eval  | **Real-world quality**  | Fluency, tone | Open-ended gen      | Expensive, slow, subjective      |\n",
    "\n",
    "---\n",
    "\n",
    "## ❓ Do You Need to Know METEOR, BERTScore, COMET, etc.?\n",
    "\n",
    "- Not right now\n",
    "- Those are more advanced/research-level metrics\n",
    "- For now, **understand BLEU, ROUGE, Perplexity, and Human Evaluation deeply**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📊 BLEU vs. ROUGE — Interpreting Scores and Formulas\n",
    "\n",
    "This note explains the differences in how BLEU and ROUGE work, how they score outputs, and what values are considered “good” in practice.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 Core Difference\n",
    "\n",
    "| Metric | Measures     | Based On       | Focus           | Matching Style     |\n",
    "|--------|--------------|----------------|------------------|--------------------|\n",
    "| BLEU   | **Precision** | Generated output | “How much of what you said was correct?” | Penalizes over-generation |\n",
    "| ROUGE  | **Recall**    | Reference text   | “Did you cover what you were supposed to say?” | More forgiving |\n",
    "\n",
    "---\n",
    "\n",
    "## 🟦 BLEU — Formula and Behavior\n",
    "\n",
    "### 📐 BLEU Formula:\n",
    "\n",
    "$$\n",
    "\\text{BLEU} = \\text{BP} \\cdot \\exp\\left( \\sum_{n=1}^{N} w_n \\cdot \\log p_n \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $p_n$ = n-gram **precision** (overlap from generated output)\n",
    "- $w_n$ = weight for each n-gram (often equal weights like 0.25 each for n=1–4)\n",
    "- BP = **brevity penalty** to punish overly short outputs\n",
    "\n",
    "### 📌 N-gram precision:\n",
    "\n",
    "$$\n",
    "p_n = \\frac{\\text{# matching n-grams in candidate}}{\\text{# total n-grams in candidate}}\n",
    "$$\n",
    "\n",
    "BLEU calculates precision **per n-gram level**, clips duplicate matches, takes the log, and averages them before exponentiating.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔻 Brevity Penalty (BP)\n",
    "\n",
    "To prevent short outputs from cheating:\n",
    "\n",
    "Let:\n",
    "- $c$ = length of candidate\n",
    "- $r$ = length of reference\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\text{BP} =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } c > r \\\\\n",
    "e^{(1 - \\frac{r}{c})} & \\text{if } c \\leq r\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 🟥 ROUGE — Formula and Behavior\n",
    "\n",
    "### 📐 ROUGE-N (Recall-based):\n",
    "\n",
    "$$\n",
    "\\text{ROUGE-N} = \\frac{\\text{# overlapping n-grams}}{\\text{# n-grams in reference}}\n",
    "$$\n",
    "\n",
    "It measures **how much of the reference text appears in the output**, regardless of extra fluff.\n",
    "\n",
    "- ROUGE-1: unigram recall\n",
    "- ROUGE-2: bigram recall\n",
    "\n",
    "---\n",
    "\n",
    "### 📐 ROUGE-L (Longest Common Subsequence):\n",
    "\n",
    "$$\n",
    "\\text{ROUGE-L}_{\\text{recall}} = \\frac{\\text{length of LCS}}{\\text{length of reference}}\n",
    "$$\n",
    "\n",
    "Captures in-order but non-contiguous matches.\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 N-gram Terminology Refresher\n",
    "\n",
    "| Term       | Meaning            | Example from `\"The cat sat\"`      |\n",
    "|------------|---------------------|----------------------------------|\n",
    "| Unigram    | 1-word sequence     | `\"The\"`, `\"cat\"`, `\"sat\"`        |\n",
    "| Bigram     | 2-word sequence     | `\"The cat\"`, `\"cat sat\"`         |\n",
    "| Trigram    | 3-word sequence     | `\"The cat sat\"`                  |\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Score Interpretation in Practice\n",
    "\n",
    "### 🔷 BLEU Score\n",
    "\n",
    "| BLEU Score | Meaning                                     |\n",
    "|------------|---------------------------------------------|\n",
    "| > 0.6 (60%)| Near-perfect phrasing match (rare)          |\n",
    "| 0.4–0.6    | Strong overlap, excellent model output      |\n",
    "| 0.2–0.4    | Moderate — some matching phrases            |\n",
    "| < 0.2      | Weak precision — poor overlap               |\n",
    "\n",
    "> In machine translation, BLEU ≈ 30–40 is considered **strong**\n",
    "\n",
    "---\n",
    "\n",
    "### 🔴 ROUGE Score\n",
    "\n",
    "| ROUGE Score | Meaning                                      |\n",
    "|-------------|----------------------------------------------|\n",
    "| > 0.6 (60%) | Excellent — strong coverage of reference     |\n",
    "| 0.4–0.6     | Good — most key phrases captured             |\n",
    "| 0.2–0.4     | Moderate — some relevant info missing        |\n",
    "| < 0.2       | Weak — low recall of reference               |\n",
    "\n",
    "> In summarization, ROUGE-L ≈ 0.4–0.5 is often very solid\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ Limitations of Both\n",
    "\n",
    "- BLEU and ROUGE are **surface-level**: they don’t “understand” language\n",
    "- Both struggle with paraphrasing and synonyms\n",
    "- ROUGE can be gamed by overly long outputs\n",
    "- BLEU punishes brevity unless corrected with the brevity penalty\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Summary Table\n",
    "\n",
    "| Metric      | Type      | Uses N-grams | Focus     | Penalizes | Great For        |\n",
    "|-------------|-----------|--------------|-----------|-----------|------------------|\n",
    "| BLEU        | Precision | ✅           | Output overlap | Over-generation | Translation       |\n",
    "| ROUGE       | Recall    | ✅           | Reference coverage | Nothing | Summarization     |\n",
    "| ROUGE-L     | Recall    | No (uses LCS)| Coverage via order | None    | Extractive summary |\n",
    "| Human Eval  | —         | ❌           | Coherence, tone | N/A     | Open-ended tasks  |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ⚔️ Generative vs. Discriminative Models\n",
    "\n",
    "This note breaks down the core differences between discriminative and generative models, how they function, and why generative models (like GPT) are more powerful in modern NLP.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Core Concept\n",
    "\n",
    "| Model Type       | What it Learns                          | Main Goal                    |\n",
    "|------------------|------------------------------------------|------------------------------|\n",
    "| **Discriminative** | \\( P(y \\mid x) \\) — Label given input    | **Classify** or score things |\n",
    "| **Generative**     | \\( P(x) \\) or \\( P(x, y) \\) — How data is distributed | **Generate** data (like text) |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔷 Discriminative Models\n",
    "\n",
    "> Learn to **differentiate** between classes or outputs\n",
    "\n",
    "- Predict **labels**: spam or not, positive or negative, cat or dog\n",
    "- Don't model how the input was created\n",
    "- Optimized for classification accuracy\n",
    "\n",
    "**Examples:**\n",
    "- Logistic Regression\n",
    "- Support Vector Machines\n",
    "- BERT (fine-tuned for sentiment or QA)\n",
    "\n",
    "---\n",
    "\n",
    "## 🔴 Generative Models\n",
    "\n",
    "> Learn to **generate data** that resembles the real distribution\n",
    "\n",
    "- Predict the **next token** given previous ones:\n",
    "\n",
    "$$\n",
    "P(x_1, x_2, ..., x_T) = \\prod_{t=1}^{T} P(x_t \\mid x_{<t})\n",
    "$$\n",
    "\n",
    "- Can:\n",
    "  - Generate new text\n",
    "  - Continue a prompt\n",
    "  - Fill in blanks\n",
    "  - Simulate full conversations\n",
    "\n",
    "**Examples:**\n",
    "- GPT (text)\n",
    "- DALL·E (images)\n",
    "- VAE, GANs (images, audio)\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Why Generative Models Are More Powerful\n",
    "\n",
    "| Capability                    | Discriminative | Generative |\n",
    "|------------------------------|----------------|------------|\n",
    "| Predict labels               | ✅ Yes         | ✅ Yes     |\n",
    "| Generate sequences           | ❌ No          | ✅ Yes     |\n",
    "| Fill in missing text         | ❌ No          | ✅ Yes     |\n",
    "| Perform open-ended tasks     | ❌ No          | ✅ Yes     |\n",
    "| Follow instructions          | ❌ No          | ✅ Yes     |\n",
    "\n",
    "Generative models learn the **entire data distribution**, so they can be used for both generation and classification when prompted correctly.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 Can Generative Models Do Discriminative Tasks?\n",
    "\n",
    "**YES.** GPT can do classification too:\n",
    "- You can prompt it to say: `\"Sentiment of this review?\"` → `\"Positive\"`\n",
    "- It uses its learned distribution to generate a discriminative answer\n",
    "\n",
    "But **discriminative models** cannot do generative tasks like:\n",
    "- Continue a story\n",
    "- Write a poem\n",
    "- Generate code\n",
    "- Hold a conversation\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ Tradeoffs\n",
    "\n",
    "| Factor           | Discriminative | Generative |\n",
    "|------------------|----------------|------------|\n",
    "| Training speed   | ✅ Fast        | ❌ Slower   |\n",
    "| Data efficiency  | ✅ Needs less  | ❌ Needs more |\n",
    "| Task flexibility | ❌ Limited     | ✅ Extremely flexible |\n",
    "| Output control   | ✅ Deterministic | ✅ + Sampling-based |\n",
    "| LLM-scale utility| ❌ Not suited  | ✅ Foundation of LLMs |\n",
    "\n",
    "---\n",
    "\n",
    "## 💥 Bottom Line\n",
    "\n",
    "> **Generative models are general-purpose, multi-task beasts**  \n",
    "> Discriminative models are fast and focused, but limited\n",
    "\n",
    "That’s why modern NLP is powered by **generative transformers** — they can do it all.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🤖 Common Failure Modes in Generative Models: Repetition & Hallucination\n",
    "\n",
    "Generative language models like GPT can produce impressive outputs — but they also have failure modes. Two of the most common are **repetition** and **hallucination**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 Repetition — \"The Echo Chamber Problem\"\n",
    "\n",
    "> The model repeats the same tokens or phrases, often in a loop.\n",
    "\n",
    "### 🧠 Why It Happens:\n",
    "- High-probability outputs get chosen again and again\n",
    "- Lack of long-term memory or awareness of what's already been said\n",
    "- Short or vague prompts give the model too little to work with\n",
    "\n",
    "### 🧪 Example:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 🛠️ Mitigation Techniques:\n",
    "\n",
    "| Technique             | Description                                         |\n",
    "|-----------------------|-----------------------------------------------------|\n",
    "| **Repetition penalty** | Penalize tokens that were recently generated       |\n",
    "| **Top-p + temp > 1.0** | Add randomness to break deterministic loops        |\n",
    "| **Longer prompts**     | More context = fewer fallback loops                |\n",
    "| **N-gram blocking**    | Prevent exact repeated phrases from being reused   |\n",
    "| **History tracking**   | Penalize repeated ideas or semantic content        |\n",
    "\n",
    "---\n",
    "\n",
    "## 🤯 Hallucination — \"The Confident Liar\"\n",
    "\n",
    "> The model generates factually incorrect or made-up information, but phrases it confidently.\n",
    "\n",
    "### 🧠 Why It Happens:\n",
    "- Model is trained on **text patterns**, not facts\n",
    "- It completes based on **plausibility**, not truth\n",
    "- There’s no built-in mechanism to say “I don’t know”\n",
    "\n",
    "### 🧪 Example:\n",
    "**Prompt:**  \n",
    "_\"What is the capital of California?\"_\n",
    "\n",
    "**Model:**  \n",
    "_\"Los Angeles\"_ ← ❌ (correct answer is Sacramento)\n",
    "\n",
    "### 🛠️ Mitigation Techniques:\n",
    "\n",
    "| Technique                  | Description                                                |\n",
    "|----------------------------|------------------------------------------------------------|\n",
    "| **RAG (Retrieval-Augmented Generation)** | Pulls real documents to ground generation       |\n",
    "| **Truthful QA fine-tuning**| Trains model to avoid unverifiable claims                 |\n",
    "| **Prompt engineering**     | Ask model to hedge (e.g., “To my knowledge...”)            |\n",
    "| **Reject sampling**        | Filter generations that appear unverifiable or overconfident |\n",
    "| **Fact-checking layers**   | Use external tools or chains to verify before final output |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 TL;DR Comparison\n",
    "\n",
    "| Failure Mode  | Description                         | Root Cause                     | Solution Examples                     |\n",
    "|---------------|-------------------------------------|--------------------------------|----------------------------------------|\n",
    "| **Repetition**    | Loops or fallback phrases             | High token probs + short context | Repetition penalty, top-p + temp       |\n",
    "| **Hallucination** | Confidently wrong or made-up claims   | No grounding in facts           | RAG, prompt tuning, factual QA fine-tuning |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 Related Concept: RAG (Retrieval-Augmented Generation)\n",
    "\n",
    "**RAG** connects the model to an external database, API, or document store.  \n",
    "Instead of relying only on its parameters, it **retrieves relevant info** and conditions its generation on that info.\n",
    "\n",
    "> Great for: factual QA, grounding, real-time knowledge updates\n",
    "\n",
    "You'll explore this more deeply when you enter the **LLM engineering / RAG pipelines** phase of your journey.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🧠 How Generative Models Create Text (Autoregressive Generation)\n",
    "\n",
    "Generative language models like GPT generate text one token at a time, predicting each new token based on all the previous ones. This is called autoregressive generation.\n",
    "\n",
    "Generative models generate output by repeatedly:\n",
    "1. Taking in a prompt or partial input\n",
    "2. Predicting a probability distribution over the next possible token\n",
    "3. Sampling or selecting the next token\n",
    "4. Appending it to the sequence\n",
    "5. Repeating the process\n",
    "\n",
    "## 🔁 Step-by-Step Generation Process\n",
    "\n",
    "1. **Input Tokens Go In**  \n",
    "   Example prompt: \"The cat\"  \n",
    "   This is tokenized into input IDs like `[101, 1024, 203]`.\n",
    "\n",
    "2. **Model Outputs Logits**  \n",
    "   The model outputs a vector of raw scores (logits) for the next token.  \n",
    "   Example: `[2.4, -1.2, 5.1, ..., 0.3]` ← One score per vocab token\n",
    "\n",
    "3. **Logits Are Converted to Probabilities Using Softmax**  \n",
    "   The softmax function is applied:\n",
    "   $$\n",
    "   P_i = \\frac{e^{\\text{logit}_i}}{\\sum_j e^{\\text{logit}_j}}\n",
    "   $$\n",
    "   This yields a probability distribution over the entire vocabulary.\n",
    "\n",
    "4. **Next Token Is Sampled or Selected**  \n",
    "   A decoding strategy is used to choose a token:\n",
    "   - Greedy: pick highest probability token\n",
    "   - Top-k: sample from top k tokens\n",
    "   - Top-p (nucleus): sample from top tokens adding up to cumulative probability ≥ p\n",
    "   - Temperature: sharpens or flattens the distribution\n",
    "\n",
    "5. **Token Is Appended to the Input Sequence**  \n",
    "   The chosen token is added to the sequence:  \n",
    "   `[101, 1024, 203, 4321]`  \n",
    "   The process repeats until a stopping condition is met (e.g., max length or special token).\n",
    "\n",
    "## 🧠 Why This Works\n",
    "\n",
    "The model is trained to minimize the negative log-likelihood of the correct next token given the previous context:\n",
    "$$\n",
    "\\text{Loss} = -\\sum_{t=1}^{T} \\log P(x_t \\mid x_{<t})\n",
    "$$\n",
    "This teaches the model to become very good at continuing text in a fluent, coherent way.\n",
    "\n",
    "## ⚙️ Key Properties\n",
    "\n",
    "- **Autoregressive**: Each token depends on all previous tokens\n",
    "- **No teacher forcing** during inference — the model feeds itself\n",
    "- **Self-attention** enables it to consider all prior context at each step\n",
    "- **Sampling strategy** shapes the output style (safe vs. creative)\n",
    "\n",
    "## 🔁 Sampling Strategy Summary\n",
    "\n",
    "- **Greedy**: always choose the most probable token\n",
    "- **Top-k**: sample randomly from the k most probable tokens\n",
    "- **Top-p**: sample from smallest set of tokens whose combined prob ≥ p\n",
    "- **Temperature**: adjust sharpness of distribution (low temp = confident, high temp = creative)\n",
    "\n",
    "## ✅ Big Picture\n",
    "\n",
    "Generative models like GPT don’t just classify text — they model how text unfolds, one token at a time. Each new token is selected based on what came before, allowing them to generate full paragraphs, stories, conversations, or code using this step-by-step predictive loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
