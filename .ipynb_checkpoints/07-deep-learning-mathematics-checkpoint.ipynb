{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Derivative Rules, Guidelines for Substitution, and Common Function Derivatives\n",
    "\n",
    "This note summarizes essential differentiation rules for deep learning, when to use substitution (aka chain rule), and how to derive common functions like sigmoid and exponentials.\n",
    "\n",
    "---\n",
    "\n",
    "## üìò Core Derivative Rules\n",
    "\n",
    "| Rule             | Description                                | Formula |\n",
    "|------------------|--------------------------------------------|---------|\n",
    "| Constant Rule     | Derivative of a constant                   | $\\frac{d}{dx}(c) = 0$ |\n",
    "| Power Rule        | For $x^n$, pull the exponent down          | $\\frac{d}{dx}(x^n) = nx^{n-1}$ |\n",
    "| Sum Rule          | Derivative of a sum = sum of derivatives   | $\\frac{d}{dx}(f + g) = f' + g'$ |\n",
    "| Product Rule      | For multiplying functions                  | $(fg)' = f'g + fg'$ |\n",
    "| Quotient Rule     | For dividing functions                     | $\\left( \\frac{f}{g} \\right)' = \\frac{f'g - fg'}{g^2}$ |\n",
    "| Chain Rule        | For composite functions $f(g(x))$          | $\\frac{d}{dx}f(g(x)) = f'(g(x)) \\cdot g'(x)$ |\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Guidelines for Substitution / Chain Rule\n",
    "\n",
    "Use substitution (and chain rule) when:\n",
    "- You're differentiating a **composite function**\n",
    "- The expression includes **something more complex than plain $x$**\n",
    "- Example: $e^{-x}$, $\\log(1 + x^2)$, $\\tanh(x^3)$\n",
    "\n",
    "**Key principle:**  \n",
    "If you're not differentiating directly with respect to $x$, you're likely using the chain rule.\n",
    "\n",
    "---\n",
    "\n",
    "### üí° Chain Rule in Action (with $e^{-x}$)\n",
    "\n",
    "Let:\n",
    "- $f(x) = e^{-x}$\n",
    "- Think of this as $f(x) = e^{u(x)}$, where $u(x) = -x$\n",
    "\n",
    "Then:\n",
    "$$\n",
    "\\frac{d}{dx}(e^{-x}) = e^{-x} \\cdot \\frac{d}{dx}(-x) = e^{-x} \\cdot (-1) = -e^{-x}\n",
    "$$\n",
    "\n",
    "‚úÖ The negative sign comes from the derivative of the inner function ($-x$).\n",
    "\n",
    "---\n",
    "\n",
    "## üî¢ Derivatives of Common Functions\n",
    "\n",
    "### üî∑ Polynomials:\n",
    "- $\\frac{d}{dx}(x^2) = 2x$\n",
    "- $\\frac{d}{dx}(x^n) = nx^{n-1}$\n",
    "\n",
    "### üî∑ Exponentials:\n",
    "- $\\frac{d}{dx}(e^x) = e^x$\n",
    "- $\\frac{d}{dx}(e^{-x}) = -e^{-x}$\n",
    "- $\\frac{d}{dx}(a^x) = a^x \\log a$\n",
    "\n",
    "### üî∑ Logarithms (natural log):\n",
    "- $\\frac{d}{dx}(\\log x) = \\frac{1}{x}$\n",
    "\n",
    "### üî∑ Trigonometric (FYI only):\n",
    "- $\\frac{d}{dx}(\\sin x) = \\cos x$\n",
    "- $\\frac{d}{dx}(\\cos x) = -\\sin x$\n",
    "- $\\frac{d}{dx}(\\tan x) = \\sec^2 x$\n",
    "\n",
    "### üî∑ Hyperbolic / Neural Net Activations:\n",
    "- $\\frac{d}{dx}(\\tanh x) = 1 - \\tanh^2 x$\n",
    "- $\\frac{d}{dx}(\\text{sigmoid}(x)) = \\sigma(x)(1 - \\sigma(x))$\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "\n",
    "- Derivative rules = tools. Chain rule = glue.\n",
    "- Substitution helps identify when to apply chain rule\n",
    "- Even simple expressions like $e^{-x}$ are composite under the hood\n",
    "- Practice rewriting and differentiating in baby steps with annotations\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Activation Functions](activations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Intuition Behind Sigmoid vs Tanh in Deep Learning\n",
    "\n",
    "Understanding activation functions isn‚Äôt just about taking derivatives ‚Äî it's about how they behave **during optimization**, especially for **gradient flow**, **convergence speed**, and **vanishing gradients**.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ 1. Output Range\n",
    "\n",
    "| Function | Output Range | Zero-Centered? |\n",
    "|----------|--------------|----------------|\n",
    "| Sigmoid  | $(0, 1)$     | ‚ùå No          |\n",
    "| Tanh     | $(-1, 1)$    | ‚úÖ Yes         |\n",
    "\n",
    "- **Why it matters:**  \n",
    "  Zero-centered outputs (like tanh) help gradients flow **positively and negatively**, making weight updates more balanced and efficient.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ 2. Vanishing Gradient Problem\n",
    "\n",
    "Both functions **saturate** when input $x$ is very positive or negative.\n",
    "\n",
    "| Function | Saturation Zones                  | Derivative Trend |\n",
    "|----------|-----------------------------------|------------------|\n",
    "| Sigmoid  | $x < -3$ or $x > 3$ ‚Üí flattens    | Derivative $\\approx 0$ |\n",
    "| Tanh     | $x < -3$ or $x > 3$ ‚Üí flattens    | Derivative $\\approx 0$ |\n",
    "\n",
    "- **Why it matters:**  \n",
    "  When neurons output in these zones, their gradients vanish ‚Üí **very slow training or dead neurons** in deeper layers.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ 3. Gradient Strength\n",
    "\n",
    "| Function | Max Derivative | Location     |\n",
    "|----------|----------------|--------------|\n",
    "| Sigmoid  | $0.25$         | At $x = 0$   |\n",
    "| Tanh     | $1.0$          | At $x = 0$   |\n",
    "\n",
    "- **Why it matters:**  \n",
    "  Stronger gradients mean faster updates near $0$ ‚Äî **tanh is more expressive and efficient** in the core training zone.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ 4. Use Cases in Deep Learning\n",
    "\n",
    "| Use Case                        | Activation |\n",
    "|----------------------------------|------------|\n",
    "| Binary classification output     | Sigmoid    |\n",
    "| Multiclass classification output | Softmax    |\n",
    "| Hidden layers (historically)     | Tanh       |\n",
    "| Modern hidden layers             | ReLU       |\n",
    "\n",
    "- **Why tanh over sigmoid in hidden layers?**  \n",
    "  It's zero-centered and provides stronger gradients.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ 5. Why ReLU Took Over\n",
    "\n",
    "While tanh and sigmoid are still useful:\n",
    "- **ReLU** doesn‚Äôt saturate for $x > 0$\n",
    "- It keeps gradients alive\n",
    "- Great for deep networks\n",
    "- Easier to optimize\n",
    "\n",
    "But:\n",
    "- **Sigmoid** is still used in output layers\n",
    "- **Tanh + Sigmoid** still power LSTM/GRU gates\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Visual Summary\n",
    "\n",
    "#### Sigmoid & Tanh\n",
    "\n",
    "- Sigmoid squashes input to $(0, 1)$, flattens out at extremes  \n",
    "- Tanh squashes to $(-1, 1)$, symmetric and zero-centered\n",
    "\n",
    "#### Their Derivatives\n",
    "\n",
    "- **Sigmoid Derivative** peaks at $0.25$ and vanishes quickly  \n",
    "- **Tanh Derivative** peaks at $1$ and is wider around center  \n",
    "- Both die off at $|x| > 3$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî• Activation Functions + Softmax + Cross-Entropy ‚Äî Deep Learning Intuition\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ ReLU (Rectified Linear Unit)\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$\n",
    "\\text{ReLU}(x) = \\max(0, x)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{ReLU}'(x) =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } x > 0 \\\\\n",
    "0 & \\text{if } x \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**Intuition:**\n",
    "- Keep positive values, zero out negatives.\n",
    "- Fast to compute, no saturation in the positive range.\n",
    "\n",
    "**Issues:**\n",
    "- \"Dead neurons\" ‚Äî if a neuron gets stuck negative, it may never recover.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Leaky ReLU\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$\n",
    "\\text{LeakyReLU}(x) =\n",
    "\\begin{cases}\n",
    "x & \\text{if } x > 0 \\\\\n",
    "\\alpha x & \\text{if } x \\leq 0\n",
    "\\end{cases}\n",
    "\\quad\\text{where } \\alpha \\approx 0.01 \\text{ or } 0.1\n",
    "$$\n",
    "\n",
    "**Derivative:**\n",
    "\n",
    "$$\n",
    "\\text{LeakyReLU}'(x) =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } x > 0 \\\\\n",
    "\\alpha & \\text{if } x \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**Fixes:**\n",
    "- Lets small gradients pass through for $x < 0$ ‚Üí avoids dead neurons.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ GELU (Gaussian Error Linear Unit)\n",
    "\n",
    "**Exact Formula:**\n",
    "\n",
    "$$\n",
    "\\text{GELU}(x) = x \\cdot \\Phi(x)\n",
    "= x \\cdot \\frac{1}{2} \\left[ 1 + \\text{erf}\\left( \\frac{x}{\\sqrt{2}} \\right) \\right]\n",
    "$$\n",
    "\n",
    "**Derivative (Exact):**\n",
    "\n",
    "$$\n",
    "\\frac{d}{dx} \\text{GELU}(x) = \\Phi(x) + x \\cdot \\phi(x)\n",
    "\\quad\\text{where } \\phi(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-x^2/2}\n",
    "$$\n",
    "\n",
    "**Approximate Formula (Used in practice):**\n",
    "\n",
    "$$\n",
    "\\text{GELU}(x) \\approx 0.5x \\left[ 1 + \\tanh\\left( \\sqrt{\\frac{2}{\\pi}}(x + 0.044715x^3) \\right) \\right]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Softmax + Cross-Entropy\n",
    "\n",
    "#### Softmax:\n",
    "\n",
    "Given logits vector $ \\mathbf{z} $, softmax converts to probabilities:\n",
    "\n",
    "$$\n",
    "\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Cross-Entropy Loss:\n",
    "\n",
    "Given true class $ \\mathbf{y} $ (one-hot), and predicted probs $ \\hat{\\mathbf{y}} = \\text{softmax}(\\mathbf{z}) $:\n",
    "\n",
    "$$\n",
    "\\text{CE}(\\mathbf{y}, \\hat{\\mathbf{y}}) = -\\sum_i y_i \\log(\\hat{y}_i)\n",
    "= -\\log(\\hat{y}_{\\text{true class}})\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Combined Derivative:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z_i} = \\hat{y}_i - y_i\n",
    "$$\n",
    "\n",
    "This is why frameworks like PyTorch use `CrossEntropyLoss(logits, targets)` directly.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Visual Intuitions\n",
    "\n",
    "- Softmax with larger logits ‚Üí more confident predictions (sharper output)\n",
    "- Cross-entropy loss:\n",
    "  - Low when $ \\hat{y}_{\\text{true}} \\approx 1 $\n",
    "  - Very high when $ \\hat{y}_{\\text{true}} \\approx 0 $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî• Activation Functions + Softmax + Cross-Entropy ‚Äî Deep Learning Intuition (With Math)\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ ReLU (Rectified Linear Unit)\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$\n",
    "\\text{ReLU}(x) = \\max(0, x)\n",
    "$$\n",
    "\n",
    "**Derivative:**\n",
    "\n",
    "$$\n",
    "\\text{ReLU}'(x) =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } x > 0 \\\\\n",
    "0 & \\text{if } x \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**Intuition:**\n",
    "- Outputs the input directly if it's positive, else outputs 0.\n",
    "- Introduces non-linearity while maintaining simplicity.\n",
    "- **No gradient saturation** in the positive region ‚Üí keeps gradients alive.\n",
    "\n",
    "**Drawback:**\n",
    "- Neurons can \"die\" if they fall into the $x \\leq 0$ region (zero gradient forever).\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Leaky ReLU\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$\n",
    "\\text{LeakyReLU}(x) =\n",
    "\\begin{cases}\n",
    "x & \\text{if } x > 0 \\\\\n",
    "\\alpha x & \\text{if } x \\leq 0\n",
    "\\end{cases}\n",
    "\\quad\\text{where } \\alpha \\in [0.01, 0.1]\n",
    "$$\n",
    "\n",
    "**Derivative:**\n",
    "\n",
    "$$\n",
    "\\text{LeakyReLU}'(x) =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } x > 0 \\\\\n",
    "\\alpha & \\text{if } x \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**Intuition:**\n",
    "- Small negative slope instead of flat zero.\n",
    "- Allows small gradient when $x < 0$ ‚Üí avoids dead neurons.\n",
    "- Often used in GANs or deep CNNs for better convergence.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ GELU (Gaussian Error Linear Unit)\n",
    "\n",
    "**Exact Formula:**\n",
    "\n",
    "$$\n",
    "\\text{GELU}(x) = x \\cdot \\Phi(x)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$\n",
    "\\Phi(x) = \\frac{1}{2} \\left[ 1 + \\text{erf}\\left( \\frac{x}{\\sqrt{2}} \\right) \\right]\n",
    "$$\n",
    "\n",
    "**Derivative (Exact)(Product Rule):**\n",
    "\n",
    "$$\n",
    "\\frac{d}{dx} \\text{GELU}(x) = \\Phi(x) + x \\cdot \\phi(x)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$\n",
    "\\phi(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-x^2/2}\n",
    "$$\n",
    "\n",
    "**Approximate Formula (used in practice):**\n",
    "\n",
    "$$\n",
    "\\text{GELU}(x) \\approx 0.5x \\left[ 1 + \\tanh\\left( \\sqrt{\\frac{2}{\\pi}}(x + 0.044715x^3) \\right) \\right]\n",
    "$$\n",
    "\n",
    "**Intuition:**\n",
    "- Smoothed version of ReLU that uses probability weighting.\n",
    "- Weighs inputs by their likelihood of being positive under a standard normal distribution.\n",
    "- No hard threshold ‚Üí smoother gradient flow.\n",
    "\n",
    "**Use case:**\n",
    "- Default activation in Transformer models (e.g. BERT, GPT).\n",
    "- Helps with convergence and generalization in large-scale deep learning.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Softmax + Cross-Entropy\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚úÖ Softmax Function\n",
    "\n",
    "Given raw logits vector:\n",
    "\n",
    "$$\n",
    "\\mathbf{z} = [z_1, z_2, \\dots, z_n]\n",
    "$$\n",
    "\n",
    "Softmax converts it to a probability distribution:\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = \\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}}\n",
    "$$\n",
    "\n",
    "- Output: $0 < \\hat{y}_i < 1$\n",
    "- Ensures: $\\sum_i \\hat{y}_i = 1$\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚úÖ Cross-Entropy Loss\n",
    "\n",
    "Given one-hot encoded true label $ \\mathbf{y} $ and predicted probabilities $ \\hat{\\mathbf{y}} $, the cross-entropy loss is:\n",
    "\n",
    "$$\n",
    "\\text{CE}(\\mathbf{y}, \\hat{\\mathbf{y}}) = -\\sum_i y_i \\log(\\hat{y}_i)\n",
    "$$\n",
    "\n",
    "Since only one $ y_i = 1 $, this simplifies to:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = -\\log(\\hat{y}_{\\text{true class}})\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚úÖ Log-Softmax Identity\n",
    "\n",
    "Softmax followed by log simplifies to:\n",
    "\n",
    "$$\n",
    "\\log(\\text{softmax}(z_i)) = z_i - \\log\\left( \\sum_j e^{z_j} \\right)\n",
    "$$\n",
    "\n",
    "Used in practice as `log_softmax()` for **numerical stability** (avoids overflow).\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚úÖ Derivative of Softmax + Cross-Entropy\n",
    "\n",
    "Combined, the gradient becomes extremely clean:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z_i} = \\hat{y}_i - y_i\n",
    "$$\n",
    "\n",
    "- Just the **difference between predicted and actual class**\n",
    "- Avoids needing to separately backprop through softmax and log\n",
    "- Efficient and stable ‚Äî this is why it‚Äôs **always implemented as a single combined op** in PyTorch/TF\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Visual Insights\n",
    "\n",
    "- **Softmax**:\n",
    "  - With small logit differences ‚Üí soft probability distribution\n",
    "  - With large logit differences ‚Üí sharp confidence spike\n",
    "\n",
    "- **Cross-Entropy**:\n",
    "  - Loss is **low** when the predicted probability for the true class is **close to 1**\n",
    "  - Loss is **high** when the model is confident **but wrong**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî• Log-Softmax: Full Derivation + Gradient\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ 1. Recall Softmax\n",
    "\n",
    "Given logits:\n",
    "\n",
    "$$\n",
    "z = [z_1, z_2, \\dots, z_n]\n",
    "$$\n",
    "\n",
    "The softmax function is:\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = \\frac{e^{z_i}}{\\sum_j e^{z_j}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ 2. Derive Log-Softmax\n",
    "\n",
    "Take the log of softmax:\n",
    "\n",
    "$$\n",
    "\\log(\\hat{y}_i) = \\log\\left( \\frac{e^{z_i}}{\\sum_j e^{z_j}} \\right)\n",
    "= z_i - \\log \\left( \\sum_j e^{z_j} \\right)\n",
    "$$\n",
    "\n",
    "This is the **log-softmax identity**:\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\log(\\text{softmax}(z_i)) = z_i - \\log\\left( \\sum_j e^{z_j} \\right)\n",
    "}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ 3. Log-Softmax as a Function\n",
    "\n",
    "Let‚Äôs define:\n",
    "\n",
    "$$\n",
    "\\ell_i = \\log(\\text{softmax}(z_i)) = z_i - \\log \\left( \\sum_j e^{z_j} \\right)\n",
    "$$\n",
    "\n",
    "We want to compute the derivative:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\ell_i}{\\partial z_k}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### üî∏ Case 1: $i = k$\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\ell_i}{\\partial z_i} =\n",
    "\\frac{\\partial}{\\partial z_i} \\left( z_i - \\log \\sum_j e^{z_j} \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "Split it:\n",
    "\n",
    "- First term: $ \\frac{\\partial z_i}{\\partial z_i} = 1 $\n",
    "- Second term:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial z_i} \\log \\left( \\sum_j e^{z_j} \\right)\n",
    "= \\frac{e^{z_i}}{\\sum_j e^{z_j}} = \\hat{y}_i\n",
    "$$\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\ell_i}{\\partial z_i} = 1 - \\hat{y}_i\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### üî∏ Case 2: $i \\ne k$\n",
    "\n",
    "Only the second term depends on $z_k$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\ell_i}{\\partial z_k} = -\\frac{\\partial}{\\partial z_k} \\log \\left( \\sum_j e^{z_j} \\right)\n",
    "= -\\frac{e^{z_k}}{\\sum_j e^{z_j}} = -\\hat{y}_k\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ 4. Final Result ‚Äî Jacobian of Log-Softmax\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\ell_i}{\\partial z_k} =\n",
    "\\begin{cases}\n",
    "1 - \\hat{y}_i & \\text{if } i = k \\\\\n",
    "-\\hat{y}_k & \\text{if } i \\ne k\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "This is the **Jacobian matrix** of log-softmax, and it's used in general backprop.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ 5. Cross-Entropy with Log-Softmax (Combined)\n",
    "\n",
    "Cross-entropy loss:\n",
    "\n",
    "$$\n",
    "L = -\\sum_i y_i \\log(\\hat{y}_i)\n",
    "= -\\sum_i y_i \\cdot \\ell_i\n",
    "$$\n",
    "\n",
    "Now take derivative of $L$ w.r.t. logits $z_k$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z_k}\n",
    "= -\\sum_i y_i \\cdot \\frac{\\partial \\ell_i}{\\partial z_k}\n",
    "$$\n",
    "\n",
    "Now plug in the two cases from above:\n",
    "\n",
    "- When $i = k$: contributes $y_k (1 - \\hat{y}_k)$\n",
    "- When $i \\ne k$: contributes $y_i (-\\hat{y}_k)$\n",
    "\n",
    "So total derivative:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z_k} =\n",
    "- \\left( y_k (1 - \\hat{y}_k) + \\sum_{i \\ne k} y_i (-\\hat{y}_k) \\right)\n",
    "$$\n",
    "\n",
    "Factor out $\\hat{y}_k$:\n",
    "\n",
    "$$\n",
    "= -y_k (1 - \\hat{y}_k) + \\hat{y}_k \\sum_{i \\ne k} y_i\n",
    "$$\n",
    "\n",
    "Since $\\sum_i y_i = 1$, we know $\\sum_{i \\ne k} y_i = 1 - y_k$, so:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z_k} =\n",
    "- y_k + \\hat{y}_k\n",
    "$$\n",
    "\n",
    "Rewritten:\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\frac{\\partial L}{\\partial z_k} = \\hat{y}_k - y_k\n",
    "}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Final Takeaway\n",
    "\n",
    "- This is why softmax + cross-entropy are **always combined**\n",
    "- The gradient simplifies to:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\mathbf{z}} L = \\hat{\\mathbf{y}} - \\mathbf{y}\n",
    "$$\n",
    "\n",
    "- No need to manually backprop through softmax or log\n",
    "- Frameworks like PyTorch use this exact trick for `CrossEntropyLoss(logits, labels)`\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Summary\n",
    "\n",
    "| Component | Expression |\n",
    "|-----------|------------|\n",
    "| Log-Softmax | $ \\log(\\text{softmax}(z_i)) = z_i - \\log \\sum_j e^{z_j} $ |\n",
    "| $\\frac{\\partial \\ell_i}{\\partial z_k}$ | $ 1 - \\hat{y}_i$ if $i=k$, else $-\\hat{y}_k$ |\n",
    "| Cross-Entropy | $ L = -\\sum_i y_i \\log(\\hat{y}_i) $ |\n",
    "| Final Gradient | $ \\frac{\\partial L}{\\partial z_k} = \\hat{y}_k - y_k $ |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Log of Softmax ‚Äî What It Actually Means (Step-by-Step Breakdown)\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Goal:\n",
    "\n",
    "Understand the identity:\n",
    "\n",
    "$$\n",
    "\\log(\\text{softmax}(z_i)) = z_i - \\log \\left( \\sum_j e^{z_j} \\right)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ Example Logits:\n",
    "\n",
    "Let:\n",
    "\n",
    "$$\n",
    "z = [2.0, 1.0, 0.1]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Step 1: Compute Softmax\n",
    "\n",
    "The softmax formula is:\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = \\frac{e^{z_i}}{\\sum_j e^{z_j}}\n",
    "$$\n",
    "\n",
    "Numerically:\n",
    "\n",
    "- $e^2 \\approx 7.39$\n",
    "- $e^1 \\approx 2.72$\n",
    "- $e^{0.1} \\approx 1.105$\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "\\sum_j e^{z_j} \\approx 7.39 + 2.72 + 1.105 = 11.215\n",
    "$$\n",
    "\n",
    "Softmax outputs:\n",
    "\n",
    "- $\\hat{y}_0 = \\frac{7.39}{11.215} \\approx 0.659$\n",
    "- $\\hat{y}_1 = \\frac{2.72}{11.215} \\approx 0.242$\n",
    "- $\\hat{y}_2 = \\frac{1.105}{11.215} \\approx 0.099$\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Step 2: Take the Log of Softmax\n",
    "\n",
    "Now we compute:\n",
    "\n",
    "$$\n",
    "\\log(\\text{softmax}(z_i)) = \\log \\left( \\frac{e^{z_i}}{\\sum_j e^{z_j}} \\right)\n",
    "= \\log(e^{z_i}) - \\log\\left( \\sum_j e^{z_j} \\right)\n",
    "= z_i - \\log \\sum_j e^{z_j}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### üîç Numerical Example for $z_0 = 2.0$\n",
    "\n",
    "$$\n",
    "\\log(\\text{softmax}(2.0)) = 2.0 - \\log(11.215) \\approx 2.0 - 2.417 = -0.417\n",
    "$$\n",
    "\n",
    "Other entries:\n",
    "\n",
    "- $\\log(\\text{softmax}(1.0)) \\approx 1.0 - 2.417 = -1.417$\n",
    "- $\\log(\\text{softmax}(0.1)) \\approx 0.1 - 2.417 = -2.317$\n",
    "\n",
    "---\n",
    "\n",
    "### üß® Common Mistake Explained\n",
    "\n",
    "If you saw something like:\n",
    "\n",
    "$$\n",
    "2 - (-0.417) = 2.417 \\Rightarrow \\text{(wrong interpretation)}\n",
    "$$\n",
    "\n",
    "You probably did:\n",
    "\n",
    "$$\n",
    "2 - \\log(\\text{softmax}(2.0)) = 2 - \\log(0.659) \\approx 2 - (-0.417) = 2.417\n",
    "$$\n",
    "\n",
    "That‚Äôs **not** how log-softmax works ‚Äî that‚Äôs *backing out the log of the softmax probability*, not applying log to softmax directly.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Final Takeaway:\n",
    "\n",
    "To compute log-softmax **correctly**:\n",
    "\n",
    "$$\n",
    "\\log(\\text{softmax}(z_i)) = z_i - \\log \\sum_j e^{z_j}\n",
    "$$\n",
    "\n",
    "This is:\n",
    "- **Numerically stable**\n",
    "- **Logically correct**\n",
    "- **Used in all deep learning frameworks** (e.g., `log_softmax()` in PyTorch)\n",
    "\n",
    "---\n",
    "\n",
    "## üî• Combined Log-Softmax + Cross-Entropy ‚Äî Why Gradient is $\\hat{y} - y$\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ 1. What Happens Separately\n",
    "\n",
    "Logits:\n",
    "\n",
    "$$\n",
    "z = [2.0, 1.0, 0.1]\n",
    "$$\n",
    "\n",
    "Softmax:\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = \\frac{e^{z_i}}{\\sum_j e^{z_j}} \\Rightarrow \\hat{y} \\approx [0.71, 0.21, 0.08]\n",
    "$$\n",
    "\n",
    "Cross-entropy loss with true label $y = [1, 0, 0]$:\n",
    "\n",
    "$$\n",
    "L = -\\sum_i y_i \\log(\\hat{y}_i) = -\\log(0.71) \\approx 0.342\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ 2. What Happens When Combined\n",
    "\n",
    "Instead of computing softmax and log separately:\n",
    "\n",
    "$$\n",
    "\\log(\\text{softmax}(z_i)) = z_i - \\log \\sum_j e^{z_j}\n",
    "$$\n",
    "\n",
    "So cross-entropy becomes:\n",
    "\n",
    "$$\n",
    "L = -\\sum_i y_i \\cdot \\left(z_i - \\log \\sum_j e^{z_j}\\right)\n",
    "= -\\sum_i y_i z_i + \\log \\sum_j e^{z_j}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ 3. Derivative of Cross-Entropy w.r.t. Logits $z_k$\n",
    "\n",
    "Take derivative of:\n",
    "\n",
    "$$\n",
    "L = -\\sum_i y_i z_i + \\log \\sum_j e^{z_j}\n",
    "$$\n",
    "\n",
    "Split into two parts:\n",
    "\n",
    "#### (1) First term:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial z_k} \\left( -\\sum_i y_i z_i \\right) = -y_k\n",
    "$$\n",
    "\n",
    "#### (2) Second term:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial z_k} \\left( \\log \\sum_j e^{z_j} \\right) = \\frac{e^{z_k}}{\\sum_j e^{z_j}} = \\hat{y}_k\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Final Result:\n",
    "\n",
    "Add both parts:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z_k} = \\hat{y}_k - y_k\n",
    "$$\n",
    "\n",
    "This gives you **clean, simple gradients** from raw logits ‚Äî no need to manually softmax first.\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Summary\n",
    "\n",
    "| Concept                     | Formula |\n",
    "|----------------------------|---------|\n",
    "| Softmax                    | $ \\hat{y}_i = \\frac{e^{z_i}}{\\sum_j e^{z_j}} $ |\n",
    "| Log-Softmax                | $ \\log(\\hat{y}_i) = z_i - \\log \\sum_j e^{z_j} $ |\n",
    "| Cross-Entropy Loss         | $ L = -\\sum_i y_i \\log(\\hat{y}_i) $ |\n",
    "| Combined Derivative        | $ \\frac{\\partial L}{\\partial z_k} = \\hat{y}_k - y_k $ |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
