{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö 1. Basic Derivatives and Chain Rule\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ What is a Derivative?\n",
    "\n",
    "- Measures the **rate of change** of a function.\n",
    "- In deep learning, derivatives tell us **how much to adjust weights** to minimize loss.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "$$\n",
    "f(x) = x^2\n",
    "\\quad \\Rightarrow \\quad\n",
    "f'(x) = 2x\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Common Derivatives You Must Know\n",
    "\n",
    "| Function                  | Derivative                          |\n",
    "|----------------------------|-------------------------------------|\n",
    "| $ x^n $                    | $ nx^{n-1} $                       |\n",
    "| $ e^x $                    | $ e^x $                            |\n",
    "| $ \\ln(x) $                 | $ \\frac{1}{x} $                    |\n",
    "| $ \\sin(x) $                | $ \\cos(x) $                        |\n",
    "| $ \\cos(x) $                | $ -\\sin(x) $                       |\n",
    "| $ \\tanh(x) $               | $ 1 - \\tanh^2(x) $                 |\n",
    "| $ \\sigma(x) = \\frac{1}{1+e^{-x}} $ (Sigmoid) | $ \\sigma(x)(1-\\sigma(x)) $ |\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Chain Rule (Single-variable)\n",
    "\n",
    "If:\n",
    "\n",
    "$$\n",
    "y = f(g(x))\n",
    "$$\n",
    "\n",
    "then:\n",
    "\n",
    "$$\n",
    "\\frac{dy}{dx} = \\frac{dy}{dg} \\times \\frac{dg}{dx}\n",
    "$$\n",
    "\n",
    "**Intuition:**  \n",
    "You chain together how changes in $ x $ affect $ g(x) $, then how $ g(x) $ affects $ y $.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Chain Rule (Multivariable)\n",
    "\n",
    "Suppose:\n",
    "\n",
    "$$\n",
    "z = f(x, y), \\quad x = g(t), \\quad y = h(t)\n",
    "$$\n",
    "\n",
    "then:\n",
    "\n",
    "$$\n",
    "\\frac{dz}{dt} = \\frac{\\partial z}{\\partial x} \\frac{dx}{dt} + \\frac{\\partial z}{\\partial y} \\frac{dy}{dt}\n",
    "$$\n",
    "\n",
    "- Multivariable chain rule **adds up contributions** from each path.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Product Rule\n",
    "\n",
    "If:\n",
    "\n",
    "$$\n",
    "f(x) = u(x) v(x)\n",
    "$$\n",
    "\n",
    "then:\n",
    "\n",
    "$$\n",
    "f'(x) = u'(x) v(x) + u(x) v'(x)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Quotient Rule\n",
    "\n",
    "If:\n",
    "\n",
    "$$\n",
    "f(x) = \\frac{u(x)}{v(x)}\n",
    "$$\n",
    "\n",
    "then:\n",
    "\n",
    "$$\n",
    "f'(x) = \\frac{u'(x) v(x) - u(x) v'(x)}{v(x)^2}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Key Insight:\n",
    "\n",
    "- **Chain Rule** glues functions together.\n",
    "- **Product/Quotient Rules** handle multiplying/dividing functions.\n",
    "- **These tools are the heart of backpropagation.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö 2. Multivariable Calculus (Partial Derivatives, Gradients, Jacobians)\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Partial Derivatives\n",
    "\n",
    "- When a function depends on multiple variables, we take the **partial derivative** with respect to one variable at a time, treating the others as constants.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "If:\n",
    "\n",
    "$$\n",
    "f(x, y) = x^2 + 3xy + y^2\n",
    "$$\n",
    "\n",
    "then:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x} = 2x + 3y\n",
    "\\quad\\quad\n",
    "\\frac{\\partial f}{\\partial y} = 3x + 2y\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Gradient Vector\n",
    "\n",
    "- The **gradient** is a vector of all partial derivatives.\n",
    "- Points in the direction of steepest increase.\n",
    "\n",
    "**Definition:**\n",
    "\n",
    "$$\n",
    "\\nabla f(x, y) =\n",
    "\\left[\n",
    "\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Jacobian Matrix\n",
    "\n",
    "- Generalizes gradients when output is a vector (multi-output functions).\n",
    "\n",
    "Suppose:\n",
    "\n",
    "$$\n",
    "\\mathbf{y} =\n",
    "\\begin{bmatrix}\n",
    "y_1(x_1, x_2, ..., x_n) \\\\\n",
    "y_2(x_1, x_2, ..., x_n) \\\\\n",
    "\\vdots \\\\\n",
    "y_m(x_1, x_2, ..., x_n)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "then the **Jacobian** is:\n",
    "\n",
    "$$\n",
    "J_{ij} = \\frac{\\partial y_i}{\\partial x_j}\n",
    "$$\n",
    "\n",
    "- Rows = output components\n",
    "- Columns = input components\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Why Jacobians Matter in Deep Learning\n",
    "\n",
    "- **Backpropagation** across layers uses **Jacobian matrices**.\n",
    "- Especially crucial for operations like **softmax**, where each output depends on all inputs.\n",
    "- Allows efficient calculation of gradients across complex compositions.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Key Intuition:\n",
    "\n",
    "- **Partial derivatives**: \"How sensitive is output to one input?\"\n",
    "- **Gradient**: \"Which way should I move to increase output fastest?\"\n",
    "- **Jacobian**: \"How do multiple outputs change with multiple inputs?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¢ Deep Learning Math Essentials: Exponentials, Logarithms, and Their Role in Loss Functions\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ What is $e$?\n",
    "\n",
    "- $e \\approx 2.718$ is Euler's number.\n",
    "- Defined by:\n",
    "\n",
    "$$\n",
    "e = \\lim_{n \\to \\infty} \\left(1 + \\frac{1}{n} \\right)^n\n",
    "$$\n",
    "\n",
    "- Unique property:\n",
    "\n",
    "$$\n",
    "\\frac{d}{dx} e^x = e^x\n",
    "$$\n",
    "\n",
    "Used heavily in deep learning because it provides smooth, always-positive, non-vanishing gradients.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Exponential Function: $e^x$\n",
    "\n",
    "- Always positive: $e^x > 0$\n",
    "- Grows rapidly as $x \\to \\infty$\n",
    "- Flattens near zero as $x \\to -\\infty$\n",
    "- Derivative:\n",
    "\n",
    "$$\n",
    "\\frac{d}{dx} e^x = e^x\n",
    "$$\n",
    "\n",
    "- Operations:\n",
    "  - $e^{a + b} = e^a \\cdot e^b$\n",
    "  - $e^{a - b} = \\frac{e^a}{e^b}$\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Logarithmic Function: $\\log(x)$\n",
    "\n",
    "- Inverse of $e^x$\n",
    "- Defined as:\n",
    "\n",
    "$$\n",
    "\\log(x) = y \\iff e^y = x\n",
    "$$\n",
    "\n",
    "- Only defined for $x > 0$\n",
    "- Grows slowly, explodes negatively as $x \\to 0^+$\n",
    "- Derivative:\n",
    "\n",
    "$$\n",
    "\\frac{d}{dx} \\log(x) = \\frac{1}{x}\n",
    "$$\n",
    "\n",
    "- Operation rules:\n",
    "  - $\\log(ab) = \\log a + \\log b$\n",
    "  - $\\log\\left(\\frac{a}{b}\\right) = \\log a - \\log b$\n",
    "  - $\\log(a^b) = b \\log a$\n",
    "  - $\\log(e^x) = x$\n",
    "  - $e^{\\log(x)} = x$\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Inverse Identity\n",
    "\n",
    "Exponentials and logs undo each other:\n",
    "\n",
    "$$\n",
    "\\log(e^x) = x \\quad \\text{and} \\quad e^{\\log(x)} = x\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Why Use $\\log$ and $e^x$ in Deep Learning?\n",
    "\n",
    "| Purpose                            | Example                                           | Why                         |\n",
    "|-----------------------------------|---------------------------------------------------|------------------------------|\n",
    "| Convert scores to probabilities   | $\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}}$ | $e^x$ exaggerates differences |\n",
    "| Stabilize products                | $\\log(p_1 \\cdot p_2) = \\log p_1 + \\log p_2$       | Avoids underflow             |\n",
    "| Gradient-based optimization       | $\\frac{d}{dx} e^x,\\ \\frac{d}{dx} \\log(x)$         | Smooth derivatives           |\n",
    "\n",
    "---\n",
    "\n",
    "![Alt Text](log_e_viz.png)\n",
    "\n",
    "\n",
    "### üîπ Binary Cross-Entropy (BCE)\n",
    "\n",
    "Used in binary classification tasks:\n",
    "\n",
    "$$\n",
    "\\text{BCE}(\\hat{y}, y) = -[y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y})]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $y \\in \\{0, 1\\}$ is the true label\n",
    "- $\\hat{y} \\in (0, 1)$ is the predicted probability\n",
    "\n",
    "Special cases:\n",
    "- If $y = 1$: $\\text{Loss} = -\\log(\\hat{y})$\n",
    "- If $y = 0$: $\\text{Loss} = -\\log(1 - \\hat{y})$\n",
    "\n",
    "Gradients:\n",
    "- If $y = 1$: $\\frac{dL}{d\\hat{y}} = -\\frac{1}{\\hat{y}}$\n",
    "- If $y = 0$: $\\frac{dL}{d\\hat{y}} = \\frac{1}{1 - \\hat{y}}$\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Softmax Function\n",
    "\n",
    "Used to convert logits to a probability distribution:\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = \\frac{e^{z_i}}{\\sum_j e^{z_j}}\n",
    "$$\n",
    "\n",
    "Properties:\n",
    "- $\\hat{y}_i \\in (0, 1)$\n",
    "- $\\sum_i \\hat{y}_i = 1$\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Cross-Entropy Loss (Multiclass)\n",
    "\n",
    "With one-hot true labels:\n",
    "\n",
    "$$\n",
    "\\text{CE}(y, \\hat{y}) = -\\sum_i y_i \\log(\\hat{y}_i)\n",
    "$$\n",
    "\n",
    "If class $k$ is true, and $y_k = 1$:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = -\\log(\\hat{y}_k)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Log-Softmax Trick\n",
    "\n",
    "Instead of computing:\n",
    "\n",
    "$$\n",
    "\\log(\\text{softmax}(z_i)) = \\log\\left( \\frac{e^{z_i}}{\\sum_j e^{z_j}} \\right)\n",
    "$$\n",
    "\n",
    "Use:\n",
    "\n",
    "$$\n",
    "\\log(\\text{softmax}(z_i)) = z_i - \\log\\left(\\sum_j e^{z_j}\\right)\n",
    "$$\n",
    "\n",
    "This is numerically stable and gives clean gradients:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z_i} = \\hat{y}_i - y_i\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ KL Divergence\n",
    "\n",
    "Measures how much one distribution $Q$ diverges from a true distribution $P$:\n",
    "\n",
    "$$\n",
    "D_{\\text{KL}}(P \\parallel Q) = \\sum_i P(i) \\log\\left(\\frac{P(i)}{Q(i)}\\right)\n",
    "$$\n",
    "\n",
    "Can be rewritten as:\n",
    "\n",
    "$$\n",
    "D_{\\text{KL}}(P \\parallel Q) = H(P, Q) - H(P)\n",
    "$$\n",
    "\n",
    "- $H(P, Q)$ = cross-entropy\n",
    "- $H(P)$ = entropy of true distribution\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Final Summary\n",
    "\n",
    "| Concept             | Formula                                                        | Use Case                     |\n",
    "|---------------------|----------------------------------------------------------------|------------------------------|\n",
    "| Exponential         | $e^x$                                                          | Amplify scores (softmax)     |\n",
    "| Logarithm           | $\\log(x)$                                                      | Stabilize, inverse of $e^x$  |\n",
    "| BCE                 | $- [y \\log(\\hat{y}) + (1-y) \\log(1 - \\hat{y})]$                | Binary classification        |\n",
    "| Softmax             | $\\hat{y}_i = \\frac{e^{z_i}}{\\sum_j e^{z_j}}$                  | Convert logits to probs      |\n",
    "| Cross-Entropy       | $-\\sum_i y_i \\log(\\hat{y}_i)$                                   | Multiclass classification    |\n",
    "| Log-Softmax         | $z_i - \\log \\sum_j e^{z_j}$                                     | Numerically stable softmax   |\n",
    "| KL Divergence       | $\\sum_i P(i) \\log \\frac{P(i)}{Q(i)}$                            | Measure distribution mismatch|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö 3. Activation Functions and Their Derivatives\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Sigmoid Activation\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "**Derivative:**\n",
    "\n",
    "$$\n",
    "\\sigma'(x) = \\sigma(x) (1 - \\sigma(x))\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Tanh Activation\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$\n",
    "\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "$$\n",
    "\n",
    "**Derivative:**\n",
    "\n",
    "$$\n",
    "\\frac{d}{dx} \\tanh(x) = 1 - \\tanh^2(x)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "![activation_sigmoid_tanh](activations.png)\n",
    "\n",
    "## üîπ ReLU (Rectified Linear Unit)\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$\n",
    "\\text{ReLU}(x) = \\max(0, x)\n",
    "$$\n",
    "\n",
    "**Derivative:**\n",
    "\n",
    "$$\n",
    "\\text{ReLU}'(x) =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } x > 0 \\\\\n",
    "0 & \\text{if } x \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**Intuition:**\n",
    "- Keeps positive values, zeros out negatives.\n",
    "- Prevents vanishing gradients for \\( x > 0 \\).\n",
    "- Issue: Dead neurons when \\( x \\leq 0 \\).\n",
    "\n",
    "![relu](relu_viz.png)\n",
    "---\n",
    "\n",
    "## üîπ Leaky ReLU\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$\n",
    "\\text{LeakyReLU}(x) =\n",
    "\\begin{cases}\n",
    "x & \\text{if } x > 0 \\\\\n",
    "\\alpha x & \\text{if } x \\leq 0\n",
    "\\end{cases}\n",
    "\\quad \\text{where } \\alpha \\in [0.01, 0.1]\n",
    "$$\n",
    "\n",
    "**Derivative:**\n",
    "\n",
    "$$\n",
    "\\text{LeakyReLU}'(x) =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } x > 0 \\\\\n",
    "\\alpha & \\text{if } x \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**Fix:**\n",
    "- Allows small gradient for \\( x < 0 \\), preventing dead neurons.\n",
    "\n",
    "![leaky_relu](leaky_relu.png)\n",
    "---\n",
    "\n",
    "## üîπ GELU (Gaussian Error Linear Unit)\n",
    "\n",
    "**Exact Formula:**\n",
    "\n",
    "$$\n",
    "\\text{GELU}(x) = x \\cdot \\Phi(x)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "\\Phi(x) = \\frac{1}{2} \\left[ 1 + \\text{erf}\\left( \\frac{x}{\\sqrt{2}} \\right) \\right]\n",
    "$$\n",
    "\n",
    "**Derivative (using Product Rule):**\n",
    "\n",
    "$$\n",
    "\\frac{d}{dx} \\text{GELU}(x) = \\Phi(x) + x \\cdot \\phi(x)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "\\phi(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-x^2/2}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Approximate Formula (used in practice):**\n",
    "\n",
    "$$\n",
    "\\text{GELU}(x) \\approx 0.5x \\left[ 1 + \\tanh\\left( \\sqrt{\\frac{2}{\\pi}} (x + 0.044715x^3) \\right) \\right]\n",
    "$$\n",
    "\n",
    "**Notes:**\n",
    "- Smooth version of ReLU.\n",
    "- Default activation in Transformers like **BERT** and **GPT**.\n",
    "- Encourages smoother gradient flows for better optimization.\n",
    "\n",
    "![gelu](gelu_viz.png)\n",
    "---\n",
    "\n",
    "## üß† Key Intuition:\n",
    "\n",
    "- Activations introduce **non-linearities** ‚Üí allow deep networks to approximate complex functions.\n",
    "- **ReLU variants** (Leaky ReLU, GELU) address dead neuron issues or smoothness needs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö 4. Softmax and Cross-Entropy: Full Gradient Derivations\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Softmax Function\n",
    "\n",
    "Given logits vector \\( \\mathbf{z} \\), the softmax outputs probabilities:\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = \\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}}\n",
    "$$\n",
    "\n",
    "Properties:\n",
    "- $$ 0 < \\hat{y}_i < 1 $$\n",
    "- $$ \\sum_i \\hat{y}_i = 1 $$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Cross-Entropy Loss\n",
    "\n",
    "Given:\n",
    "- True labels \\( \\mathbf{y} \\) (one-hot vector)\n",
    "- Predicted probabilities \\( \\hat{\\mathbf{y}} \\)\n",
    "\n",
    "The cross-entropy loss is:\n",
    "\n",
    "$$\n",
    "L = -\\sum_i y_i \\log(\\hat{y}_i)\n",
    "$$\n",
    "\n",
    "Since only one \\( y_i = 1 \\) in a one-hot vector:\n",
    "\n",
    "$$\n",
    "L = -\\log(\\hat{y}_{\\text{true class}})\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "# üöÄ Full Derivation: Gradient of Softmax w.r.t. Logits\n",
    "\n",
    "We want:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\hat{y}_i}{\\partial z_j}\n",
    "$$\n",
    "\n",
    "Expand:\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = \\frac{e^{z_i}}{\\sum_k e^{z_k}}\n",
    "$$\n",
    "\n",
    "Apply quotient rule carefully:\n",
    "\n",
    "---\n",
    "\n",
    "### üî∏ Case 1: \\( i = j \\)\n",
    "\n",
    "When differentiating \\( \\hat{y}_i \\) w.r.t. \\( z_i \\):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\hat{y}_i}{\\partial z_i} =\n",
    "\\frac{e^{z_i} (\\sum_k e^{z_k}) - e^{z_i} e^{z_i}}{(\\sum_k e^{z_k})^2}\n",
    "= \\hat{y}_i (1 - \\hat{y}_i)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### üî∏ Case 2: \\( i != j \\)\n",
    "\n",
    "When differentiating \\( \\hat{y}_i \\) w.r.t. a different \\( z_j \\):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\hat{y}_i}{\\partial z_j} =\n",
    "\\frac{0 \\cdot (\\sum_k e^{z_k}) - e^{z_i} e^{z_j}}{(\\sum_k e^{z_k})^2}\n",
    "= -\\hat{y}_i \\hat{y}_j\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "# üöÄ Now: Derivative of Loss w.r.t. Logits \\( z \\)\n",
    "\n",
    "We derive two ways:\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Method 1: Using Chain Rule and Gradient of Softmax\n",
    "\n",
    "By chain rule:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z_j} = \\sum_i \\frac{\\partial L}{\\partial \\hat{y}_i} \\frac{\\partial \\hat{y}_i}{\\partial z_j}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\hat{y}_i} = -\\frac{y_i}{\\hat{y}_i}\n",
    "$$\n",
    "\n",
    "Thus:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z_j} = -\\sum_i \\frac{y_i}{\\hat{y}_i} \\frac{\\partial \\hat{y}_i}{\\partial z_j}\n",
    "$$\n",
    "\n",
    "Substituting the softmax gradients:\n",
    "\n",
    "- For \\( i = j \\):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\hat{y}_i}{\\partial z_i} = \\hat{y}_i (1 - \\hat{y}_i)\n",
    "$$\n",
    "\n",
    "- For \\( i != j \\):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\hat{y}_i}{\\partial z_j} = -\\hat{y}_i \\hat{y}_j\n",
    "$$\n",
    "\n",
    "Splitting sums:\n",
    "\n",
    "$$\n",
    "= -\\left( \\frac{y_j}{\\hat{y}_j} \\hat{y}_j (1-\\hat{y}_j) + \\sum_{i \\neq j} \\frac{y_i}{\\hat{y}_i} (-\\hat{y}_i \\hat{y}_j) \\right)\n",
    "$$\n",
    "\n",
    "Simplify:\n",
    "\n",
    "First term:\n",
    "\n",
    "$$\n",
    "- y_j (1-\\hat{y}_j)\n",
    "$$\n",
    "\n",
    "Second term:\n",
    "\n",
    "$$\n",
    "+ \\hat{y}_j \\sum_{i \\neq j} y_i\n",
    "$$\n",
    "\n",
    "Since:\n",
    "\n",
    "$$\n",
    "\\sum_i y_i = 1\n",
    "\\quad \\Rightarrow \\quad\n",
    "\\sum_{i \\neq j} y_i = 1 - y_j\n",
    "$$\n",
    "\n",
    "Thus:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z_j} = -y_j (1-\\hat{y}_j) + \\hat{y}_j (1-y_j)\n",
    "$$\n",
    "\n",
    "Expand:\n",
    "\n",
    "$$\n",
    "= -y_j + y_j \\hat{y}_j + \\hat{y}_j - \\hat{y}_j y_j\n",
    "$$\n",
    "\n",
    "Simplify:\n",
    "\n",
    "$$\n",
    "= \\hat{y}_j - y_j\n",
    "$$\n",
    "\n",
    "‚úÖ Final result.\n",
    "\n",
    "---\n",
    "\n",
    "![backprop]( cross_enropy_gradient_wrt_z_via_traditional_softmax_dervitatives_pt_1.png)\n",
    "![backprop](cross_enropy_gradient_wrt_z_via_softmax_dervitatives_pt_2.png)\n",
    "\n",
    "## üîπ Method 2: Using Log-Sum-Exp Trick Directly\n",
    "\n",
    "Expand cross-entropy loss:\n",
    "\n",
    "First:\n",
    "\n",
    "$$\n",
    "L = -\\sum_i y_i \\log(\\hat{y}_i)\n",
    "$$\n",
    "\n",
    "Substitute \\( \\hat{y}_i = \\frac{e^{z_i}}{\\sum_k e^{z_k}} \\):\n",
    "\n",
    "$$\n",
    "= -\\sum_i y_i (z_i - \\log\\sum_k e^{z_k})\n",
    "$$\n",
    "\n",
    "Expand:\n",
    "\n",
    "$$\n",
    "= -\\sum_i y_i z_i + \\log\\sum_k e^{z_k}\n",
    "$$\n",
    "\n",
    "Now differentiate w.r.t. \\( z_j \\):\n",
    "\n",
    "First term:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial z_j} \\left( -\\sum_i y_i z_i \\right) = -y_j\n",
    "$$\n",
    "\n",
    "Second term:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial z_j} \\log\\sum_k e^{z_k} = \\frac{e^{z_j}}{\\sum_k e^{z_k}} = \\hat{y}_j\n",
    "$$\n",
    "\n",
    "Thus:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z_j} = \\hat{y}_j - y_j\n",
    "$$\n",
    "\n",
    "‚úÖ Same clean result.\n",
    "\n",
    "---\n",
    "\n",
    "# üéØ Final Gradient Summary\n",
    "\n",
    "Whether you differentiate via:\n",
    "\n",
    "- Chain rule using softmax gradient\n",
    "- Log-sum-exp trick\n",
    "\n",
    "**You always get:**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z_j} = \\hat{y}_j - y_j\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "# üß† Key Takeaways\n",
    "\n",
    "- Softmax derivatives differ depending if \\( i = j \\) or \\( i != j \\).\n",
    "- Cross-entropy loss with softmax makes backpropagation efficient.\n",
    "- Deep learning libraries (e.g., PyTorch, TensorFlow) fuse softmax + cross-entropy for numerical stability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÅ Final Result ‚Äî Jacobian of the Softmax Function\n",
    "\n",
    "---\n",
    "\n",
    "### üß† What Are We Doing?\n",
    "\n",
    "We want to compute the **Jacobian matrix** of the softmax function:\n",
    "\n",
    "Given logits:\n",
    "\n",
    "$$\n",
    "\\mathbf{z} = [z_1, z_2, \\dots, z_n]\n",
    "$$\n",
    "\n",
    "The softmax output is:\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = \\frac{e^{z_i}}{\\sum_k e^{z_k}} = \\frac{e^{z_i}}{S}, \\quad \\text{where } S = \\sum_k e^{z_k}\n",
    "$$\n",
    "\n",
    "Our goal:  \n",
    "Compute the partial derivatives:\n",
    "\n",
    "$$\n",
    "J_{ij} = \\frac{\\partial \\hat{y}_i}{\\partial z_j}\n",
    "$$\n",
    "\n",
    "This tells us **how each softmax output** $\\hat{y}_i$ changes if we **nudge the input logit** $z_j$.\n",
    "\n",
    "---\n",
    "\n",
    "### üîß Step-by-Step Derivation\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚úÖ Case 1: $i = j$\n",
    "\n",
    "We're looking at:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\hat{y}_i}{\\partial z_i}\n",
    "$$\n",
    "\n",
    "Using the quotient rule:\n",
    "\n",
    "Let:\n",
    "- $u = e^{z_i}$\n",
    "- $v = S = \\sum_k e^{z_k}$\n",
    "- $u' = e^{z_i}$\n",
    "- $v' = e^{z_i}$ (since only one term in the sum depends on $z_i$)\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\hat{y}_i}{\\partial z_i}\n",
    "= \\frac{u' \\cdot v - u \\cdot v'}{v^2}\n",
    "= \\frac{e^{z_i} \\cdot S - e^{z_i} \\cdot e^{z_i}}{S^2}\n",
    "$$\n",
    "\n",
    "Factor:\n",
    "\n",
    "$$\n",
    "= \\frac{e^{z_i}}{S} \\left(1 - \\frac{e^{z_i}}{S} \\right)\n",
    "= \\hat{y}_i (1 - \\hat{y}_i)\n",
    "$$\n",
    "\n",
    "‚úÖ This is the **diagonal of the Jacobian**\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚úÖ Case 2: $i \\ne j$\n",
    "\n",
    "Now $z_j$ doesn‚Äôt affect the numerator $e^{z_i}$, but **does** affect the denominator:\n",
    "\n",
    "Let:\n",
    "- $u = e^{z_i}$ ‚Üí constant\n",
    "- $v = S = \\sum_k e^{z_k}$\n",
    "- $u' = 0$\n",
    "- $v' = e^{z_j}$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\hat{y}_i}{\\partial z_j}\n",
    "= \\frac{0 \\cdot S - e^{z_i} \\cdot e^{z_j}}{S^2}\n",
    "= - \\frac{e^{z_i} e^{z_j}}{S^2}\n",
    "= -\\hat{y}_i \\hat{y}_j\n",
    "$$\n",
    "\n",
    "‚úÖ These are the **off-diagonal entries**\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Final Formula (Unified for All $i, j$):\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\frac{\\partial \\hat{y}_i}{\\partial z_j} = \\hat{y}_i \\left( \\delta_{ij} - \\hat{y}_j \\right)\n",
    "}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\delta_{ij} = 1$ if $i = j$, else $0$  \n",
    "  (this is the **Kronecker delta** ‚Äî a switch that says: ‚Äúare we on the diagonal?‚Äù)\n",
    "\n",
    "---\n",
    "\n",
    "### üîÅ Matrix Form:\n",
    "\n",
    "Let $\\hat{\\mathbf{y}} \\in \\mathbb{R}^n$ be the softmax output vector. Then:\n",
    "\n",
    "- Diagonal matrix:\n",
    "\n",
    "$$\n",
    "\\text{diag}(\\hat{\\mathbf{y}}) =\n",
    "\\begin{bmatrix}\n",
    "\\hat{y}_1 & 0 & \\dots & 0 \\\\\n",
    "0 & \\hat{y}_2 & \\dots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & \\dots & \\hat{y}_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- Outer product:\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{y}} \\hat{\\mathbf{y}}^\\top =\n",
    "\\begin{bmatrix}\n",
    "\\hat{y}_1^2 & \\hat{y}_1 \\hat{y}_2 & \\dots \\\\\n",
    "\\hat{y}_2 \\hat{y}_1 & \\hat{y}_2^2 & \\dots \\\\\n",
    "\\vdots & \\vdots & \\ddots\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "So the **Jacobian of softmax is:**\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "J = \\text{diag}(\\hat{\\mathbf{y}}) - \\hat{\\mathbf{y}} \\hat{\\mathbf{y}}^\\top\n",
    "}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### üîç What It Really Means:\n",
    "\n",
    "- **Diagonal entries**: $\\hat{y}_i (1 - \\hat{y}_i)$  \n",
    "  ‚Üí If I increase my own logit $z_i$, my probability goes up (but never past 1)\n",
    "\n",
    "- **Off-diagonal entries**: $-\\hat{y}_i \\hat{y}_j$  \n",
    "  ‚Üí If another logit $z_j$ increases, it steals probability mass from $z_i$\n",
    "\n",
    "---\n",
    "\n",
    "### üí° TL;DR\n",
    "\n",
    "> **Softmax Jacobian** =  \n",
    "> *\"I get a diagonal boost if I nudge my own logit...  \n",
    "> but I lose ground if any other logit rises.\"*\n",
    "\n",
    "That‚Äôs how softmax redistributes probability mass ‚Äî and why it‚Äôs perfect for multiclass classification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö 5. Full Chain Rule: Backpropagation through Composite Layers\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Setup: A Simple 3-Layer Neural Network\n",
    "\n",
    "Suppose we have:\n",
    "\n",
    "1. **Input** \\( x \\)\n",
    "2. **Hidden layer 1 output**:\n",
    "\n",
    "$$\n",
    "h_1 = f_1(W_1 x + b_1)\n",
    "$$\n",
    "\n",
    "3. **Hidden layer 2 output**:\n",
    "\n",
    "$$\n",
    "h_2 = f_2(W_2 h_1 + b_2)\n",
    "$$\n",
    "\n",
    "4. **Final output (logits)**:\n",
    "\n",
    "$$\n",
    "z = W_3 h_2 + b_3\n",
    "$$\n",
    "\n",
    "5. **Predictions** (after softmax):\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\text{softmax}(z)\n",
    "$$\n",
    "\n",
    "6. **Loss**:\n",
    "\n",
    "$$\n",
    "L = \\text{CrossEntropy}(\\hat{y}, y)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Goal\n",
    "\n",
    "We want to compute:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_1}, \\quad \\frac{\\partial L}{\\partial W_2}, \\quad \\frac{\\partial L}{\\partial W_3}\n",
    "$$\n",
    "\n",
    "using **backpropagation**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Step-by-Step Chain of Gradients\n",
    "\n",
    "1. From **loss to logits**:\n",
    "\n",
    "Already derived earlier:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z} = \\hat{y} - y\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "2. From **logits to second hidden layer**:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial h_2} = \\frac{\\partial L}{\\partial z} W_3^T\n",
    "$$\n",
    "\n",
    "because:\n",
    "\n",
    "$$\n",
    "z = W_3 h_2 + b_3\n",
    "$$\n",
    "\n",
    "and the derivative of a linear transformation is just the matrix transpose.\n",
    "\n",
    "---\n",
    "\n",
    "3. From **second hidden layer to first hidden layer**:\n",
    "\n",
    "Apply chain rule through activation \\( f_2 \\):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial (W_2 h_1 + b_2)} = \\frac{\\partial L}{\\partial h_2} \\circ f_2'(W_2 h_1 + b_2)\n",
    "$$\n",
    "\n",
    "where \\( \\circ \\) denotes elementwise multiplication (Hadamard product).\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_2} = \\left( \\frac{\\partial L}{\\partial (W_2 h_1 + b_2)} \\right) h_1^T\n",
    "$$\n",
    "\n",
    "because:\n",
    "\n",
    "$$\n",
    "W_2 h_1 + b_2\n",
    "$$\n",
    "\n",
    "is a linear transformation.\n",
    "\n",
    "---\n",
    "\n",
    "4. From **first hidden layer back to input**:\n",
    "\n",
    "Similarly:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial h_1} = \\left( \\frac{\\partial L}{\\partial (W_2 h_1 + b_2)} \\right) W_2^T\n",
    "$$\n",
    "\n",
    "Then chain rule through \\( f_1 \\):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial (W_1 x + b_1)} = \\frac{\\partial L}{\\partial h_1} \\circ f_1'(W_1 x + b_1)\n",
    "$$\n",
    "\n",
    "Thus:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_1} = \\left( \\frac{\\partial L}{\\partial (W_1 x + b_1)} \\right) x^T\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Visual Flow of Backpropagation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![backprop](back_prop_computation_graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each step:\n",
    "- Multiply by the weight matrix transpose\n",
    "- Apply elementwise derivative of activation function\n",
    "- Keep chaining backward\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Key Intuitions:\n",
    "\n",
    "- **Each layer's gradient** depends on the **gradients from layers after it**.\n",
    "- **Matrix transposes** appear naturally from how linear layers work.\n",
    "- **Elementwise multiplications** appear naturally from activations like ReLU, tanh, sigmoid.\n",
    "- **Backpropagation is just repeated chain rule** ‚Äî no magic, only bookkeeping!\n",
    "\n",
    "---\n",
    "\n",
    "# üéØ Summary of Full Backprop Chain\n",
    "\n",
    "| Gradient | Formula |\n",
    "|----------|---------|\n",
    "| $ \\frac{\\partial L}{\\partial z} $ | $ \\hat{y} - y $ |\n",
    "| $ \\frac{\\partial L}{\\partial W_3} $ | $ (\\hat{y} - y) h_2^T $ |\n",
    "| $ \\frac{\\partial L}{\\partial h_2} $ | $ (\\hat{y} - y) W_3^T $ |\n",
    "| $ \\frac{\\partial L}{\\partial W_2} $ | $ (\\partial L / \\partial h_2) \\circ f_2'(W_2 h_1 + b_2) \\times h_1^T $ |\n",
    "| $ \\frac{\\partial L}{\\partial h_1} $ | $ ((\\partial L / \\partial h_2) \\circ f_2') W_2^T $ |\n",
    "| $ \\frac{\\partial L}{\\partial W_1} $ | $ (\\partial L / \\partial h_1) \\circ f_1'(W_1 x + b_1) \\times x^T $ |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîê Log-Softmax and the Log-Sum-Exp Trick\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ The Problem\n",
    "\n",
    "When computing:\n",
    "\n",
    "$$\n",
    "\\log(\\text{softmax}(z_i)) = \\log\\left( \\frac{e^{z_i}}{\\sum_j e^{z_j}} \\right)\n",
    "= z_i - \\log\\left( \\sum_j e^{z_j} \\right)\n",
    "$$\n",
    "\n",
    "You're at risk of:\n",
    "\n",
    "- **Overflow** if any $z_j$ is large (e.g. $e^{1000}$)\n",
    "- **Underflow** if softmax returns very small values (log of near-zero ‚Üí $-\\infty$)\n",
    "- **NaNs** in training and gradient instability\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ The Log-Sum-Exp Trick (LSE)\n",
    "\n",
    "To stabilize:\n",
    "\n",
    "$$\n",
    "\\log \\sum_j e^{z_j}\n",
    "$$\n",
    "\n",
    "We apply:\n",
    "\n",
    "$$\n",
    "\\log \\sum_j e^{z_j} = \\max_j z_j + \\log \\sum_j e^{z_j - \\max_j z_j}\n",
    "$$\n",
    "\n",
    "This avoids numerical overflow by subtracting the largest logit before exponentiation.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Used in Practice: `log_softmax`\n",
    "\n",
    "Instead of doing:\n",
    "\n",
    "```python\n",
    "log_probs = torch.log(torch.softmax(logits, dim=-1))\n",
    "```\n",
    "\n",
    "Which is unsafe‚Ä¶\n",
    "\n",
    "Use:\n",
    "\n",
    "```python\n",
    "log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "```\n",
    "\n",
    "This is:\n",
    "\n",
    "- **Numerically stable** (uses log-sum-exp trick internally)\n",
    "- **Efficient** (avoids extra computation)\n",
    "- **Safe for backprop** (no NaNs, exploding gradients)\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Log-Softmax Identity\n",
    "\n",
    "The math behind `log_softmax`:\n",
    "\n",
    "$$\n",
    "\\log(\\text{softmax}(z_i)) = z_i - \\log \\sum_j e^{z_j}\n",
    "$$\n",
    "\n",
    "With the LSE trick applied:\n",
    "\n",
    "$$\n",
    "= z_i - \\left[ \\max_j z_j + \\log \\sum_j e^{z_j - \\max_j z_j} \\right]\n",
    "$$\n",
    "\n",
    "This ensures the entire operation is stable, even if logits are huge.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Why You Shouldn‚Äôt Do `log(softmax(...))`\n",
    "\n",
    "| Issue                        | Result                        |\n",
    "|-----------------------------|-------------------------------|\n",
    "| $e^x$ on large logits        | Overflow / `inf`              |\n",
    "| Dividing large exponentials | Loss of precision             |\n",
    "| Taking $\\log(0)$             | `-inf`                        |\n",
    "| Manual implementation       | No use of LSE trick           |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ `log_softmax` Summary\n",
    "\n",
    "| Property                  | Description                                      |\n",
    "|---------------------------|--------------------------------------------------|\n",
    "| Input                     | Raw logits (unbounded real numbers)             |\n",
    "| Output                    | Log-probabilities (sums to 1 in log space)      |\n",
    "| Implementation            | Uses log-sum-exp trick internally               |\n",
    "| Use case                  | Preferred for NLL, CE, KL, language modeling    |\n",
    "| Gradient                  | Clean: $\\nabla_{z_i} L = \\hat{y}_i - y_i$       |\n",
    "| Alternative to            | `log(softmax(z))` (don‚Äôt do this)              |\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Final Mental Model\n",
    "\n",
    "- `softmax` ‚Üí converts scores to probs (risk of overflow)\n",
    "- `log(softmax(...))` ‚Üí unstable unless you apply LSE manually\n",
    "- `log_softmax` ‚Üí **optimized fusion** with **log-sum-exp built in**\n",
    "\n",
    "> **Moral of the story:**  \n",
    "> üíØ Always use `log_softmax` when working with log-probabilities from logits.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî• KL Divergence in Deep Learning ‚Äî Full Intuition + Math\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ What is KL Divergence?\n",
    "\n",
    "KL divergence measures **how different** a predicted probability distribution $Q$ is from a reference (true) distribution $P$:\n",
    "\n",
    "$$\n",
    "D_{\\text{KL}}(P \\parallel Q) = \\sum_i P(i) \\log \\left( \\frac{P(i)}{Q(i)} \\right)\n",
    "$$\n",
    "\n",
    "> Read as: ‚ÄúKL of $P$ relative to $Q$‚Äù\n",
    "\n",
    "- It is **not symmetric**:\n",
    "  $$\n",
    "  D_{\\text{KL}}(P \\parallel Q) \\ne D_{\\text{KL}}(Q \\parallel P)\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Log Rule Breakdown\n",
    "\n",
    "We can break the KL formula using log rules:\n",
    "\n",
    "$$\n",
    "\\log\\left(\\frac{P(i)}{Q(i)}\\right) = \\log P(i) - \\log Q(i)\n",
    "$$\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "D_{\\text{KL}}(P \\parallel Q) = \\sum_i P(i) [\\log P(i) - \\log Q(i)] = H(P, Q) - H(P)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $H(P, Q)$ is the **cross-entropy**\n",
    "- $H(P)$ is the **entropy** of the true distribution\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ When $P$ is One-Hot (Normal Classification)\n",
    "\n",
    "Let:\n",
    "$$\n",
    "P = [0, 1, 0] \\quad \\text{(true class is index 1)}\n",
    "$$\n",
    "\n",
    "Then:\n",
    "\n",
    "- $\\log P(i)$ is only defined for $i=1$ ‚Üí others contribute 0\n",
    "- So:\n",
    "  $$\n",
    "  H(P) = -\\sum_i P(i) \\log P(i) = 0\n",
    "  $$\n",
    "\n",
    "And:\n",
    "\n",
    "$$\n",
    "H(P, Q) = -\\sum_i P(i) \\log Q(i) = -\\log Q(1)\n",
    "$$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$\n",
    "D_{\\text{KL}}(P \\parallel Q) = -\\log Q(\\text{true class})\n",
    "$$\n",
    "\n",
    "‚úÖ KL = Cross-Entropy\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ When $P$ is NOT One-Hot (Soft Targets)\n",
    "\n",
    "Let:\n",
    "\n",
    "$$\n",
    "P = [0.7,\\ 0.2,\\ 0.1], \\quad Q = [0.6,\\ 0.3,\\ 0.1]\n",
    "$$\n",
    "\n",
    "Then:\n",
    "\n",
    "- $H(P, Q) = -\\sum_i P(i) \\log Q(i)$\n",
    "- $H(P) = -\\sum_i P(i) \\log P(i)$\n",
    "- $D_{\\text{KL}} = H(P, Q) - H(P)$\n",
    "\n",
    "Now KL $\\ne$ CE. The **difference reflects how much Q diverges from the full shape of P**, not just its top class.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Intuition\n",
    "\n",
    "- KL divergence measures the **inefficiency** of assuming $Q$ when the true distribution is $P$\n",
    "- It quantifies the **extra bits of information** needed to encode samples from $P$ using $Q$\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Connection to Maximum Likelihood\n",
    "\n",
    "Maximum Likelihood Estimation (MLE) seeks to:\n",
    "\n",
    "$$\n",
    "\\arg\\max_\\theta \\sum_x \\log Q_\\theta(x)\n",
    "$$\n",
    "\n",
    "This is equivalent to:\n",
    "\n",
    "$$\n",
    "\\arg\\min_\\theta D_{\\text{KL}}(P \\parallel Q_\\theta)\n",
    "$$\n",
    "\n",
    "‚úÖ Minimizing KL divergence = Maximizing log-likelihood\n",
    "\n",
    "They are mathematically tied:\n",
    "- **KL is just cross-entropy minus entropy**\n",
    "- If $P$ is known, **minimizing KL = maximizing model fit**\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ When to Use KL vs Cross-Entropy\n",
    "\n",
    "| Scenario | Is $P$ one-hot? | Use CE = KL? | Best Loss |\n",
    "|----------|------------------|---------------|------------|\n",
    "| Classification | ‚úÖ Yes | ‚úÖ Yes | CrossEntropyLoss |\n",
    "| Label smoothing | ‚ùå No | ‚ùå No | KL Divergence |\n",
    "| Knowledge distillation | ‚ùå No | ‚ùå No | KL Divergence |\n",
    "| Probabilistic models (e.g. VAEs, RL) | ‚ùå No | ‚ùå No | KL Divergence |\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Summary\n",
    "\n",
    "| Concept | Formula | Notes |\n",
    "|--------|---------|-------|\n",
    "| KL Divergence | $D_{\\text{KL}}(P \\parallel Q) = \\sum_i P(i) \\log \\frac{P(i)}{Q(i)}$ | Measures inefficiency |\n",
    "| KL = CE - Entropy | $D_{\\text{KL}} = H(P, Q) - H(P)$ | When $P$ is not one-hot |\n",
    "| KL = CE (special case) | $D_{\\text{KL}} = -\\log Q(\\text{true})$ | When $P$ is one-hot |\n",
    "| MLE ‚âà KL minimization | $\\arg\\max \\log Q_\\theta(x) = \\arg\\min D_{\\text{KL}}(P \\parallel Q_\\theta)$ | Core learning principle |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
