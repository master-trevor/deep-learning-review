{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìò Cross-Entropy: Recap & Intuition\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ What Is Cross-Entropy?\n",
    "\n",
    "Cross-entropy is a **loss function** commonly used in classification tasks. It measures the difference between two probability distributions:\n",
    "\n",
    "- The **true distribution** (from the actual labels)\n",
    "- The **predicted distribution** (from the model's outputs)\n",
    "\n",
    "It answers:  \n",
    "> *\"How well does the model's predicted distribution match the actual labels?\"*\n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ Cross-Entropy Formula\n",
    "\n",
    "### üîπ General (multi-class):\n",
    "\n",
    "$$\n",
    "\\text{Loss} = -\\sum_{i} y_i \\log(\\hat{y}_i)\n",
    "$$\n",
    "\n",
    "- \\( y_i \\): one-hot encoded true label (1 for the correct class, 0 for others)  \n",
    "- \\( \\hat{y}_i \\): predicted probability for class \\( i \\)\n",
    "\n",
    "Since only the correct class has \\( y_i = 1 \\), this simplifies to:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = -\\log(\\hat{y}_{\\text{correct class}})\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Binary Classification (two classes):\n",
    "\n",
    "$$\n",
    "\\text{Loss} = -\\left[y \\log(\\hat{y}) + (1 - y)\\log(1 - \\hat{y})\\right]\n",
    "$$\n",
    "\n",
    "- \\( y \\in \\{0, 1\\} \\) is the true label  \n",
    "- \\( \\hat{y} \\in [0, 1] \\) is the predicted probability of the positive class\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Intuition\n",
    "\n",
    "- Cross-entropy **punishes wrong predictions** more harshly as they get more confident and incorrect.\n",
    "- If the model assigns a high probability to the correct class, the loss is low.\n",
    "- If the model assigns a low probability to the correct class, the loss is high.\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Example\n",
    "\n",
    "### Multi-class example:\n",
    "- True class = A (index 0)\n",
    "- Predicted: [0.7, 0.2, 0.1]\n",
    "\n",
    "$$\n",
    "\\text{Loss} = -\\log(0.7) \\approx 0.357\n",
    "$$\n",
    "\n",
    "- If predicted [0.1, 0.2, 0.7], loss = \\( -\\log(0.1) \\approx 2.3 \\)\n",
    "\n",
    "---\n",
    "\n",
    "## üìà Why Use Log?\n",
    "\n",
    "- Log penalizes low probabilities **exponentially**.\n",
    "- Converts multiplication (likelihood) into addition (log-likelihood).\n",
    "- Helps with numerical stability.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Connection to Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "### Likelihood of correct prediction:\n",
    "\n",
    "$$\n",
    "L = \\prod_{i=1}^{n} P(y_i \\mid \\hat{y}_i)\n",
    "$$\n",
    "\n",
    "Take log to get log-likelihood:\n",
    "\n",
    "$$\n",
    "\\log L = \\sum_{i=1}^{n} \\log P(y_i \\mid \\hat{y}_i)\n",
    "$$\n",
    "\n",
    "Cross-entropy loss = **negative log-likelihood**:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = -\\sum_{i=1}^{n} \\log(\\text{model's predicted prob for true class})\n",
    "$$\n",
    "\n",
    "### MLE Goal:\n",
    "> \"Choose model parameters that make the observed labels as probable as possible.\"\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ How It Works in Gradient Descent\n",
    "\n",
    "- Loss is computed via cross-entropy\n",
    "- Gradient tells the model how to adjust weights to **increase predicted probability of the true class**\n",
    "- Weight update:\n",
    "\n",
    "$$\n",
    "W \\leftarrow W - \\eta \\cdot \\nabla_W \\text{Loss}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$\n",
    "\\nabla_W \\text{Loss} = (\\hat{y} - y) \\cdot x^T\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ Softmax Derivative Summary\n",
    "\n",
    "Given:\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = \\frac{e^{z_i}}{\\sum_j e^{z_j}}, \\quad z_i = w_i^T x\n",
    "$$\n",
    "\n",
    "Derivative of softmax:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\hat{y}_i}{\\partial z_j} =\n",
    "\\begin{cases}\n",
    "\\hat{y}_i(1 - \\hat{y}_i) & \\text{if } i = j \\\\\n",
    "-\\hat{y}_i \\hat{y}_j & \\text{if } i \\ne j\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "When using **softmax + cross-entropy**, the gradient simplifies to:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{Loss}}{\\partial z} = \\hat{y} - y\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Entropy & Information Theory\n",
    "\n",
    "- **Entropy \\( H(p) \\)**: average uncertainty of a true distribution  \n",
    "  $$\n",
    "  H(p) = -\\sum p_i \\log p_i\n",
    "  $$\n",
    "\n",
    "- **Cross-entropy \\( H(p, q) \\)**: how many bits needed to encode \\( p \\) using \\( q \\)  \n",
    "  $$\n",
    "  H(p, q) = -\\sum p_i \\log q_i\n",
    "  $$\n",
    "\n",
    "- **KL Divergence**: measures the \"distance\" between two distributions  \n",
    "  $$\n",
    "  \\text{KL}(p \\parallel q) = H(p, q) - H(p)\n",
    "  $$\n",
    "\n",
    "Minimizing cross-entropy = minimizing KL divergence = getting predicted distribution \\( q \\) close to true \\( p \\)\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Use Cases\n",
    "\n",
    "- Binary classification (with sigmoid): binary cross-entropy  \n",
    "- Multi-class classification (with softmax): categorical cross-entropy  \n",
    "- Multi-label classification: multiple binary cross-entropies (one per label)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìò Mean Squared Error (MSE) ‚Äî Study Notes\n",
    "\n",
    "## üî¢ Definition\n",
    "\n",
    "**Mean Squared Error (MSE)** is a loss function that measures the average of the squares of the errors ‚Äî that is, the average squared difference between actual and predicted values.\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $y_i$ is the **true value** (actual/ground truth)\n",
    "- $\\hat{y}_i$ is the **predicted value**\n",
    "- $n$ is the number of data points\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Intuition\n",
    "\n",
    "1. **Error**: For each prediction, calculate the error: $y_i - \\hat{y}_i$\n",
    "2. **Square it**: We square the error to make all differences positive and emphasize larger errors.\n",
    "3. **Average**: Add up all the squared errors and divide by the number of observations.\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Why use the squared error?\n",
    "\n",
    "- Squaring **penalizes larger errors** more than smaller ones.\n",
    "- It's differentiable and smooth, making it great for optimization with gradient descent.\n",
    "- It's the default loss function for regression problems in many machine learning models (like linear regression, neural nets doing regression, etc).\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Properties\n",
    "\n",
    "- MSE is always **non-negative**\n",
    "- **Lower is better**: 0 means perfect predictions\n",
    "- Units are **squared** compared to the original data (if you're predicting dollars, MSE is in dollars¬≤)\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Example\n",
    "\n",
    "Let‚Äôs say we have actual vs predicted:\n",
    "\n",
    "| Actual ($y$) | Predicted ($\\hat{y}$) |\n",
    "|--------------|------------------------|\n",
    "| 3            | 2                      |\n",
    "| -0.5         | 0                      |\n",
    "| 2            | 2                      |\n",
    "| 7            | 8                      |\n",
    "\n",
    "Now compute errors and MSE:\n",
    "\n",
    "1. Errors: $[1, -0.5, 0, -1]$\n",
    "2. Squared: $[1, 0.25, 0, 1]$\n",
    "3. Mean: $\\frac{1 + 0.25 + 0 + 1}{4} = 0.5625$\n",
    "\n",
    "**MSE = 0.5625**\n",
    "\n",
    "---\n",
    "\n",
    "## ü§ñ MSE and Gradient Descent\n",
    "\n",
    "Assume your model is:\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = w x_i + b\n",
    "$$\n",
    "\n",
    "MSE loss:\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "### Gradient w.r.t. $w$:\n",
    "\n",
    "Using the chain rule:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w} = -\\frac{2}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i) x_i\n",
    "$$\n",
    "\n",
    "### Gradient w.r.t. $b$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b} = -\\frac{2}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)\n",
    "$$\n",
    "\n",
    "So yes ‚Äî you compute the gradient at each data point, **sum them**, and **divide by $n$** to get the average gradient for updating your weights in batch gradient descent.\n",
    "\n",
    "---\n",
    "\n",
    "## ü§ù MSE vs MAE vs RMSE\n",
    "\n",
    "| Metric | Description               | Pros                          | Cons                         |\n",
    "|--------|---------------------------|-------------------------------|------------------------------|\n",
    "| MSE    | Mean of squared errors    | Smooth, convex, common        | Sensitive to outliers        |\n",
    "| MAE    | Mean of absolute errors   | Robust to outliers            | Non-smooth gradient          |\n",
    "| RMSE   | Square root of MSE        | Same units as target          | Still sensitive to outliers  |\n",
    "\n",
    "---\n",
    "\n",
    "## üö´ When not to use MSE\n",
    "\n",
    "- Classification tasks (use **cross-entropy loss** instead)\n",
    "- Heavy outliers dominate the dataset (use **MAE** or **Huber loss**)\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ Alternatives\n",
    "\n",
    "- **MAE** ‚Äî good for robustness\n",
    "- **Huber loss** ‚Äî hybrid between MSE and MAE\n",
    "- **Quantile loss** ‚Äî for percentile prediction\n",
    "- **Log-Cosh loss** ‚Äî smooth and outlier-resistant\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Key Takeaways\n",
    "\n",
    "- MSE is the **go-to loss** for regression tasks\n",
    "- Penalizes large errors more due to squaring\n",
    "- Has a smooth, differentiable gradient ‚Äî perfect for deep learning optimization\n",
    "- Can be skewed by outliers ‚Üí know when to use alternatives like **MAE** or **Huber**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßØ Dropout: Regularization in Deep Learning\n",
    "\n",
    "### üîç What is Dropout?\n",
    "\n",
    "Dropout is a regularization technique used to prevent overfitting in neural networks. During training, dropout randomly \"drops\" a subset of neurons by setting their output to **zero**. This forces the network to learn **redundant, robust representations** that do not rely on any specific neuron.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Implementation Example (Pseudocode)\n",
    "\n",
    "```python\n",
    "# Forward pass with dropout (PyTorch-style logic)\n",
    "mask = torch.bernoulli(torch.full_like(activations, p))  # p = keep probability\n",
    "output = activations * mask / p\n",
    "```\n",
    "\n",
    "- **p**: probability of keeping a neuron (e.g., 0.8 means 20% are dropped)\n",
    "- Division by **p** scales activations to preserve expected value\n",
    "\n",
    "---\n",
    "\n",
    "### üéì Why Does Dropout Work?\n",
    "\n",
    "Dropout prevents co-adaptation of neurons: they can't rely on specific other neurons being present.\n",
    "\n",
    "It trains an ensemble of subnetworks, effectively improving generalization.\n",
    "\n",
    "Dropout improves robustness by forcing the model to spread out learned representations.\n",
    "\n",
    "---\n",
    "\n",
    "### üî¨ Key Points to Remember\n",
    "\n",
    "| Feature                     | Description                                                        |\n",
    "|-----------------------------|--------------------------------------------------------------------|\n",
    "| Applied to                  | Activations (after ReLU, typically in fully connected layers)      |\n",
    "| Typical dropout rates       | 0.2 ‚Äì 0.5 (common: 0.5 in dense layers)                            |\n",
    "| During training             | Randomly masks out activations                                     |\n",
    "| During inference            | No dropout, optionally scale activations by keep_prob              |\n",
    "| Effect on gradient flow     | Dropped neurons receive no gradient                                |\n",
    "| Difference from ReLU = 0    | ReLU output = 0 is computed; Dropout = 0 is overwritten            |\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Intuition\n",
    "\n",
    "Dropout is like training a different, smaller network each time and averaging them ‚Äî but efficiently, without explicitly building all the networks.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå When Not to Use Dropout\n",
    "\n",
    "- Small models with little overfitting risk  \n",
    "- Convolutional layers (often unnecessary or harmful here)  \n",
    "- Recurrent networks (requires special care: e.g., `nn.Dropout` with consistent masks)  \n",
    "- When using other strong regularizers (e.g., heavy augmentation, weight decay)\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ When to Use Dropout\n",
    "\n",
    "- Deep fully connected networks  \n",
    "- Large datasets with risk of overfitting  \n",
    "- As a simple, powerful addition to your regularization toolkit\n",
    "\n",
    "---\n",
    "\n",
    "### üìê Math Recap\n",
    "\n",
    "If a neuron's activation is:\n",
    "\n",
    "$$\n",
    "a = \\phi(w^T x + b)\n",
    "$$\n",
    "\n",
    "Then during dropout:\n",
    "\n",
    "$$\n",
    "a_{\\text{dropped}} = a \\cdot m\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- \\( m \\sim \\text{Bernoulli}(p) \\), a binary mask\n",
    "- \\( p \\): the keep probability (e.g., 0.8)\n",
    "\n",
    "During training, scale:\n",
    "\n",
    "$$\n",
    "a_{\\text{dropped}} = \\frac{a \\cdot m}{p}\n",
    "$$\n",
    "\n",
    "During inference, use full \\( a \\) (optionally scaled by \\( p \\)).\n",
    "\n",
    "---\n",
    "\n",
    "### üîó References\n",
    "\n",
    "- Srivastava et al. (2014). [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](https://jmlr.org/papers/v15/srivastava14a.html)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Regularization & Weight Decay: Detailed Notes\n",
    "\n",
    "---\n",
    "\n",
    "### üßÆ L1 and L2 Regularization\n",
    "\n",
    "#### ‚úÖ L1 Regularization\n",
    "\n",
    "Adds a penalty proportional to the **absolute value** of weights:\n",
    "\n",
    "$$\n",
    "\\text{Loss}_{\\text{total}} = \\text{Loss}_{\\text{data}} + \\lambda \\sum_i |w_i|\n",
    "$$\n",
    "\n",
    "- Encourages **sparsity** (many weights go to exactly zero)\n",
    "- Useful for **feature selection**\n",
    "- Not smooth at 0 ‚Äî harder to optimize\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚úÖ L2 Regularization\n",
    "\n",
    "Adds a penalty proportional to the **squared value** of weights:\n",
    "\n",
    "$$\n",
    "\\text{Loss}_{\\text{total}} = \\text{Loss}_{\\text{data}} + \\lambda \\sum_i w_i^2\n",
    "$$\n",
    "\n",
    "- Encourages **small** weights (but not exactly zero)\n",
    "- Helps with **overfitting**\n",
    "- Common in deep learning\n",
    "- Equivalent to **weight decay** in SGD\n",
    "\n",
    "---\n",
    "\n",
    "### üîç Gradient of the Regularization Term\n",
    "\n",
    "#### Loss with L2 regularization:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{total}} = \\mathcal{L}_{\\text{data}} + \\lambda \\cdot \\frac{1}{2} \\sum_i w_i^2\n",
    "$$\n",
    "\n",
    "Taking the gradient w.r.t. each weight:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}_{\\text{total}}}{\\partial w_i} = \\frac{\\partial \\mathcal{L}_{\\text{data}}}{\\partial w_i} + \\lambda w_i\n",
    "$$\n",
    "\n",
    "So the **gradient of the regularization term** is:\n",
    "\n",
    "$$\n",
    "\\nabla_w (\\lambda \\cdot \\frac{1}{2} \\|w\\|_2^2) = \\lambda w\n",
    "$$\n",
    "\n",
    "> The summation \"disappears\" in the gradient because the partial derivative of the sum isolates each \\( w_i \\).\n",
    "\n",
    "---\n",
    "\n",
    "### üß≤ What Is Weight Decay?\n",
    "\n",
    "Weight decay is a technique that **shrinks weights during training** to prevent them from growing too large:\n",
    "\n",
    "- It is **mathematically equivalent** to L2 regularization in **SGD**\n",
    "- It modifies the gradient update rule:\n",
    "\n",
    "$$\n",
    "w \\leftarrow w - \\eta \\cdot \\left( \\nabla_w \\mathcal{L}_{\\text{data}} + \\lambda w \\right)\n",
    "$$\n",
    "\n",
    "> \\( \\lambda \\) controls how strongly weights are pulled toward zero.\n",
    "\n",
    "---\n",
    "\n",
    "### üß® The Problem with Adam + L2 Regularization\n",
    "\n",
    "Adam rescales gradients based on **moving averages of squared gradients**.  \n",
    "So when you inject L2 into the gradient:\n",
    "\n",
    "$$\n",
    "g_t = \\nabla_w \\mathcal{L}_{\\text{data}} + \\lambda w\n",
    "$$\n",
    "\n",
    "It gets passed into the adaptive machinery (momentum & RMSProp):\n",
    "\n",
    "$$\n",
    "m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t \\\\\n",
    "v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2\n",
    "$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "w \\leftarrow w - \\eta \\cdot \\frac{m_t}{\\sqrt{v_t} + \\epsilon}\n",
    "$$\n",
    "\n",
    "> ‚ùå The weight decay term \\( \\lambda w \\) is **scaled and distorted**, so it no longer behaves like clean L2 shrinkage.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ AdamW: The Fix\n",
    "\n",
    "AdamW **decouples** weight decay from the gradient:\n",
    "\n",
    "1. Do a regular Adam step using only the gradient of the data loss\n",
    "2. Apply weight decay **afterward**, directly to the weights:\n",
    "\n",
    "$$\n",
    "w \\leftarrow w - \\eta \\cdot \\frac{m_t}{\\sqrt{v_t} + \\epsilon} - \\eta \\cdot \\lambda w\n",
    "$$\n",
    "\n",
    "> ‚úÖ This restores proper \"pull toward zero\" behavior, consistent with L2 regularization in SGD.\n",
    "\n",
    "---\n",
    "\n",
    "### üÜö Adam vs AdamW\n",
    "\n",
    "| Feature                    | Adam + L2 Regularization               | AdamW                                |\n",
    "|----------------------------|----------------------------------------|--------------------------------------|\n",
    "| Applies decay via          | Gradient injection                    | Directly on weights post-update      |\n",
    "| Affected by gradient scaling? | ‚úÖ Yes                              | ‚ùå No                                 |\n",
    "| Consistent with SGD/L2?    | ‚ùå No                                  | ‚úÖ Yes                                |\n",
    "| Generalization performance | Often worse                           | More reliable                        |\n",
    "\n",
    "---\n",
    "\n",
    "### üîó References\n",
    "\n",
    "- Loshchilov & Hutter (2019). [Decoupled Weight Decay Regularization](https://arxiv.org/abs/1711.05101)\n",
    "- PyTorch Optimizers: [`AdamW`](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Batch Normalization (BatchNorm)\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Why BatchNorm?\n",
    "\n",
    "BatchNorm was introduced to solve a key training problem in deep networks:  \n",
    "**internal covariate shift**.\n",
    "\n",
    "> Internal covariate shift = The distribution of inputs to each layer keeps changing during training because the layers before it are also updating.\n",
    "\n",
    "This \"shifting target\" makes it harder for deeper layers to learn ‚Äî like trying to hit a moving bullseye.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ What Does BatchNorm Do?\n",
    "\n",
    "For each feature/channel in a layer, **per mini-batch**:\n",
    "\n",
    "1. Compute batch mean and variance:\n",
    "   $$\n",
    "   \\mu_B = \\frac{1}{m} \\sum_{i=1}^{m} x_i, \\quad\n",
    "   \\sigma_B^2 = \\frac{1}{m} \\sum_{i=1}^{m} (x_i - \\mu_B)^2\n",
    "   $$\n",
    "\n",
    "2. Normalize the activations:\n",
    "   $$\n",
    "   \\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}\n",
    "   $$\n",
    "\n",
    "3. Apply learned scale and shift:\n",
    "   $$\n",
    "   y_i = \\gamma \\cdot \\hat{x}_i + \\beta\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "### üß© What Are $ \\gamma $ and $ \\beta $?\n",
    "\n",
    "- $ \\gamma $: learnable **scale** parameter\n",
    "- $ \\beta $: learnable **shift** parameter\n",
    "\n",
    "Without them, the layer output would always have mean 0 and variance 1 ‚Äî limiting expressiveness.\n",
    "\n",
    "> BatchNorm normalizes for training stability,  \n",
    "> but \\( \\gamma \\) and \\( \\beta \\) allow the network to **learn any useful distribution** again if needed.\n",
    "\n",
    "---\n",
    "\n",
    "### üîÄ Behavior During Training vs Inference\n",
    "\n",
    "#### üèãÔ∏è Training Mode\n",
    "\n",
    "- Compute mean and variance **from the current mini-batch**\n",
    "- Normalization is noisy ‚Üí helps regularize and stabilize training\n",
    "\n",
    "#### üß™ Inference Mode\n",
    "\n",
    "- Use **running averages** of mean and variance collected during training:\n",
    "  $$\n",
    "  \\mu_{\\text{running}} \\leftarrow \\rho \\cdot \\mu_{\\text{running}} + (1 - \\rho) \\cdot \\mu_B\n",
    "  $$\n",
    "\n",
    "- Prevents randomness at test time\n",
    "- Ensures consistent output even when batch size is 1\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Where to Use BatchNorm\n",
    "\n",
    "Typically placed:\n",
    "\n",
    "- After linear/convolution layers\n",
    "- Before activation functions\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "nn.Sequential(\n",
    "    nn.Linear(128, 64),\n",
    "    nn.BatchNorm1d(64),\n",
    "    nn.ReLU()\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Benefits of BatchNorm\n",
    "\n",
    "- Allows **higher learning rates**\n",
    "- Reduces sensitivity to initialization\n",
    "- Mitigates vanishing/exploding gradients\n",
    "- Adds mild **regularization effect** (from batch noise)\n",
    "- Enables training of **very deep networks**\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùó Best Practices\n",
    "\n",
    "- Always call `.eval()` before inference to switch BN to test mode\n",
    "- Works best with reasonably sized mini-batches (e.g., ‚â•32)\n",
    "- Be cautious when combining with Dropout ‚Äî BatchNorm already acts as a regularizer\n",
    "\n",
    "---\n",
    "\n",
    "### üîó References\n",
    "\n",
    "- Ioffe & Szegedy (2015). [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Layer Normalization (LayerNorm)\n",
    "\n",
    "---\n",
    "\n",
    "### üìå What Is LayerNorm?\n",
    "\n",
    "LayerNorm is a normalization technique that normalizes activations **across the features** of each individual sample.\n",
    "\n",
    "Unlike BatchNorm, it does **not use batch statistics**, which makes it ideal for:\n",
    "\n",
    "- Recurrent models (RNNs, LSTMs)\n",
    "- Transformers (BERT, GPT, etc.)\n",
    "- Situations with small or variable batch sizes\n",
    "- Autoregressive generation (batch size = 1)\n",
    "\n",
    "---\n",
    "\n",
    "### üîç How Does It Work?\n",
    "\n",
    "Given a vector of activations \\( x = [x_1, x_2, \\dots, x_H] \\) for a single sample, LayerNorm computes:\n",
    "\n",
    "**Mean and variance over the features:**\n",
    "\n",
    "$$\n",
    "\\mu = \\frac{1}{H} \\sum_{i=1}^{H} x_i, \\quad\n",
    "\\sigma^2 = \\frac{1}{H} \\sum_{i=1}^{H} (x_i - \\mu)^2\n",
    "$$\n",
    "\n",
    "**Normalize:**\n",
    "\n",
    "$$\n",
    "\\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\n",
    "$$\n",
    "\n",
    "**Apply learnable scale and shift:**\n",
    "\n",
    "$$\n",
    "y_i = \\gamma \\cdot \\hat{x}_i + \\beta\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- \\( \\gamma \\): learnable scale parameter\n",
    "- \\( \\beta \\): learnable shift parameter\n",
    "- \\( \\epsilon \\): small constant to prevent division by zero\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Why Normalize Across Features?\n",
    "\n",
    "In NLP and Transformers:\n",
    "\n",
    "- Each **token** is represented by a feature vector (embedding + positional info)\n",
    "- You **don't want to normalize across tokens** ‚Äî each token should be treated independently\n",
    "- Instead, normalize **within each token**, across its features\n",
    "\n",
    "> This preserves each token‚Äôs position and semantic identity.\n",
    "\n",
    "---\n",
    "\n",
    "### üìê Input Shapes and Normalization Axes\n",
    "\n",
    "| Input Shape                          | LayerNorm Normalizes Over            |\n",
    "|--------------------------------------|--------------------------------------|\n",
    "| `(batch_size, features)`             | The `features` axis (per sample)     |\n",
    "| `(batch_size, seq_len, embed_dim)`   | The `embed_dim` axis (per token)     |\n",
    "\n",
    "Each row or token vector is normalized **independently**.\n",
    "\n",
    "---\n",
    "\n",
    "### üÜö LayerNorm vs BatchNorm\n",
    "\n",
    "| Feature                     | BatchNorm                                  | LayerNorm                                |\n",
    "|-----------------------------|---------------------------------------------|-------------------------------------------|\n",
    "| Normalizes over             | Features **across** batch                  | Features **within** each sample           |\n",
    "| Requires batch stats        | ‚úÖ Yes                                     | ‚ùå No                                      |\n",
    "| Consistent for batch size=1 | ‚ùå No                                      | ‚úÖ Yes                                     |\n",
    "| Good for CNNs               | ‚úÖ Yes                                     | üëé Not common                              |\n",
    "| Good for RNNs/Transformers  | ‚ùå No                                      | ‚úÖ Yes                                     |\n",
    "| Used in GPT/BERT/etc        | ‚ùå No                                      | ‚úÖ Yes                                     |\n",
    "| Needs `.train()` / `.eval()`| ‚úÖ Yes (depends on mode)                   | ‚ùå No (mode-agnostic)                      |\n",
    "\n",
    "---\n",
    "\n",
    "### üì¶ Where Is LayerNorm Used?\n",
    "\n",
    "- **MLPs**: `Linear ‚Üí LayerNorm ‚Üí Activation`\n",
    "- **Transformers**: applied around residual blocks\n",
    "  - **PreNorm**: `x + Sublayer(LayerNorm(x))`\n",
    "  - **PostNorm**: `LayerNorm(x + Sublayer(x))`\n",
    "- **RNNs / LSTMs**: normalize input and/or hidden state per time step\n",
    "\n",
    "---\n",
    "\n",
    "### üß† What‚Äôs a ‚ÄúToken‚Äù in NLP?\n",
    "\n",
    "- A **token** is a unit of text (word, subword, character, etc.)\n",
    "- After embedding, each token becomes a **vector** (e.g., 768-dim)\n",
    "- Positional encoding is **added** to the token vector\n",
    "- That final vector is what gets normalized by LayerNorm\n",
    "\n",
    "So yes ‚Äî in NLP, a token is **represented as a feature vector**, and LayerNorm ensures that vector has:\n",
    "\n",
    "- Mean ‚âà 0\n",
    "- Std ‚âà 1\n",
    "- ‚úÖ Without touching or depending on other tokens or the batch\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Summary\n",
    "\n",
    "- LayerNorm helps with stable, consistent training\n",
    "- Doesn't depend on batch statistics\n",
    "- Perfect for sequence models, small batches, and autoregressive decoding\n",
    "- Normalizes **per token**, **across features**\n",
    "- Keeps token-level information clean and separate\n",
    "\n",
    "---\n",
    "\n",
    "### üîó Reference\n",
    "\n",
    "- Ba et al. (2016). [Layer Normalization](https://arxiv.org/abs/1607.06450)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Core Hyperparameters for LLMs / GenAI Models\n",
    "\n",
    "These are the key hyperparameters that control training stability, convergence, and generalization in large models like Transformers and LLMs.\n",
    "\n",
    "| Hyperparameter         | Why It Matters                                                                                   |\n",
    "|------------------------|--------------------------------------------------------------------------------------------------|\n",
    "| **Learning Rate**      | Most critical hyperparameter ‚Äî controls step size of optimization. Use schedules (e.g. cosine, linear decay). |\n",
    "| **Batch Size**         | Affects gradient stability and memory use. Larger = smoother gradients. Smaller = noisier but faster updates. |\n",
    "| **Weight Decay**       | Penalizes large weights to reduce overfitting. Usually used with AdamW.                          |\n",
    "| **Dropout Rate**       | Randomly drops activations for regularization. Common values: 0.1‚Äì0.3                            |\n",
    "| **Gradient Clipping**  | Caps gradient norm to prevent instability/exploding gradients.                                   |\n",
    "| **Warmup Steps**       | Gradually increase LR early in training to avoid overshooting.                                   |\n",
    "| **LR Scheduler**       | Adjusts learning rate throughout training.                                                       |\n",
    "| **Max Sequence Length**| Controls how much context the model sees. Higher = more memory/computation.                      |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÇÔ∏è Gradient Clipping (Deep Dive)\n",
    "\n",
    "### üîç What it does:\n",
    "Clips the **L2 norm** of the total gradient **after** backpropagation and batch aggregation:\n",
    "\n",
    "$$\n",
    "g = [g_1, g_2, \\dots, g_n], \\quad \\|g\\|_2 = \\sqrt{\\sum_i g_i^2}\n",
    "$$\n",
    "\n",
    "If $ \\|g\\|_2 > \\text{max_norm} $, then:\n",
    "\n",
    "$$\n",
    "g \\leftarrow g \\cdot \\frac{\\text{max\\_norm}}{\\|g\\|_2}\n",
    "$$\n",
    "\n",
    "### ‚úÖ Intuition:\n",
    "\n",
    "> Scale the \"macro\" gradient (aggregated over the batch), not each individual datapoint‚Äôs gradient.\n",
    "\n",
    "- Preserves gradient direction\n",
    "- Controls gradient magnitude\n",
    "- Prevents large, unstable weight updates\n",
    "\n",
    "### üß™ PyTorch example:\n",
    "\n",
    "```python\n",
    "loss.backward()\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ When and Why to Use:\n",
    "\n",
    "### üîπ Weight Decay (L2 Regularization)\n",
    "\n",
    "- Shrinks weights over time: \\( \\nabla_w \\mathcal{L} + \\lambda w \\)\n",
    "- Encourages smoother, lower-capacity models\n",
    "- ‚úÖ Common with **AdamW** (decoupled implementation)\n",
    "\n",
    "### üîπ L1 Regularization\n",
    "\n",
    "- Encourages **sparse** weights (many exactly zero)\n",
    "- Not commonly used in LLMs\n",
    "- ‚ö†Ô∏è Adds optimization challenges (non-differentiable at zero)\n",
    "\n",
    "### üîπ Dropout\n",
    "\n",
    "- Randomly zeros out activations during training\n",
    "- Acts like an ensemble of subnetworks\n",
    "- ‚úÖ Used in fully connected and attention layers\n",
    "- ‚ö†Ô∏è Can conflict with BatchNorm (both inject noise)\n",
    "\n",
    "### üîπ BatchNorm\n",
    "\n",
    "- Normalizes **across batch dimension**\n",
    "- Helps CNNs and small feedforward nets train faster\n",
    "- ‚ùå Not used in Transformers/LLMs (conflicts with small/variable batch sizes)\n",
    "\n",
    "### üîπ LayerNorm\n",
    "\n",
    "- Normalizes **within each sample**, across features\n",
    "- Independent of batch size\n",
    "- ‚úÖ Default normalization for **Transformers / LLMs**\n",
    "- Used before or after residual connections\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Summary: When to Use What\n",
    "\n",
    "| Technique        | Use Case                              | When to Avoid                          |\n",
    "|------------------|----------------------------------------|----------------------------------------|\n",
    "| **Dropout**       | FC layers, attention, FFNs             | May conflict with BatchNorm            |\n",
    "| **Weight Decay**  | Almost always (use AdamW)              | Don‚Äôt use naive L2 with Adam           |\n",
    "| **L2 Regularization** | SGD or AdamW                       | Not with Adam (use AdamW instead)      |\n",
    "| **L1 Regularization** | Sparse models / feature selection  | Not typical for LLMs                   |\n",
    "| **BatchNorm**     | CNNs, ResNets                         | Not for sequences / Transformers       |\n",
    "| **LayerNorm**     | Transformers, LLMs, RNNs              | Rarely used in CNNs                    |\n",
    "| **Gradient Clipping** | Transformers, deep models          | Set to 1.0 or 0.5 (very safe default)  |\n",
    "\n",
    "---\n",
    "\n",
    "### üîó References\n",
    "\n",
    "- Loshchilov & Hutter (2019). [Decoupled Weight Decay Regularization (AdamW)](https://arxiv.org/abs/1711.05101)\n",
    "- Ba et al. (2016). [Layer Normalization](https://arxiv.org/abs/1607.06450)\n",
    "- Ioffe & Szegedy (2015). [BatchNorm](https://arxiv.org/abs/1502.03167)\n",
    "- Srivastava et al. (2014). [Dropout](https://jmlr.org/papers/v15/srivastava14a.html)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîó Regularization & Normalization Compatibility\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Good Combinations\n",
    "\n",
    "| Combo                         | Works Well? | Why                                                             |\n",
    "|-------------------------------|-------------|------------------------------------------------------------------|\n",
    "| **Dropout + Weight Decay**    | ‚úÖ Yes      | Dropout adds noise during training, weight decay shrinks unused weights |\n",
    "| **BatchNorm + Weight Decay**  | ‚úÖ Yes      | BatchNorm normalizes activations, weight decay still keeps weights small |\n",
    "| **LayerNorm + Dropout**       | ‚úÖ Yes      | Common in Transformers ‚Äî stable + regularized training           |\n",
    "| **LayerNorm + Weight Decay**  | ‚úÖ Yes      | Works perfectly, especially with AdamW                           |\n",
    "| **AdamW + Weight Decay**      | ‚úÖ Yes      | Decoupled weight decay avoids distortion of gradient scaling     |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è Combos to Use with Caution\n",
    "\n",
    "| Combo                                    | Warning              | Why                                                           |\n",
    "|------------------------------------------|-----------------------|----------------------------------------------------------------|\n",
    "| **Dropout + BatchNorm**                  | ‚ö†Ô∏è Be careful         | Both add stochasticity ‚Üí can conflict or cause instability     |\n",
    "| **Dropout + Heavy Augmentation + Label Smoothing** | ‚ö†Ô∏è Can over-regularize | Too much noise may hurt convergence or underfit                |\n",
    "| **L2 Regularization + Adam (not AdamW)** | ‚ùå Bad idea           | L2 gets distorted by adaptive gradient scaling ‚Äî use AdamW instead |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Combos to Avoid\n",
    "\n",
    "| Combo                          | Why Not?                                                            |\n",
    "|--------------------------------|----------------------------------------------------------------------|\n",
    "| **BatchNorm + LayerNorm**      | Redundant ‚Äî both normalize but along different axes, confusing signal |\n",
    "| **BatchNorm in Transformers**  | Needs batch stats ‚Äî breaks in variable-length or autoregressive settings |\n",
    "| **L1 Regularization in LLMs**  | Too harsh for dense models, rarely needed outside sparse setups       |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Recommended \"Starter Stack\" for LLMs / Transformers\n",
    "\n",
    "Use this setup in modern LLM/Transformer training:\n",
    "\n",
    "- **LayerNorm** for normalization (pre/post residual)\n",
    "- **Dropout** after attention and feedforward blocks (e.g., 0.1‚Äì0.3)\n",
    "- **Weight Decay** via **AdamW optimizer**\n",
    "- **Gradient Clipping** to stabilize large updates (e.g., max norm = 1.0)\n",
    "- **No BatchNorm**\n",
    "- **No direct L2 regularization** (already handled via weight decay)\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Weight Decay Reminder\n",
    "\n",
    "With AdamW, weight decay is applied separately:\n",
    "\n",
    "$$\n",
    "w \\leftarrow w - \\eta \\cdot \\frac{m_t}{\\sqrt{v_t} + \\epsilon} - \\eta \\cdot \\lambda w\n",
    "$$\n",
    "\n",
    "This avoids distorting the adaptive gradient and ensures clean decay.\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Gradient Clipping Reminder\n",
    "\n",
    "Clip the **L2 norm** of the full gradient vector across all parameters:\n",
    "\n",
    "If:\n",
    "\n",
    "$$\n",
    "\\|g\\|_2 > \\text{max\\_norm}\n",
    "$$\n",
    "\n",
    "Then rescale:\n",
    "\n",
    "$$\n",
    "g \\leftarrow g \\cdot \\frac{\\text{max\\_norm}}{\\|g\\|_2}\n",
    "$$\n",
    "\n",
    "This stabilizes training, especially in deep networks and Transformers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## üìä Monitoring & Debugging Training with Curves\n",
    "\n",
    "Understanding and monitoring **learning curves** is essential for training deep learning models effectively. These curves help identify issues like underfitting, overfitting, unstable training, or misconfigured hyperparameters.\n",
    "\n",
    "---\n",
    "\n",
    "### üìâ What to Track During Training\n",
    "\n",
    "| Curve Type           | Description                                      | Purpose                                 |\n",
    "|----------------------|--------------------------------------------------|------------------------------------------|\n",
    "| **Training Loss**     | Error on the training data                       | Measures how well the model is fitting  |\n",
    "| **Validation Loss**   | Error on unseen data (validation set)           | Measures generalization                 |\n",
    "| **Training Accuracy** | Classification accuracy on training set         | Useful for classification tasks         |\n",
    "| **Validation Accuracy** | Accuracy on validation set                   | Monitors real performance               |\n",
    "\n",
    "---\n",
    "\n",
    "### üß† What to Look For\n",
    "\n",
    "#### ‚úÖ Healthy Training\n",
    "- **Train and Val loss both decrease** and plateau gradually\n",
    "- **Validation accuracy increases**\n",
    "- Generalization gap is **small or steady**\n",
    "\n",
    "#### üö® Overfitting\n",
    "- **Training loss keeps going down**\n",
    "- **Validation loss goes up**\n",
    "- **Training accuracy >> Validation accuracy**\n",
    "\n",
    "‚úÖ Fix:\n",
    "- Add **Dropout**\n",
    "- Add **Weight Decay**\n",
    "- Use **Data Augmentation**\n",
    "- Apply **Early Stopping**\n",
    "\n",
    "#### ‚ö†Ô∏è Underfitting\n",
    "- **Both losses remain high**\n",
    "- Model struggles to fit training data\n",
    "\n",
    "‚úÖ Fix:\n",
    "- Increase model capacity (depth, width)\n",
    "- Increase learning rate\n",
    "- Improve data preprocessing\n",
    "\n",
    "---\n",
    "\n",
    "### üìê Generalization Gap\n",
    "\n",
    "$$\n",
    "\\text{Generalization Gap} = \\text{Validation Loss} - \\text{Training Loss}\n",
    "$$\n",
    "\n",
    "- Small gap ‚Üí model generalizes well ‚úÖ\n",
    "- Large gap ‚Üí overfitting üö®\n",
    "- Shrinking gap ‚Üí improvements in generalization üß†\n",
    "\n",
    "---\n",
    "\n",
    "### üîÅ Curve Usage by Task\n",
    "\n",
    "| Task Type         | Curve to Prioritize       | Notes                                |\n",
    "|-------------------|---------------------------|--------------------------------------|\n",
    "| Classification    | Accuracy + Loss curves    | Accuracy is easy to interpret        |\n",
    "| Regression        | Loss curves (MSE, MAE)    | Accuracy is not meaningful           |\n",
    "| Sequence models   | Loss curves + BLEU/ROUGE  | Use task-specific metrics as needed  |\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ Tools for Tracking\n",
    "\n",
    "- **TensorBoard**: Visualize loss/accuracy in real time\n",
    "- **Weights & Biases (wandb)**: Powerful experiment tracking\n",
    "- **Matplotlib / Seaborn**: Local plotting for custom training scripts\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Final Reminders\n",
    "\n",
    "- Always compare **Training vs Validation**\n",
    "- Look at **curves over time**, not just final metrics\n",
    "- Use them to guide **early stopping**, **scheduler triggers**, and **regularization tuning**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìâ Deep Learning Curve Interpretation ‚Äî Advanced Tips\n",
    "\n",
    "---\n",
    "\n",
    "### 1Ô∏è‚É£ Noisy Loss Curves? ‚Üí Check Learning Rate or Batch Size\n",
    "\n",
    "If your loss curve is **bouncy or erratic**:\n",
    "\n",
    "- Your learning rate may be too high\n",
    "- Your batch size may be too small\n",
    "- Data pipeline may not be shuffling properly\n",
    "\n",
    "‚úÖ Fix:\n",
    "- Smooth with moving average\n",
    "- Lower learning rate\n",
    "- Increase batch size\n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ Plateaued Training Loss Early? ‚Üí LR May Be Too Low or Layers Still Frozen\n",
    "\n",
    "If training loss **flattens too early**, but you suspect underfitting:\n",
    "\n",
    "- Learning rate might be too low\n",
    "- In transfer learning, some layers might still be frozen\n",
    "\n",
    "‚úÖ Fix:\n",
    "- Use a **learning rate finder**\n",
    "- Gradually unfreeze layers during training\n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è‚É£ Accuracy vs. Loss (Especially for Classification)\n",
    "\n",
    "Sometimes:\n",
    "- **Validation loss increases** while\n",
    "- **Validation accuracy still improves**\n",
    "\n",
    "This can happen when:\n",
    "- The model gets more confident (sharper softmax), which hurts loss\n",
    "- But predictions are more often correct\n",
    "\n",
    "‚úÖ Plot both **accuracy** and **loss** to get the full picture\n",
    "\n",
    "---\n",
    "\n",
    "### 4Ô∏è‚É£ Generalization Gap\n",
    "\n",
    "Define:\n",
    "\n",
    "$$\n",
    "\\text{Gap} = \\text{Validation Loss} - \\text{Training Loss}\n",
    "$$\n",
    "\n",
    "- Small gap ‚Üí model generalizes well ‚úÖ\n",
    "- Large gap ‚Üí model is overfitting üö®\n",
    "- Gap growing over time ‚Üí model needs more regularization\n",
    "\n",
    "‚úÖ Use:\n",
    "- Dropout\n",
    "- Weight decay\n",
    "- Data augmentation\n",
    "- Early stopping\n",
    "\n",
    "---\n",
    "\n",
    "### 5Ô∏è‚É£ Watch Out for Divergence or NaNs\n",
    "\n",
    "If you suddenly see:\n",
    "- Loss = NaN\n",
    "- Loss spikes massively\n",
    "- Accuracy goes to 0 or 100%\n",
    "\n",
    "It could mean:\n",
    "- Exploding gradients\n",
    "- Numerical instability (division by zero, log(0), etc.)\n",
    "\n",
    "‚úÖ Fix:\n",
    "- Use **gradient clipping**\n",
    "- Reduce learning rate\n",
    "- Check for numerical bugs (especially with custom layers or mixed precision)\n",
    "\n",
    "---\n",
    "\n",
    "### 6Ô∏è‚É£ Compare Across Experiments\n",
    "\n",
    "Use experiment tracking tools like:\n",
    "\n",
    "- **TensorBoard**\n",
    "- **Weights & Biases (wandb)**\n",
    "- Save logs + versioned runs\n",
    "\n",
    "‚úÖ Look at curves **side-by-side** to validate improvements\n",
    "\n",
    "---\n",
    "\n",
    "### 7Ô∏è‚É£ Don't Stop Too Early ‚Äî Deep Models Take Time\n",
    "\n",
    "- Just because training loss plateaus doesn‚Äôt mean the model is done learning\n",
    "- LLMs and deep nets often need:\n",
    "  - LR decay (cosine, linear, step)\n",
    "  - Patience\n",
    "  - 10s or 100s of epochs to converge\n",
    "\n",
    "‚úÖ Combine **early stopping** with **LR scheduling** for best results\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Final Tip: Don't Chase Zero Training Loss\n",
    "\n",
    "- It‚Äôs not the goal\n",
    "- A model that fits the training data **perfectly** might generalize **poorly**\n",
    "- Instead:\n",
    "  - Optimize for **low validation loss**\n",
    "  - Minimize generalization gap\n",
    "  - Monitor **both accuracy and loss curves**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
