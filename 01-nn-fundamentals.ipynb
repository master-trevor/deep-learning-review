{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📐 Matrix Multiplication & Vectorization\n",
    "\n",
    "**Matrix Multiplication Formula:**\n",
    "\n",
    "$$\n",
    "C_{ij} = \\sum_{k=1}^{n} A_{ik} B_{kj}\n",
    "$$\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Given matrices:\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}1 & 2 \\\\ 3 & 4\\end{bmatrix},\\quad\n",
    "B = \\begin{bmatrix}5 & 6 \\\\ 7 & 8\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Their product $C = AB$:\n",
    "\n",
    "$$\n",
    "C = \\begin{bmatrix}\n",
    "(1)(5)+(2)(7) & (1)(6)+(2)(8) \\\\\n",
    "(3)(5)+(4)(7) & (3)(6)+(4)(8)\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}19 & 22 \\\\ 43 & 50\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Vectorization:**\n",
    "Replace loops with efficient operations (e.g., NumPy's `np.dot()`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🧠 Broadcasting in NumPy & PyTorch: Deep Learning Notes\n",
    "\n",
    "## ✅ What is Broadcasting?\n",
    "\n",
    "Broadcasting allows element-wise operations between tensors of **different shapes** by **automatically expanding** their shapes *without copying data* .\n",
    "\n",
    "It makes code **concise**, **fast**, and **vectorized**.\n",
    "\n",
    "---\n",
    "\n",
    "## 📏 Broadcasting Rule\n",
    "\n",
    "To check if two shapes are broadcast-compatible:\n",
    "\n",
    "1. Align shapes **from right to left**\n",
    "2. For each dimension:\n",
    "   - ✅ They must be equal, **or**\n",
    "   - ✅ One of them must be `1`\n",
    "   - ❌ Otherwise: broadcasting fails\n",
    "3. If ranks differ, **left-pad the shorter shape with `1`s**\n",
    "\n",
    "---\n",
    "\n",
    "## 📐 Examples\n",
    "\n",
    "### ✅ Valid Broadcasting\n",
    "\n",
    "| Tensor A Shape | Tensor B Shape | Explanation                     |\n",
    "|----------------|----------------|---------------------------------|\n",
    "| `(3,)`         | `(2, 3)`       | `(3,)` becomes `(1, 3)`         |\n",
    "| `(2, 3)`       | `(2, 1)`       | `1` broadcasts to `3` (columns) |\n",
    "| `(2, 3)`       | `(1, 3)`       | `1` broadcasts to `2` (rows)    |\n",
    "| `(4, 3, 2)`    | `(1, 3, 1)`     | Broadcasts across batch & last  |\n",
    "\n",
    "### ❌ Invalid Broadcasting\n",
    "\n",
    "| Tensor A Shape | Tensor B Shape | Why It Fails                     |\n",
    "|----------------|----------------|----------------------------------|\n",
    "| `(2, 3)`       | `(4, 1)`       | `2 ≠ 4`, neither is `1`          |\n",
    "| `(4, 3, 2)`    | `(2, 1)`       | `(2, 1)` becomes `(1, 2, 1)`, but `3 ≠ 2` |\n",
    "\n",
    "---\n",
    "\n",
    "## 📤 Left Padding\n",
    "\n",
    "If shapes differ in length, pad the **left side** of the smaller shape with `1`s.\n",
    "\n",
    "\\[\n",
    "\\text{Example: } (4, 3, 2) \\text{ and } (2, 1) \\rightarrow (4, 3, 2) \\text{ and } (1, 2, 1)\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 Bias Addition in Deep Learning\n",
    "\n",
    "Broadcasting enables efficient bias addition in layers like:\n",
    "\n",
    "\\[\n",
    "y = x W^T + b\n",
    "\\]\n",
    "\n",
    "- \\( x \\): shape \\((\\text{batch\\_size}, \\text{in\\_features}) \\)\n",
    "- \\( W \\): shape \\((\\text{out\\_features}, \\text{in\\_features}) \\)\n",
    "- \\( b \\): shape \\((\\text{out\\_features},)\\) → broadcast across batch\n",
    "\n",
    "Result:\n",
    "\\[\n",
    "y \\in \\mathbb{R}^{\\text{batch\\_size} \\times \\text{out\\_features}}\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## 🛠️ Useful PyTorch Tools\n",
    "\n",
    "| Function           | Use Case                               |\n",
    "|--------------------|-----------------------------------------|\n",
    "| `.unsqueeze(dim)`  | Add a singleton dimension               |\n",
    "| `.view(...)`       | Reshape tensor (flexible)               |\n",
    "| `.expand(...)`     | Broadcast without copying memory        |\n",
    "| `.repeat(...)`     | Duplicate data (makes a copy)           |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔥 Tips\n",
    "\n",
    "- Always align shapes **right to left**\n",
    "- Broadcasting is used in:\n",
    "  - Bias addition\n",
    "  - Normalization\n",
    "  - Attention masking\n",
    "  - Feature-wise operations across batches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔥 Activation Functions & Logits\n",
    "\n",
    "**Logits:** Raw, unnormalized outputs from neural networks.\n",
    "\n",
    "**Euler’s number $e$:**\n",
    "- Approximately $e \\approx 2.71828$\n",
    "- Smooth differentiability: $\\frac{d}{dx}(e^x)=e^x$\n",
    "\n",
    "### Detailed Activation Functions:\n",
    "\n",
    "| Activation | Formula | Output Range | Use |\n",
    "| --- | --- | --- | --- |\n",
    "| **ReLU** | $\\max(0,x)$ | $[0,\\infty)$ | Hidden layers |\n",
    "| **Sigmoid** | $\\frac{1}{1+e^{-x}}$ | $(0,1)$ | Binary classification |\n",
    "| **Tanh** | $\\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}$ | $(-1,1)$ | Hidden layers |\n",
    "| **Softmax** | $\\frac{e^{x_i}}{\\sum_j e^{x_j}}$ | $[0,1]$, sum=1 | Multi-class output |\n",
    "| **GELU** | See below | Smooth ReLU | Transformers |\n",
    "\n",
    "### Detailed GELU (Gaussian Error Linear Unit):\n",
    "\n",
    "**Intuition:**\n",
    "GELU smoothly activates neurons using Gaussian probability.\n",
    "\n",
    "**Exact Formula:**\n",
    "\n",
    "$$\n",
    "\\text{GELU}(x)=x\\cdot\\frac{1}{2}\\left[1+\\text{erf}\\left(\\frac{x}{\\sqrt{2}}\\right)\\right]\n",
    "$$\n",
    "\n",
    "**Approximation (common practice):**\n",
    "\n",
    "$$\n",
    "\\text{GELU}(x) \\approx 0.5x\\left[1 + \\tanh\\left(\\sqrt{\\frac{2}{\\pi}}\\left(x + 0.044715x^3\\right)\\right)\\right]\n",
    "$$\n",
    "\n",
    "**Why GELU?**\n",
    "- Smooth gradients\n",
    "- Improved performance in modern NLP (Transformers, GPT models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔧 Activation Functions & Gradient Problems\n",
    "\n",
    "## ✅ Why Use Activation Functions?\n",
    "\n",
    "- Add **non-linearity** to the network.\n",
    "- Allow stacking layers to model complex, non-linear patterns.\n",
    "- Without activation functions, the network is just a linear mapping:  \n",
    "  \\[\n",
    "  f(W_2 (W_1 x)) = (W_2 W_1) x\n",
    "  \\]\n",
    "\n",
    "---\n",
    "\n",
    "## ⚡ Common Activation Functions\n",
    "\n",
    "| Function | Formula | Gradient Behavior | Notes |\n",
    "|---------|---------|------------------|-------|\n",
    "| **ReLU** | \\( \\max(0, x) \\) | 1 for \\(x > 0\\), 0 for \\(x < 0\\) | Fast, simple, but can \"die\" |\n",
    "| **GELU** | \\( x \\cdot \\Phi(x) \\approx 0.5x[1 + \\tanh(\\sqrt{2/\\pi}(x + 0.044715x^3))] \\) | Smooth, non-zero for all \\(x\\) | Used in Transformers |\n",
    "| **Sigmoid** | \\( \\frac{1}{1 + e^{-x}} \\) | Vanishes for large \\(|x|\\) | Not zero-centered |\n",
    "| **Tanh** | \\( \\tanh(x) \\) | Vanishes for large \\(|x|\\) | Zero-centered |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Vanishing Gradient Problem\n",
    "\n",
    "### What It Is:\n",
    "- Gradients shrink **exponentially** during backpropagation.\n",
    "- Eventually, they become so small that **weights stop updating** → network stops learning.\n",
    "\n",
    "### When It Happens:\n",
    "- Deep networks\n",
    "- Using **sigmoid** or **tanh**\n",
    "- Poor weight initialization\n",
    "\n",
    "### Visual Clue:\n",
    "- Gradient of sigmoid/tanh flattens out for large \\(|x|\\)\n",
    "\n",
    "---\n",
    "\n",
    "## 💥 Exploding Gradient Problem\n",
    "\n",
    "### What It Is:\n",
    "- Gradients grow **exponentially** during backpropagation.\n",
    "- Leads to **instability**, large weights, or NaNs.\n",
    "\n",
    "### When It Happens:\n",
    "- Deep or recurrent networks\n",
    "- Large initial weights\n",
    "- ReLU without proper controls\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 Why ReLU & GELU Are Popular\n",
    "\n",
    "### **ReLU**:\n",
    "- No vanishing gradient for \\(x > 0\\)\n",
    "- Sparse activation helps generalization\n",
    "- Risk of \"dying neurons\" (stuck at 0)\n",
    "\n",
    "### **GELU**:\n",
    "- Smooth, differentiable everywhere\n",
    "- Allows small negative values\n",
    "- Excellent gradient flow\n",
    "- Standard in **Transformer** architectures\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Gradient Behavior Summary (Visual Insight)\n",
    "\n",
    "- **ReLU**: 0 for \\(x < 0\\), 1 for \\(x > 0\\)\n",
    "- **GELU**: Smooth curve; never flat like sigmoid\n",
    "- **Sigmoid/Tanh**: Gradient vanishes as \\(|x|\\) increases → problem in deep nets\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "# 🧮 Summary Table of Activation Functions\n",
    "\n",
    "| Activation | Output Range | Advantages | Issues |\n",
    "| --- | --- | --- | --- |\n",
    "| **ReLU** | $[0,\\infty)$ | Fast, reduces vanishing gradients | Can \"die\" |\n",
    "| **Sigmoid** | $(0,1)$ | Probabilities | Vanishing gradients |\n",
    "| **Tanh** | $(-1,1)$ | Zero-centered | Vanishing gradients |\n",
    "| **Softmax** | $[0,1]$, sums=1 | Multi-class probability | Computation cost |\n",
    "| **GELU** | Smooth ReLU | Stable gradients, modern NLP | Slightly complex |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔁 Neural Network Layer Stacking (Shape Flow)\n",
    "\n",
    "### 🧮 Tensor Dimensions Across Layers (Batch Size = 16)\n",
    "\n",
    "```\n",
    "Input: x → (16, 100)\n",
    "\n",
    "Layer 1: Linear(100 → 64)\n",
    "Weights: W1 → (100, 64)\n",
    "Bias:    b1 → (64,)\n",
    "Output:  (16, 64)\n",
    "\n",
    "Activation: ReLU\n",
    "Output: (16, 64)\n",
    "\n",
    "Layer 2: Linear(64 → 32)\n",
    "Weights: W2 → (64, 32)\n",
    "Bias:    b2 → (32,)\n",
    "Output:  (16, 32)\n",
    "\n",
    "Activation: ReLU\n",
    "Output: (16, 32)\n",
    "\n",
    "Layer 3: Linear(32 → 1)\n",
    "Weights: W3 → (32, 1)\n",
    "Bias:    b3 → (1,)\n",
    "Output:  (16, 1)\n",
    "\n",
    "Activation: Sigmoid\n",
    "Final Output: (16, 1)\n",
    "```\n",
    "\n",
    "📌 Each layer performs `output = input @ weights + bias`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 🎯 3. Gradient Descent & Optimization\n",
    "\n",
    "### Gradient Descent (Basics)\n",
    "\n",
    "Parameter update to minimize loss $L(\\theta)$:\n",
    "\n",
    "$$\n",
    "\\theta = \\theta - \\eta\\nabla_\\theta L(\\theta)\n",
    "$$\n",
    "\n",
    "### 🏠 House Price Gradient Descent Example (Intuitive):\n",
    "\n",
    "Predict house price from square footage:\n",
    "\n",
    "$$\n",
    "\\text{Price} = w \\times (\\text{SqFt}) + b\n",
    "$$\n",
    "\n",
    "Given data:\n",
    "- $x = 1000$, actual price $y = 200000$.\n",
    "- Initial guess: $w=100$, $b=0$, prediction = \\$100,000 (error = \\$100,000)\n",
    "\n",
    "**Loss (MSE):**\n",
    "$$\n",
    "L=(y-(wx+b))^2\n",
    "$$\n",
    "\n",
    "**Gradients:**\n",
    "- Gradient w.r.t. weight $w$:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w}=-2x(y-(wx+b))\n",
    "$$\n",
    "- Gradient w.r.t. bias $b$:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b}=-2(y-(wx+b))\n",
    "$$\n",
    "\n",
    "**Numerical Gradient Calculation:**\n",
    "- w.r.t. $w$: $-2\\times1000\\times(200000-100000)=-200,000,000$\n",
    "- w.r.t. $b$: $-2\\times(200000-100000)=-200,000$\n",
    "\n",
    "Update (learning rate $\\eta=0.00000001$):\n",
    "- New $w=102$, New $b=0.002$ (Loss decreases iteratively)\n",
    "\n",
    "---\n",
    "\n",
    "# 🚀 Adam Optimizer\n",
    "\n",
    "Combines momentum and adaptive scaling:\n",
    "\n",
    "**Formulas:**\n",
    "- First moment (momentum):\n",
    "$$\n",
    "m_t=\\beta_1 m_{t-1}+(1-\\beta_1)g_t\n",
    "$$\n",
    "- Second moment (adaptive scaling):\n",
    "$$\n",
    "v_t=\\beta_2 v_{t-1}+(1-\\beta_2)g_t^2\n",
    "$$\n",
    "- Bias correction:\n",
    "$$\n",
    "\\hat{m}_t=\\frac{m_t}{1-\\beta_1^t},\\quad\n",
    "\\hat{v}_t=\\frac{v_t}{1-\\beta_2^t}\n",
    "$$\n",
    "- Parameter update:\n",
    "$$\n",
    "\\theta_t=\\theta_{t-1}-\\frac{\\eta}{\\sqrt{\\hat{v}_t}+\\epsilon}\\hat{m}_t\n",
    "$$\n",
    "\n",
    "Typical hyperparameters: $\\beta_1=0.9,\\ \\beta_2=0.999,\\ \\eta=0.001,\\ \\epsilon=10^{-8}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📘 Adam Optimizer — Summary Notes\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 What is Adam?\n",
    "\n",
    "Adam (Adaptive Moment Estimation) is an optimizer that combines the benefits of:\n",
    "\n",
    "- **Momentum**: Smooths the gradient using an exponentially weighted moving average  \n",
    "- **RMSProp**: Adapts the learning rate for each parameter based on gradient magnitudes\n",
    "\n",
    "It’s fast, robust to noise, and a strong default for deep learning tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## 📐 Adam Update Rules\n",
    "\n",
    "Let $g_t$ be the gradient at time step $t$. Adam keeps two moving averages:\n",
    "\n",
    "1. **First moment (mean of gradients)**  \n",
    "   $$ m_t = \\beta_1 \\cdot m_{t-1} + (1 - \\beta_1) \\cdot g_t $$\n",
    "\n",
    "2. **Second moment (uncentered variance of gradients)**  \n",
    "   $$ v_t = \\beta_2 \\cdot v_{t-1} + (1 - \\beta_2) \\cdot g_t^2 $$\n",
    "\n",
    "### 🧮 Bias-Corrected Estimates:\n",
    "   $$ \\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t} $$\n",
    "\n",
    "### 🔁 Parameter Update:\n",
    "   $$ \\theta_t = \\theta_{t-1} - \\alpha \\cdot \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon} $$\n",
    "\n",
    "---\n",
    "\n",
    "## 🔧 Default Hyperparameters\n",
    "\n",
    "- Learning rate: $\\alpha = 0.001$\n",
    "- $\\beta_1 = 0.9$ (momentum term)\n",
    "- $\\beta_2 = 0.999$ (RMSProp term)\n",
    "- $\\epsilon = 10^{-8}$ (prevents division by zero)\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Intuition\n",
    "\n",
    "- $m_t$ tracks **direction** (momentum)\n",
    "- $v_t$ tracks **magnitude** (adaptive step size)\n",
    "- Bias correction ensures values are accurate early on\n",
    "- Works well with **noisy** or **sparse** gradients\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 Example Calculation\n",
    "\n",
    "Given:  \n",
    "- $g_1 = 0.4$, $m_0 = 0$, $v_0 = 0$  \n",
    "- $\\beta_1 = 0.9$, $\\beta_2 = 0.999$, $\\alpha = 0.001$\n",
    "\n",
    "Then:  \n",
    "- $m_1 = 0.1 \\cdot 0.4 = 0.04$  \n",
    "- $v_1 = 0.001 \\cdot (0.4)^2 = 0.00016$  \n",
    "- $\\hat{m}_1 = \\frac{0.04}{1 - 0.9} = 0.4$  \n",
    "- $\\hat{v}_1 = \\frac{0.00016}{1 - 0.999} = 0.16$  \n",
    "- Step size:  \n",
    "  $$ \\alpha \\cdot \\frac{\\hat{m}_1}{\\sqrt{\\hat{v}_1} + \\epsilon} \\approx 0.001 \\cdot \\frac{0.4}{0.4} = 0.001 $$\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Pros\n",
    "\n",
    "- Fast convergence  \n",
    "- Good for noisy or sparse data  \n",
    "- Adaptive step sizes per parameter  \n",
    "- Little tuning required  \n",
    "\n",
    "## ❌ Cons\n",
    "\n",
    "- Sometimes worse generalization than SGD  \n",
    "- Can converge to sharp minima  \n",
    "- Slightly higher memory usage  \n",
    "\n",
    "---\n",
    "\n",
    "## 💡 Tips\n",
    "\n",
    "- Use **Adam** as your starting optimizer  \n",
    "- Try **SGD with momentum** for better generalization  \n",
    "- Use **AdamW** instead of L2 weight decay with Adam (better regularization)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📘 Gradient Descent, SGD, and Adam — Notes\n",
    "\n",
    "## 🔁 Gradient Descent (GD)\n",
    "\n",
    "**Goal**: Minimize a loss function $J(\\theta)$ with respect to parameters $\\theta$.\n",
    "\n",
    "**Update rule**:\n",
    "$$\n",
    "\\theta = \\theta - \\eta \\cdot \\nabla_\\theta J(\\theta)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\eta$: Learning rate  \n",
    "- $\\nabla_\\theta J(\\theta)$: Gradient of the loss function\n",
    "\n",
    "In **batch GD**, the gradient is computed over the entire dataset:\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\frac{1}{n} \\sum_{i=1}^n \\nabla_\\theta J(\\theta; x_i)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 🎲 Stochastic Gradient Descent (SGD)\n",
    "\n",
    "**Key idea**: Use a single (or a few) randomly selected data point(s) to approximate the gradient.\n",
    "\n",
    "**SGD update** (single data point):\n",
    "$$\n",
    "\\theta = \\theta - \\eta \\cdot \\nabla_\\theta J(\\theta; x_i)\n",
    "$$\n",
    "\n",
    "**Mini-batch SGD update** (batch $B$):\n",
    "$$\n",
    "\\theta = \\theta - \\eta \\cdot \\frac{1}{|B|} \\sum_{x_i \\in B} \\nabla_\\theta J(\\theta; x_i)\n",
    "$$\n",
    "\n",
    "**Why use SGD?**\n",
    "- Faster updates  \n",
    "- Can handle large datasets  \n",
    "- Noisy updates help escape local minima  \n",
    "\n",
    "---\n",
    "\n",
    "## ⚡ Adam Optimizer (Adaptive Moment Estimation)\n",
    "\n",
    "Adam improves SGD by adapting the learning rate using running averages of gradient moments.\n",
    "\n",
    "### Step-by-step:\n",
    "\n",
    "1. Compute gradient:\n",
    "$$\n",
    "g_t = \\nabla_\\theta J(\\theta_t)\n",
    "$$\n",
    "\n",
    "2. Update biased first moment estimate (mean):\n",
    "$$\n",
    "m_t = \\beta_1 \\cdot m_{t-1} + (1 - \\beta_1) \\cdot g_t\n",
    "$$\n",
    "\n",
    "3. Update biased second moment estimate (uncentered variance):\n",
    "$$\n",
    "v_t = \\beta_2 \\cdot v_{t-1} + (1 - \\beta_2) \\cdot g_t^2\n",
    "$$\n",
    "\n",
    "4. Bias-correct the estimates:\n",
    "$$\n",
    "\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n",
    "$$\n",
    "\n",
    "5. Update parameters:\n",
    "$$\n",
    "\\theta_t = \\theta_{t-1} - \\eta \\cdot \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n",
    "$$\n",
    "\n",
    "### Typical hyperparameters:\n",
    "- $\\eta = 0.001$  \n",
    "- $\\beta_1 = 0.9$  \n",
    "- $\\beta_2 = 0.999$  \n",
    "- $\\epsilon = 10^{-8}$  \n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Practical Notes\n",
    "\n",
    "- **Adam is usually used with mini-batches**, not full-batch.\n",
    "- SGD and its variants (like Adam) benefit from shuffling and batching data.\n",
    "- Adam often converges faster and works well out-of-the-box.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
