{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“˜ Transfer Learning & Word Embedding Practice Notebook\n",
    "# Works 100% on CPU with Anaconda + Jupyter + minimal setup\n",
    "\n",
    "# ======================\n",
    "# ðŸ“¦ 0. Setup + Imports\n",
    "# ======================\n",
    "#!pip install torch torchvision torchaudio --quiet\n",
    "#!pip install transformers gensim --quiet\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Section 1: Embedding Layer Basics\n",
      "\n",
      "Token IDs: [1, 2, 3]\n",
      "Embeddings: tensor([[[-1.6880,  1.3319, -1.2108, -0.9847, -1.5990],\n",
      "         [ 0.0612,  2.6173, -3.0739,  0.7096,  0.1382],\n",
      "         [ 1.2916, -0.2856,  1.2343, -0.4007,  2.3343]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# ðŸ“˜ 1. Learnable Embeddings\n",
    "# ==========================\n",
    "print(\"\\nðŸ”¹ Section 1: Embedding Layer Basics\\n\")\n",
    "\n",
    "# Define vocab and toy tokenizer\n",
    "vocab = {\"[PAD]\": 0, \"hello\": 1, \"world\": 2, \"bert\": 3, \"rocks\": 4}\n",
    "inv_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "def tokenize(text):\n",
    "    return [vocab.get(token, 0) for token in text.lower().split()]\n",
    "\n",
    "# TODO: Change this sentence to test different token combos\n",
    "sentence = \"hello world bert\"\n",
    "token_ids = tokenize(sentence)\n",
    "tokens_tensor = torch.tensor(token_ids).unsqueeze(0)  # batch size 1\n",
    "\n",
    "# Embedding layer\n",
    "embedding = nn.Embedding(num_embeddings=len(vocab), embedding_dim=5)\n",
    "embedded_output = embedding(tokens_tensor)\n",
    "\n",
    "print(\"Token IDs:\", token_ids)\n",
    "print(\"Embeddings:\", embedded_output)\n",
    "\n",
    "# Mini challenge: Try printing the shape of `embedded_output`\n",
    "# Mini challenge: Try adding another sentence and compare output shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([3,2,1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n",
      "torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "print(t.shape)\n",
    "print(t.unsqueeze(-2).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "print(t.shape)\n",
    "print(t.unsqueeze(-1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n",
      "torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "print(t.shape)\n",
    "print(t.unsqueeze(0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "print(t.shape)\n",
    "print(t.unsqueeze(1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Section 2: GloVe Embeddings\n",
      "\n",
      "Similarity between king and queen: 0.7839043\n",
      "Most similar to 'neural': [('neuronal', 0.8451142907142639), ('differentiation', 0.8052098751068115), ('neurons', 0.781899631023407)]\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# ðŸ“˜ 2. Pretrained GloVe Embeddings (via gensim)\n",
    "# ==========================\n",
    "print(\"\\nðŸ”¹ Section 2: GloVe Embeddings\\n\")\n",
    "\n",
    "from gensim.downloader import load\n",
    "word_vectors = load(\"glove-wiki-gigaword-50\")  # 50d for speed\n",
    "\n",
    "# TODO: Try checking similarity between different word pairs\n",
    "print(\"Similarity between king and queen:\", word_vectors.similarity(\"king\", \"queen\"))\n",
    "print(\"Most similar to 'neural':\", word_vectors.most_similar(\"neural\", topn=3))\n",
    "\n",
    "# Mini challenge: Try glove.most_similar(positive=['woman', 'king'], negative=['man'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.8523604273796082),\n",
       " ('throne', 0.7664334177970886),\n",
       " ('prince', 0.759214460849762),\n",
       " ('daughter', 0.7473882436752319),\n",
       " ('elizabeth', 0.7460219860076904),\n",
       " ('princess', 0.7424570322036743),\n",
       " ('kingdom', 0.7337411642074585),\n",
       " ('monarch', 0.721449077129364),\n",
       " ('eldest', 0.7184861898422241),\n",
       " ('widow', 0.7099431157112122)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.most_similar(positive=['woman','king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('king', 0.8859834671020508),\n",
       " ('queen', 0.8609580993652344),\n",
       " ('daughter', 0.7684512138366699),\n",
       " ('prince', 0.7640699744224548),\n",
       " ('throne', 0.7634970545768738),\n",
       " ('princess', 0.7512727975845337),\n",
       " ('elizabeth', 0.7506488561630249),\n",
       " ('father', 0.7314496636390686),\n",
       " ('kingdom', 0.7296158671379089),\n",
       " ('mother', 0.7280009984970093)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.most_similar(word_vectors['woman']+word_vectors['king'] - word_vectors['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Section 3: BERT as Feature Extractor\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] embedding shape: torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# ðŸ“˜ 3. BERT Feature Extraction\n",
    "# ==========================\n",
    "print(\"\\nðŸ”¹ Section 3: BERT as Feature Extractor\\n\")\n",
    "\n",
    "# Load BERT base model\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# TODO: Change the sentence to test different contexts\n",
    "inputs = tokenizer(\"The cat sat on the mat.\", return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Extract CLS embedding\n",
    "cls_embedding = outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
    "print(\"[CLS] embedding shape:\", cls_embedding.shape)\n",
    "\n",
    "# Mini challenge: Extract the embedding for a specific word token (e.g., \"cat\")\n",
    "# Mini challenge: Try two different sentences and compare [CLS] embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_1 = tokenizer(\"I love women with a nice ass.\", return_tensors=\"pt\")\n",
    "cls_2 = tokenizer(\"The political landscape of the United states is filled with lobbyist.\", return_tensors=\"pt\")\n",
    "\n",
    "outputs1  = model(**cls_1).last_hidden_state[:,0,:]\n",
    "outputs2 = model(**cls_2).last_hidden_state[:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 0.8238242864608765\n"
     ]
    }
   ],
   "source": [
    "cos = torch.nn.functional.cosine_similarity(outputs1, outputs2)\n",
    "print(\"Similarity:\", cos.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Section 4: Mini Fine-Tune Setup\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=768, out_features=2, bias=True)\n",
      "Loss from dummy classification task: 0.708653450012207\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# ðŸ“˜ 4. Mini Fine-Tuning Example (Optional CPU-safe)\n",
    "# ==========================\n",
    "print(\"\\nðŸ”¹ Section 4: Mini Fine-Tune Setup\\n\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Freeze everything\n",
    "for param in model.bert.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "print(model.classifier)\n",
    "\n",
    "# TODO: Add your own text examples below to try different classes\n",
    "inputs = tokenizer([\"This is great!\", \"This is terrible!\"], padding=True, return_tensors=\"pt\")\n",
    "labels = torch.tensor([1, 0])\n",
    "\n",
    "# Forward pass + loss\n",
    "outputs = model(**inputs, labels=labels)\n",
    "loss = outputs.loss\n",
    "print(\"Loss from dummy classification task:\", loss.item())\n",
    "\n",
    "# Mini challenge: Flip the labels and see what happens to the loss\n",
    "# Mini challenge: Try freezing BERT and only training the classifier\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Section 5: Freezing BERT Layers\n",
      "\n",
      "Trainable parameters: 14177282/109483778\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# ðŸ“˜ 5. Freezing Layers + Visualizing\n",
    "# ==========================\n",
    "print(\"\\nðŸ”¹ Section 5: Freezing BERT Layers\\n\")\n",
    "\n",
    "# Freeze everything\n",
    "for param in model.bert.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# TODO: Try unfreezing only first N layers instead of last 2\n",
    "for layer in model.bert.encoder.layer[-2:]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "# Count trainable params\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable parameters: {trainable}/{total}\")\n",
    "\n",
    "# Mini challenge: Try freezing/unfreezing different blocks and track parameter count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Section 6: Layer-wise Learning Rates (Simulated)\n",
      "\n",
      "Layer 0: LR = 0.000006\n",
      "Layer 1: LR = 0.000007\n",
      "Layer 2: LR = 0.000008\n",
      "Layer 3: LR = 0.000009\n",
      "Layer 4: LR = 0.000010\n",
      "Layer 5: LR = 0.000011\n",
      "Layer 6: LR = 0.000012\n",
      "Layer 7: LR = 0.000013\n",
      "Layer 8: LR = 0.000015\n",
      "Layer 9: LR = 0.000016\n",
      "Layer 10: LR = 0.000018\n",
      "Layer 11: LR = 0.000020\n",
      "\n",
      "Simulated optimizer groups created with LLRD-style scaling.\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# ðŸ“˜ 6. Simulate LLRD Setup\n",
    "# ==========================\n",
    "print(\"\\nðŸ”¹ Section 6: Layer-wise Learning Rates (Simulated)\\n\")\n",
    "\n",
    "base_lr = 2e-5\n",
    "decay = 0.9\n",
    "optim_groups = []\n",
    "\n",
    "# TODO: Try changing decay rate or base_lr to see the impact\n",
    "for i, layer in enumerate(model.bert.encoder.layer):\n",
    "    lr = base_lr * (decay ** (11 - i))\n",
    "    optim_groups.append({\"params\": layer.parameters(), \"lr\": lr})\n",
    "    print(f\"Layer {i}: LR = {lr:.6f}\")\n",
    "\n",
    "# Add classifier head with higher LR\n",
    "optim_groups.append({\"params\": model.classifier.parameters(), \"lr\": base_lr * 2})\n",
    "\n",
    "print(\"\\nSimulated optimizer groups created with LLRD-style scaling.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
