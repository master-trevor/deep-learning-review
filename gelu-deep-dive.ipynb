{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîÆ GELU Activation Function ‚Äî Deep Dive Notes\n",
    "\n",
    "---\n",
    "\n",
    "## üìò What is GELU?\n",
    "\n",
    "**GELU** stands for **Gaussian Error Linear Unit**.\n",
    "\n",
    "It is an activation function used in deep neural networks ‚Äî particularly in **transformers** and **large language models (LLMs)**.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Core Formula\n",
    "\n",
    "GELU applies a **probabilistic gating** to the input:\n",
    "\n",
    "$$\n",
    "\\text{GELU}(x) = x \\cdot \\Phi(x)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( x \\) is the input to the activation\n",
    "- \\( \\Phi(x) \\) is the **cumulative distribution function (CDF)** of the **standard normal distribution**\n",
    "\n",
    "---\n",
    "\n",
    "## üìà What does \\( \\Phi(x) \\) represent?\n",
    "\n",
    "The standard normal CDF:\n",
    "\n",
    "$$\n",
    "\\Phi(x) = \\int_{-\\infty}^{x} \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{t^2}{2}} dt\n",
    "$$\n",
    "\n",
    "It returns the **probability that a standard normal variable is less than or equal to \\( x \\)**.\n",
    "\n",
    "In GELU, this means:\n",
    "- If \\( x \\) is very negative ‚Üí \\( $\\Phi(x) \\approx$ 0 \\) ‚Üí suppress \\( x \\)\n",
    "- If \\( x \\) is very positive ‚Üí \\( $\\Phi(x) \\approx 1$ \\) ‚Üí keep \\( x \\)\n",
    "- If \\( x $\\approx 0$ \\) ‚Üí \\( $\\Phi(x) \\approx$ 0.5 \\) ‚Üí scale \\( x \\) down\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö° Practical Approximation\n",
    "\n",
    "The exact CDF is slow to compute, so GELU is often approximated using:\n",
    "\n",
    "$$\n",
    "\\text{GELU}(x) \\approx 0.5x \\left(1 + \\tanh\\left( \\sqrt{\\frac{2}{\\pi}} (x + 0.044715x^3) \\right)\\right)\n",
    "$$\n",
    "\n",
    "This version is **fast and differentiable**, making it suitable for training large models.\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Gradient of GELU\n",
    "\n",
    "Unlike ReLU (whose gradient is 0 or 1), GELU‚Äôs gradient is **smooth and continuous**:\n",
    "\n",
    "$$\n",
    "\\frac{d}{dx} \\text{GELU}(x) = \\Phi(x) + x \\cdot \\phi(x)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( \\phi(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{x^2}{2}} \\) is the standard normal **PDF**\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ GELU vs ReLU\n",
    "\n",
    "| Feature            | ReLU                          | GELU                                |\n",
    "|--------------------|-------------------------------|--------------------------------------|\n",
    "| Formula            | \\( \\max(0, x) \\)              | \\( x \\cdot \\Phi(x) \\)               |\n",
    "| Gradient           | 0 or 1                        | Smooth curve from 0 to 1             |\n",
    "| Negative inputs    | Zeroed                        | Scaled down, not discarded           |\n",
    "| Behavior at 0      | Discontinuous                 | Smooth and continuous                |\n",
    "| Used in            | CNNs, older MLPs              | Transformers, LLMs, deep networks    |\n",
    "\n",
    "---\n",
    "\n",
    "## üß¨ Why GELU Works Well\n",
    "\n",
    "- **Smooth nonlinearity** ‚Üí better gradient flow\n",
    "- **No dead neurons** ‚Üí more stable training\n",
    "- **Probabilistic gating** ‚Üí subtle inputs aren't fully discarded\n",
    "- **Natural fit for Gaussian-like logits** ‚Üí thanks to Central Limit Theorem\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ Visual Intuition\n",
    "\n",
    "- GELU behaves like identity for large positive \\( x \\)\n",
    "- GELU acts like a soft zero for large negative \\( x \\)\n",
    "- GELU transitions smoothly around 0 ‚Üí preserves nuance\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ When to Use GELU\n",
    "\n",
    "- In deep networks where **gradient stability** matters\n",
    "- When you want **soft suppression** of weak signals instead of hard cutoffs\n",
    "- Especially in **transformers** (e.g. BERT, GPT, T5)\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ùå When Not to Use It\n",
    "\n",
    "- If you need extreme sparsity or fast compute (ReLU may be faster)\n",
    "- If you‚Äôre in a simple model with little depth\n",
    "\n",
    "---\n",
    "\n",
    "## üí° TL;DR\n",
    "\n",
    "**GELU is like a probabilistic ReLU that doesn‚Äôt throw away negative inputs ‚Äî it softly damps them based on how useful they‚Äôre likely to be.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
